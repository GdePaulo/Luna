{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../code'))\n",
    "import config\n",
    "l = 0\n",
    "with open(\"data/stparkpap/def-dump.txt\", encoding=\"ISO-8859-1\") as fileobject:\n",
    "    definitions = {k: [] for k in config.scrapeTarget[\"languages\"]}\n",
    "    words_to_read = 5\n",
    "    for line in fileobject:\n",
    "        l += 1\n",
    "        # print(f\"Reading line {l}\")\n",
    "        if \"--page: \" in line: \n",
    "            continue\n",
    "        elif \"--word: \" in line:\n",
    "            if words_to_read > 0 and l != 2:\n",
    "                print(f\"The definitions prior to line {l} have < 5 entrees\")\n",
    "            words_to_read = 5\n",
    "        else:\n",
    "            # definitions += line.strip()\n",
    "            if words_to_read == -1:\n",
    "                continue\n",
    "\n",
    "            cur_key = config.scrapeTarget[\"languages\"][5 - words_to_read]\n",
    "            if line.strip() in definitions[cur_key] and cur_key == \"pap\":\n",
    "                print(f\"{line.strip()} is a pap duplicate entree. Skipping...\")\n",
    "                words_to_read = -1\n",
    "            else:\n",
    "                definitions[cur_key].append(line.strip())\n",
    "                words_to_read -= 1\n",
    "                \n",
    "        \n",
    "df = pd.DataFrame.from_dict(definitions)\n",
    "print(f\"length is {len(df)}\")\n",
    "# df.set_index(\"pap\", inplace=True)\n",
    "# print(f\"length after setting index is {len(df)}\")\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def cleanupTokens(definition):\n",
    "    without_tags = re.sub(r'[\\(\\[].*?[\\)\\]]', \"\", definition)\n",
    "    return without_tags.strip().lower()\n",
    "\n",
    "for lan in config.scrapeTarget['languages']:\n",
    "    simple_col_name = f\"{lan}-simple\"\n",
    "    df[simple_col_name] = df.apply(lambda row: cleanupTokens(row[lan]), axis=1)\n",
    "df.set_index(\"pap-simple\", inplace=True)\n",
    "df[\"type\"] = df.apply(lambda row: \"sentence\" if len(row.name.split()) > 1 else \"word\", axis=1)\n",
    "\n",
    "filename = \"-\".join(config.scrapeTarget[\"languages\"])\n",
    "df.to_csv(f\"data/stparkpap/{filename}.csv\")\n",
    "\n",
    "# Remove other signs such as quotes and questionmark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"ami ta bai kas\"\n",
    "sentence = \"mi stima mi yu-homber\"\n",
    "sentence = \"mi ta k√≤re outo\"\n",
    "sentence = \"mi ta koriendo outo\"\n",
    "translation = \"\"\n",
    "# print(df.loc[\"ta\"])\n",
    "# Try to ignore accents if the word isn't there\n",
    "\n",
    "translation = df.loc[sentence][\"en-simple\"]\n",
    "# for word in sentence.split():\n",
    "    # word = word.lower()\n",
    "    # translation += df.loc[word][\"pap_simple\"] + \" \"\n",
    "print(f\"source: {sentence} \\n translation: {translation}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5c6032c78efa7f90cd1715915e3f0102fff89cef77c12db36694d1cd45acba4a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 ('luna-translate')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
