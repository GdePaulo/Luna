{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../code'))\n",
    "import config\n",
    "from translate import Translate\n",
    "\n",
    "languages = [\"pap\", \"pap-simple\", \"nl\", \"nl-simple\"]\n",
    "\n",
    "# set filter default to deal with nan value\n",
    "hny_pap_nl = pd.read_csv(\"../data/hny/pap-nl.csv\", na_filter=False)\n",
    "stparkpap_pap_nl_name = \"-\".join(config.scrapeTarget[\"languages\"])\n",
    "stparkpap_pap_nl = pd.read_csv(f\"../data/stparkpap/{stparkpap_pap_nl_name}.csv\")#, index_col=0)\n",
    "crse = pd.read_csv(f\"../data/{config.crse['name']}/nl-pap.csv\")\n",
    "tatoeba = pd.read_csv(f\"../data/tatoeba/pap-nl.csv\")\n",
    "\n",
    "# corpus = pd.concat([hny_pap_nl], ignore_index=True)\n",
    "corpus = pd.concat([hny_pap_nl, stparkpap_pap_nl, crse, tatoeba], ignore_index=True)\n",
    "corpus = corpus[languages]\n",
    "corpus = Translate.attachType(corpus, \"pap-simple\")\n",
    "corpus\n",
    "\n",
    "# Separate out !,?.\n",
    "# There are numbers in pap-simple\n",
    "# stpark has slasher and stuff. insert as other row?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = Translate()\n",
    "\n",
    "short_sentences = [\n",
    "    \"ami ta bai kas\",\n",
    "    \"mi stima mi yu-homber\",\n",
    "    \"ami ta kòre outo\",\n",
    "    \"mi no kier bai skol\",\n",
    "    \"mi no kier kumpra bo kacho\",\n",
    "    \"e ta hopi grandi\",\n",
    "    ]\n",
    "long_sentences = [\n",
    "    # \"mi no ta guste pasobra e ta parse un kabritu\",\n",
    "    # \"nan no tabata tin ningun idea pakiko e no a hasie\"\n",
    "    # \"mi kier duna bo un kos aki\",\n",
    "    # \"Mi no kier tende loke bo ta bisa\",\n",
    "    # \"Mi no kier tende loke bo ta bisa, pasobra tur biaha bo ta gaña\",\n",
    "    # \"Mi kier kumpra un grandi i un chiki\",\n",
    "    # \"Mi tin hopi gana di bai landa i kome na mi mama su kas\",\n",
    "    # \"Mi no kier kita mi karson\",\n",
    "    # \"antiano no sa biba den sushi asina aki\",\n",
    "    # \"Mi no gusta mi ruman pasobra e ta dal mi kada bia ku e wak mi.\",\n",
    "    # \"Mi tin gana di kome un bon hamburger\",\n",
    "    # \"E programa aki ta hopi malu\",\n",
    "    # \"Pa konklui , un hende mester bisa ami si esun aki ta mas miho of mas malu.\"\n",
    "    # \"Pa konklui mi kier purba un otro zin.\"\n",
    "    \"Mi kier kome pan\"\n",
    "]\n",
    "test_sentence = \"mi yu di kriansa a pidi ami pa un pida bolo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordCorrections(sentence, words_corpus):\n",
    "    translations = {}   \n",
    "\n",
    "    for word in sentence.split():\n",
    "        word = word.lower()\n",
    "        words_corpus = Translate.attachClosest(words_corpus, word, \"pap-simple\")\n",
    "        # print(words_corpus.head(3))\n",
    "        if words_corpus[\"closest\"].iloc[0] > 0:\n",
    "            translations[word] = words_corpus.head(3)[\"pap-simple\"].to_list()\n",
    "    return translations\n",
    "\n",
    "print(getWordCorrections(\"many can\", hny_pap_nl[hny_pap_nl[\"type\"]==\"word\"]))                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, lastNode=False):\n",
    "        self.children = {}\n",
    "        self.lastNode = lastNode\n",
    "        self.accented = {\n",
    "            \"a\" : [\"à\",\"á\"], \n",
    "            \"e\" : [\"è\",\"é\"], \n",
    "            \"i\" : [\"ì\",\"í\"], \n",
    "            \"o\" : [\"ò\",\"ó\"], \n",
    "            \"u\" : [\"ù\",\"ú\"], \n",
    "            \"n\" : [\"ñ\"], \n",
    "        }\n",
    "    def __str__(self):\n",
    "        if self.children:\n",
    "            ret = f\"{len(self.children)}\"\n",
    "            for k, v in self.children.items():\n",
    "                ret += f\" {k}: {v.lastNode} {v}\"\n",
    "            return ret\n",
    "        else:\n",
    "            return \"\"\n",
    "\n",
    "    def insert(self, word):\n",
    "        current = self\n",
    "        for i, l in enumerate(word):\n",
    "            if l in current.children:\n",
    "                # print(f\"from {current} -> \")\n",
    "                current = current.children[l]\n",
    "                # print(f\"to {current}\")\n",
    "            else:\n",
    "                newNode = Node()\n",
    "                # print(f\"inserting {l} into {current}\")\n",
    "                current.children[l] = newNode\n",
    "                # print(f\"new {len(current.children)} children {current}\")\n",
    "                current = newNode\n",
    "            if i == len(word)-1:\n",
    "                current.lastNode = True\n",
    "\n",
    "    def find(self, word):\n",
    "        current = self\n",
    "        for i, l in enumerate(word):\n",
    "            if l in current.children:\n",
    "                current = current.children[l]\n",
    "                if i == len(word)-1:\n",
    "                    return current.lastNode\n",
    "            else:\n",
    "                return False\n",
    "\n",
    "    def lenientFind(self, word):\n",
    "        found_word = \"\" \n",
    "        current = self\n",
    "        for i, l in enumerate(word):\n",
    "            found_key = False\n",
    "            # print(f\"checking if {l} has accented\")\n",
    "            if l in self.accented:\n",
    "                for accented_l in self.accented[l]:\n",
    "                    # print(f\"checking {accented_l}\")\n",
    "                    if accented_l in current.children:\n",
    "                        # print(f\"found {accented_l}\")\n",
    "                        accented_search = accented_l + word[i+1:]\n",
    "                        found_accented_word = current.lenientFind(accented_search)\n",
    "                        if found_accented_word:\n",
    "                            return found_word + found_accented_word\n",
    "                \n",
    "            if l in current.children:\n",
    "                current = current.children[l]\n",
    "                found_word += l\n",
    "                found_key = True\n",
    "\n",
    "            if not found_key:\n",
    "                return \"\"\n",
    "            \n",
    "            if i == len(word)-1:\n",
    "                if current.lastNode:\n",
    "                    return found_word\n",
    "                else:\n",
    "                    return \"\"\n",
    "            \n",
    "    \n",
    "    def populate(self, words):\n",
    "        for word in words:\n",
    "            self.insert(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "corpus = hny_pap_nl[hny_pap_nl[\"type\"]==\"word\"]\n",
    "words = corpus[\"pap-simple\"].values\n",
    "# start_time = time.time()\n",
    "# print(\"--- %.2f seconds ---\" % (time.time() - start_time))\n",
    "test_acc_word = \"kalumnia\" + u'\\u0301'\n",
    "test_acc_word = \"kalumniá\"\n",
    "# corpus[corpus[\"pap-simple\"]==\"kardiologo\"]\n",
    "corpus[corpus[\"pap-simple\"]==test_acc_word]\n",
    "# print(test_acc_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getWordCorrections(\"mi man no ta bon\", corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Node()\n",
    "t.populate(words)\n",
    "result = {}\n",
    "sentence = \"mi man no ta bon\"\n",
    "for word in sentence.split():\n",
    "    result[word] = t.find(word)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t.find(\"kámara\")\n",
    "print(t.find(\"kamara\"))\n",
    "print(t.lenientFind(\"manan\"))\n",
    "print(t.lenientFind(\"kamara\"))\n",
    "print(t.lenientFind(\"inkreibel\"))\n",
    "# print(t.lenientFind(\"kalmeki\"))\n",
    "t.find(\"kalmeki\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5c6032c78efa7f90cd1715915e3f0102fff89cef77c12db36694d1cd45acba4a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 ('luna-translate')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
