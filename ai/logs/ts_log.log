2022-04-17T11:34:36,987 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-04-17T11:34:36,987 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-04-17T11:34:37,242 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.5.3
TS Home: C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages
Current directory: C:\Users\Ameno\Desktop\luna\ai
Temp directory: C:\Users\Ameno\AppData\Local\Temp
Number of GPUs: 0
Number of CPUs: 12
Max heap size: 4048 M
Python executable: C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe
Config file: deployment/config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: C:\Users\Ameno\Desktop\luna\ai\deployment\model-store
Initial Models: foodnet=foodnet_resnet18.mar
Log dir: C:\Users\Ameno\Desktop\luna\ai\logs
Metrics dir: C:\Users\Ameno\Desktop\luna\ai\logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 12
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: C:\Users\Ameno\Desktop\luna\ai\deployment\model-store
Model config: N/A
2022-04-17T11:34:37,242 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.5.3
TS Home: C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages
Current directory: C:\Users\Ameno\Desktop\luna\ai
Temp directory: C:\Users\Ameno\AppData\Local\Temp
Number of GPUs: 0
Number of CPUs: 12
Max heap size: 4048 M
Python executable: C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe
Config file: deployment/config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: C:\Users\Ameno\Desktop\luna\ai\deployment\model-store
Initial Models: foodnet=foodnet_resnet18.mar
Log dir: C:\Users\Ameno\Desktop\luna\ai\logs
Metrics dir: C:\Users\Ameno\Desktop\luna\ai\logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 12
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: C:\Users\Ameno\Desktop\luna\ai\deployment\model-store
Model config: N/A
2022-04-17T11:34:37,260 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-04-17T11:34:37,260 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-04-17T11:34:37,291 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: foodnet_resnet18.mar
2022-04-17T11:34:37,291 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: foodnet_resnet18.mar
2022-04-17T11:34:38,669 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model foodnet
2022-04-17T11:34:38,669 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model foodnet
2022-04-17T11:34:38,669 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model foodnet
2022-04-17T11:34:38,669 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model foodnet
2022-04-17T11:34:38,670 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model foodnet loaded.
2022-04-17T11:34:38,670 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model foodnet loaded.
2022-04-17T11:34:38,670 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: foodnet, count: 12
2022-04-17T11:34:38,670 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: foodnet, count: 12
2022-04-17T11:34:38,687 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-04-17T11:34:38,687 [DEBUG] W-9010-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9010]
2022-04-17T11:34:38,687 [DEBUG] W-9001-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9001]
2022-04-17T11:34:38,687 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-04-17T11:34:38,687 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-04-17T11:34:38,688 [DEBUG] W-9011-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9011]
2022-04-17T11:34:38,688 [DEBUG] W-9009-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9009]
2022-04-17T11:34:38,687 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-04-17T11:34:38,688 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-04-17T11:34:38,688 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-04-17T11:34:38,689 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9008]
2022-04-17T11:34:38,690 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-04-17T11:34:38,687 [DEBUG] W-9001-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9001]
2022-04-17T11:34:38,687 [DEBUG] W-9010-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9010]
2022-04-17T11:34:38,687 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-04-17T11:34:38,690 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-04-17T11:34:38,694 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: NioServerSocketChannel.
2022-04-17T11:34:38,688 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-04-17T11:34:38,688 [DEBUG] W-9011-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9011]
2022-04-17T11:34:38,688 [DEBUG] W-9009-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9009]
2022-04-17T11:34:38,688 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-04-17T11:34:38,700 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-04-17T11:34:38,689 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9008]
2022-04-17T11:34:38,687 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-04-17T11:34:38,694 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: NioServerSocketChannel.
2022-04-17T11:34:38,700 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-04-17T11:34:39,013 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-04-17T11:34:39,013 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-04-17T11:34:39,015 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: NioServerSocketChannel.
2022-04-17T11:34:39,015 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: NioServerSocketChannel.
2022-04-17T11:34:39,019 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-04-17T11:34:39,019 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-04-17T11:34:39,020 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: NioServerSocketChannel.
2022-04-17T11:34:39,020 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: NioServerSocketChannel.
2022-04-17T11:34:39,022 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2022-04-17T11:34:39,022 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2022-04-17T11:34:39,624 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-04-17T11:34:39,624 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-04-17T11:34:40,153 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:100.0|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:1650188080
2022-04-17T11:34:40,160 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:81.05179214477539|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:1650188080
2022-04-17T11:34:40,161 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:374.79683685302734|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:1650188080
2022-04-17T11:34:40,162 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:82.2|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:1650188080
2022-04-17T11:34:40,163 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:5837.4296875|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:1650188080
2022-04-17T11:34:40,165 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:10352.234375|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:1650188080
2022-04-17T11:34:40,166 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:63.9|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:1650188080
2022-04-17T11:34:41,484 [INFO ] W-9010-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:34:41,484 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:34:41,484 [INFO ] W-9011-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:34:41,484 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:34:41,484 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:34:41,484 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:34:41,486 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - [PID]17812
2022-04-17T11:34:41,486 [INFO ] W-9010-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:34:41,484 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:34:41,484 [INFO ] W-9009-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:34:41,484 [INFO ] W-9001-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:34:41,487 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:34:41,487 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - [PID]17764
2022-04-17T11:34:41,486 [INFO ] W-9011-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:34:41,488 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:34:41,488 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - [PID]16688
2022-04-17T11:34:41,488 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change null -> WORKER_STARTED
2022-04-17T11:34:41,487 [INFO ] W-9010-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:34:41,488 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:34:41,488 [INFO ] W-9011-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:34:41,488 [INFO ] W-9001-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:34:41,490 [INFO ] W-9011-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-04-17T11:34:41,489 [INFO ] W-9010-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-04-17T11:34:41,489 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:34:41,489 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:34:41,488 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change null -> WORKER_STARTED
2022-04-17T11:34:41,491 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:34:41,490 [INFO ] W-9010-foodnet_1.0-stdout MODEL_LOG -     worker.run_server()
2022-04-17T11:34:41,492 [INFO ] W-9010-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py", line 148, in run_server
2022-04-17T11:34:41,490 [INFO ] W-9011-foodnet_1.0-stdout MODEL_LOG -     worker.run_server()
2022-04-17T11:34:41,493 [INFO ] W-9010-foodnet_1.0-stdout MODEL_LOG -     self.sock.bind((self.sock_name, int(self.port)))
2022-04-17T11:34:41,493 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change null -> WORKER_STARTED
2022-04-17T11:34:41,490 [INFO ] W-9001-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:34:41,490 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - [PID]14848
2022-04-17T11:34:41,493 [INFO ] W-9010-foodnet_1.0-stdout MODEL_LOG - OSError: [WinError 10013] An attempt was made to access a socket in a way forbidden by its access permissions
2022-04-17T11:34:41,494 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:34:41,494 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change null -> WORKER_STARTED
2022-04-17T11:34:41,494 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change null -> WORKER_STARTED
2022-04-17T11:34:41,494 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:34:41,493 [INFO ] W-9011-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py", line 148, in run_server
2022-04-17T11:34:41,493 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - [PID]13836
2022-04-17T11:34:41,496 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change null -> WORKER_STARTED
2022-04-17T11:34:41,496 [INFO ] W-9011-foodnet_1.0-stdout MODEL_LOG -     self.sock.bind((self.sock_name, int(self.port)))
2022-04-17T11:34:41,504 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-04-17T11:34:41,493 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change null -> WORKER_STARTED
2022-04-17T11:34:41,507 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-04-17T11:34:41,494 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change null -> WORKER_STARTED
2022-04-17T11:34:41,494 [INFO ] W-9001-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-04-17T11:34:41,494 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change null -> WORKER_STARTED
2022-04-17T11:34:41,504 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-04-17T11:34:41,508 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-04-17T11:34:41,508 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-04-17T11:34:41,497 [INFO ] W-9011-foodnet_1.0-stdout MODEL_LOG - OSError: [WinError 10013] An attempt was made to access a socket in a way forbidden by its access permissions
2022-04-17T11:34:41,496 [INFO ] W-9009-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:34:41,496 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:34:41,496 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change null -> WORKER_STARTED
2022-04-17T11:34:41,509 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-04-17T11:34:41,508 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-04-17T11:34:41,508 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-04-17T11:34:41,508 [INFO ] W-9001-foodnet_1.0-stdout MODEL_LOG -     worker.run_server()
2022-04-17T11:34:41,507 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-04-17T11:34:41,509 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-04-17T11:34:41,509 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:34:41,508 [INFO ] W-9009-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:34:41,511 [INFO ] W-9001-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py", line 148, in run_server
2022-04-17T11:34:41,513 [INFO ] W-9009-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-04-17T11:34:41,513 [INFO ] W-9001-foodnet_1.0-stdout MODEL_LOG -     self.sock.bind((self.sock_name, int(self.port)))
2022-04-17T11:34:41,513 [INFO ] W-9009-foodnet_1.0-stdout MODEL_LOG -     worker.run_server()
2022-04-17T11:34:41,514 [INFO ] W-9001-foodnet_1.0-stdout MODEL_LOG - OSError: [WinError 10013] An attempt was made to access a socket in a way forbidden by its access permissions
2022-04-17T11:34:41,514 [INFO ] W-9009-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py", line 148, in run_server
2022-04-17T11:34:41,514 [INFO ] W-9009-foodnet_1.0-stdout MODEL_LOG -     self.sock.bind((self.sock_name, int(self.port)))
2022-04-17T11:34:41,515 [INFO ] W-9009-foodnet_1.0-stdout MODEL_LOG - OSError: [WinError 10013] An attempt was made to access a socket in a way forbidden by its access permissions
2022-04-17T11:34:41,530 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:34:41,532 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - [PID]3392
2022-04-17T11:34:41,533 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:34:41,534 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change null -> WORKER_STARTED
2022-04-17T11:34:41,534 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:34:41,534 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change null -> WORKER_STARTED
2022-04-17T11:34:41,536 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9007).
2022-04-17T11:34:41,536 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9003).
2022-04-17T11:34:41,536 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9006).
2022-04-17T11:34:41,536 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9002).
2022-04-17T11:34:41,536 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9000).
2022-04-17T11:34:41,537 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9008
2022-04-17T11:34:41,537 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9008
2022-04-17T11:34:41,541 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9008).
2022-04-17T11:34:41,542 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188081542
2022-04-17T11:34:41,542 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188081542
2022-04-17T11:34:41,543 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188081543
2022-04-17T11:34:41,543 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188081543
2022-04-17T11:34:41,543 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188081543
2022-04-17T11:34:41,543 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188081543
2022-04-17T11:34:41,542 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188081542
2022-04-17T11:34:41,543 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188081543
2022-04-17T11:34:41,543 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188081543
2022-04-17T11:34:41,543 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188081543
2022-04-17T11:34:41,542 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188081542
2022-04-17T11:34:41,543 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188081543
2022-04-17T11:34:41,557 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:34:41,559 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - [PID]15072
2022-04-17T11:34:41,560 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change null -> WORKER_STARTED
2022-04-17T11:34:41,560 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:34:41,560 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change null -> WORKER_STARTED
2022-04-17T11:34:41,561 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-04-17T11:34:41,561 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:34:41,561 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-04-17T11:34:41,578 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188081578
2022-04-17T11:34:41,578 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188081578
2022-04-17T11:34:41,592 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9004).
2022-04-17T11:34:41,642 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:34:41,647 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:34:41,647 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:34:41,647 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:34:41,644 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:34:41,643 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:34:41,645 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:34:41,786 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:34:41,789 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - [PID]18076
2022-04-17T11:34:41,790 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:34:41,791 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change null -> WORKER_STARTED
2022-04-17T11:34:41,791 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:34:41,791 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change null -> WORKER_STARTED
2022-04-17T11:34:41,792 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-04-17T11:34:41,792 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-04-17T11:34:41,797 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188081797
2022-04-17T11:34:41,797 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9005).
2022-04-17T11:34:41,797 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188081797
2022-04-17T11:34:41,830 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:34:41,919 [INFO ] W-9011-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9011-foodnet_1.0-stdout
2022-04-17T11:34:41,919 [INFO ] W-9011-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9011-foodnet_1.0-stderr
2022-04-17T11:34:41,919 [INFO ] W-9011-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9011-foodnet_1.0-stdout
2022-04-17T11:34:41,919 [INFO ] W-9011-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9011-foodnet_1.0-stderr
2022-04-17T11:34:41,920 [WARN ] W-9011-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9011-foodnet_1.0-stderr
2022-04-17T11:34:41,920 [WARN ] W-9011-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9011-foodnet_1.0-stderr
2022-04-17T11:34:41,921 [WARN ] W-9011-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9011-foodnet_1.0-stdout
2022-04-17T11:34:41,921 [WARN ] W-9011-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9011-foodnet_1.0-stdout
2022-04-17T11:34:41,922 [ERROR] W-9011-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:292) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:41,951 [INFO ] W-9009-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9009-foodnet_1.0-stdout
2022-04-17T11:34:41,922 [ERROR] W-9011-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:292) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:41,951 [INFO ] W-9009-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9009-foodnet_1.0-stderr
2022-04-17T11:34:41,951 [INFO ] W-9009-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9009-foodnet_1.0-stdout
2022-04-17T11:34:41,952 [WARN ] W-9009-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9009-foodnet_1.0-stderr
2022-04-17T11:34:41,951 [INFO ] W-9009-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9009-foodnet_1.0-stderr
2022-04-17T11:34:41,952 [WARN ] W-9009-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9009-foodnet_1.0-stderr
2022-04-17T11:34:41,953 [WARN ] W-9009-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9009-foodnet_1.0-stdout
2022-04-17T11:34:41,953 [WARN ] W-9009-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9009-foodnet_1.0-stdout
2022-04-17T11:34:41,953 [ERROR] W-9009-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:292) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:41,953 [ERROR] W-9009-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:292) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:41,962 [INFO ] W-9001-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-foodnet_1.0-stdout
2022-04-17T11:34:41,962 [INFO ] W-9001-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-foodnet_1.0-stderr
2022-04-17T11:34:41,962 [INFO ] W-9001-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-foodnet_1.0-stdout
2022-04-17T11:34:41,963 [WARN ] W-9001-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-foodnet_1.0-stderr
2022-04-17T11:34:41,962 [INFO ] W-9001-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-foodnet_1.0-stderr
2022-04-17T11:34:41,963 [WARN ] W-9001-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-foodnet_1.0-stderr
2022-04-17T11:34:41,964 [WARN ] W-9001-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-foodnet_1.0-stdout
2022-04-17T11:34:41,964 [WARN ] W-9001-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-foodnet_1.0-stdout
2022-04-17T11:34:41,965 [ERROR] W-9001-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:292) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:41,965 [ERROR] W-9001-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:292) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:41,977 [INFO ] W-9010-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9010-foodnet_1.0-stdout
2022-04-17T11:34:41,977 [INFO ] W-9010-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9010-foodnet_1.0-stderr
2022-04-17T11:34:41,977 [INFO ] W-9010-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9010-foodnet_1.0-stdout
2022-04-17T11:34:41,978 [WARN ] W-9010-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9010-foodnet_1.0-stderr
2022-04-17T11:34:41,977 [INFO ] W-9010-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9010-foodnet_1.0-stderr
2022-04-17T11:34:41,978 [WARN ] W-9010-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9010-foodnet_1.0-stderr
2022-04-17T11:34:41,979 [WARN ] W-9010-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9010-foodnet_1.0-stdout
2022-04-17T11:34:41,979 [WARN ] W-9010-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9010-foodnet_1.0-stdout
2022-04-17T11:34:41,980 [ERROR] W-9010-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:292) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:41,980 [ERROR] W-9010-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:292) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:42,443 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:34:42,443 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:34:42,443 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:34:42,444 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:34:42,444 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:34:42,444 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:34:42,444 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:34:42,445 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:34:42,446 [INFO ] nioEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:42,446 [INFO ] nioEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9008 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:42,446 [INFO ] nioEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:42,446 [INFO ] nioEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:42,446 [INFO ] nioEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:42,445 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:34:42,445 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:34:42,445 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:34:42,446 [INFO ] nioEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9008 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:42,448 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:34:42,446 [INFO ] nioEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:42,446 [INFO ] nioEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:42,446 [INFO ] nioEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:42,446 [INFO ] nioEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:42,449 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:42,445 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:34:42,449 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:42,445 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:34:42,449 [INFO ] nioEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:42,448 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:34:42,448 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:34:42,448 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:34:42,450 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:34:42,451 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:34:42,451 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:34:42,452 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:34:42,452 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:42,452 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:34:42,456 [INFO ] nioEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:42,456 [INFO ] nioEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:42,457 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:42,459 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:42,447 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:34:42,452 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:42,459 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:42,460 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:42,455 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:34:42,456 [INFO ] nioEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:42,456 [INFO ] nioEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:42,461 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:42,461 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:34:42,453 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:34:42,450 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:34:42,462 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:42,450 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:34:42,460 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:42,449 [INFO ] nioEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:42,463 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:42,450 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:34:42,449 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:34:42,449 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:42,449 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:42,463 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:34:42,463 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:34:42,464 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:42,464 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:42,463 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:42,465 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:42,460 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:42,462 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:34:42,462 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:34:42,462 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:42,462 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:34:42,462 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:34:42,455 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:34:42,461 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:42,468 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:42,468 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:42,468 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:42,460 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:42,470 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:42,457 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:42,468 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:42,472 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:42,460 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:34:42,472 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:42,470 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:42,473 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:42,468 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:42,468 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:42,468 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:34:42,475 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:42,475 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:42,468 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:34:42,475 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:34:42,467 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:34:42,467 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:34:42,465 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:42,478 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:42,464 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:42,465 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:34:42,479 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:42,464 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:42,465 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:34:42,479 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:34:42,481 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:42,479 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:42,478 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:42,481 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:42,481 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:42,475 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:34:42,475 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:34:42,475 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:34:42,475 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:34:42,468 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-04-17T11:34:42,475 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:42,475 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:42,485 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stderr
2022-04-17T11:34:42,485 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:42,473 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:42,485 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stderr
2022-04-17T11:34:42,472 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:34:42,472 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:42,472 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:42,488 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:42,488 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:42,486 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:34:42,485 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stderr
2022-04-17T11:34:42,488 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stdout
2022-04-17T11:34:42,485 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:42,489 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stderr
2022-04-17T11:34:42,488 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stdout
2022-04-17T11:34:42,489 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stderr
2022-04-17T11:34:42,489 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stdout
2022-04-17T11:34:42,482 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-04-17T11:34:42,482 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:34:42,482 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:34:42,481 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:34:42,490 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 1 seconds.
2022-04-17T11:34:42,481 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:34:42,481 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:42,481 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:42,491 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stderr
2022-04-17T11:34:42,491 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stderr
2022-04-17T11:34:42,481 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:34:42,491 [INFO ] W-9003-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stdout
2022-04-17T11:34:42,481 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:34:42,481 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:42,492 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:42,491 [INFO ] W-9003-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stdout
2022-04-17T11:34:42,492 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:42,491 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stderr
2022-04-17T11:34:42,490 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-04-17T11:34:42,493 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stdout
2022-04-17T11:34:42,493 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stderr
2022-04-17T11:34:42,490 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 1 seconds.
2022-04-17T11:34:42,490 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:34:42,490 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:34:42,490 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 790, in exec_module
2022-04-17T11:34:42,490 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:34:42,489 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stdout
2022-04-17T11:34:42,494 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 1 seconds.
2022-04-17T11:34:42,494 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 1 seconds.
2022-04-17T11:34:42,485 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stderr
2022-04-17T11:34:42,495 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stdout
2022-04-17T11:34:42,495 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stdout
2022-04-17T11:34:42,495 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9008 in 1 seconds.
2022-04-17T11:34:42,495 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9008 in 1 seconds.
2022-04-17T11:34:42,498 [INFO ] W-9004-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stderr
2022-04-17T11:34:42,497 [INFO ] W-9003-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stderr
2022-04-17T11:34:42,488 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:34:42,498 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:34:42,499 [INFO ] W-9005-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stderr
2022-04-17T11:34:42,499 [INFO ] W-9006-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stderr
2022-04-17T11:34:42,499 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:34:42,500 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:34:42,501 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:34:42,502 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-04-17T11:34:42,503 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-04-17T11:34:42,488 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:42,505 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:42,499 [INFO ] W-9006-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stderr
2022-04-17T11:34:42,499 [INFO ] W-9005-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stderr
2022-04-17T11:34:42,488 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:42,497 [INFO ] W-9003-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stderr
2022-04-17T11:34:42,498 [INFO ] W-9004-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stderr
2022-04-17T11:34:42,508 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stderr
2022-04-17T11:34:42,494 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:34:42,508 [INFO ] W-9004-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stdout
2022-04-17T11:34:42,494 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-04-17T11:34:42,493 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:34:42,508 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\AppData\Local\Temp\models\b31932e975844d80809048017333897f\handler.py", line 3, in <module>
2022-04-17T11:34:42,493 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:34:42,493 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stderr
2022-04-17T11:34:42,510 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stdout
2022-04-17T11:34:42,493 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-04-17T11:34:42,510 [INFO ] W-9008-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stdout
2022-04-17T11:34:42,493 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stdout
2022-04-17T11:34:42,491 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stderr
2022-04-17T11:34:42,510 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stdout
2022-04-17T11:34:42,510 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 1 seconds.
2022-04-17T11:34:42,491 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:34:42,510 [INFO ] W-9008-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stdout
2022-04-17T11:34:42,510 [INFO ] W-9008-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stderr
2022-04-17T11:34:42,509 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:34:42,511 [INFO ] W-9006-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stdout
2022-04-17T11:34:42,509 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier as ImgClsfr
2022-04-17T11:34:42,511 [INFO ] W-9000-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stderr
2022-04-17T11:34:42,509 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:34:42,508 [INFO ] W-9004-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stdout
2022-04-17T11:34:42,508 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stderr
2022-04-17T11:34:42,513 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stdout
2022-04-17T11:34:42,505 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:42,505 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 790, in exec_module
2022-04-17T11:34:42,513 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stderr
2022-04-17T11:34:42,513 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stdout
2022-04-17T11:34:42,514 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 1 seconds.
2022-04-17T11:34:42,512 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:34:42,514 [INFO ] W-9007-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stdout
2022-04-17T11:34:42,511 [INFO ] W-9000-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stderr
2022-04-17T11:34:42,512 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\torch_handler\image_classifier.py", line 8, in <module>
2022-04-17T11:34:42,511 [INFO ] W-9006-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stdout
2022-04-17T11:34:42,510 [INFO ] W-9008-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stderr
2022-04-17T11:34:42,510 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stdout
2022-04-17T11:34:42,515 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-04-17T11:34:42,511 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:34:42,516 [INFO ] W-9000-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stdout
2022-04-17T11:34:42,515 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-04-17T11:34:42,516 [INFO ] W-9000-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stdout
2022-04-17T11:34:42,510 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stdout
2022-04-17T11:34:42,515 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-04-17T11:34:42,514 [INFO ] W-9007-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stdout
2022-04-17T11:34:42,517 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 1 seconds.
2022-04-17T11:34:42,510 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 1 seconds.
2022-04-17T11:34:42,517 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\torch_handler\vision_handler.py", line 11, in <module>
2022-04-17T11:34:42,518 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-04-17T11:34:42,518 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2022-04-17T11:34:42,518 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - 
2022-04-17T11:34:42,519 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-04-17T11:34:42,519 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - 
2022-04-17T11:34:42,520 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:34:42,520 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-04-17T11:34:42,520 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -     worker.run_server()
2022-04-17T11:34:42,521 [INFO ] W-9002-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stderr
2022-04-17T11:34:42,522 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-04-17T11:34:42,522 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-04-17T11:34:42,523 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-04-17T11:34:42,514 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 1 seconds.
2022-04-17T11:34:42,513 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stderr
2022-04-17T11:34:42,524 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stdout
2022-04-17T11:34:42,513 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-04-17T11:34:42,523 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-04-17T11:34:42,524 [INFO ] W-9005-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stdout
2022-04-17T11:34:42,521 [INFO ] W-9002-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stderr
2022-04-17T11:34:42,524 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py", line 95, in load_model
2022-04-17T11:34:42,524 [INFO ] W-9005-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stdout
2022-04-17T11:34:42,524 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stdout
2022-04-17T11:34:42,526 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 1 seconds.
2022-04-17T11:34:42,525 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -     service = model_loader.load(model_name, model_dir, handler, gpu,
2022-04-17T11:34:42,517 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 1 seconds.
2022-04-17T11:34:42,526 [INFO ] W-9002-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stdout
2022-04-17T11:34:42,526 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 1 seconds.
2022-04-17T11:34:42,526 [INFO ] W-9002-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stdout
2022-04-17T11:34:42,529 [INFO ] W-9007-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stderr
2022-04-17T11:34:42,529 [INFO ] W-9007-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stderr
2022-04-17T11:34:43,491 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-04-17T11:34:43,491 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-04-17T11:34:43,497 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9008]
2022-04-17T11:34:43,497 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-04-17T11:34:43,497 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9008]
2022-04-17T11:34:43,497 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-04-17T11:34:43,513 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-04-17T11:34:43,513 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-04-17T11:34:43,517 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-04-17T11:34:43,517 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-04-17T11:34:43,519 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-04-17T11:34:43,519 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-04-17T11:34:43,519 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-04-17T11:34:43,519 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-04-17T11:34:43,529 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-04-17T11:34:43,529 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-04-17T11:34:44,945 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:34:44,946 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - [PID]14900
2022-04-17T11:34:44,947 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:44,947 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:34:44,947 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:44,948 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-04-17T11:34:44,948 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:34:44,948 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-04-17T11:34:44,952 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188084952
2022-04-17T11:34:44,952 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9004).
2022-04-17T11:34:44,952 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188084952
2022-04-17T11:34:44,965 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:34:44,972 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - [PID]16896
2022-04-17T11:34:44,973 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:44,973 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:34:44,974 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:34:44,973 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:44,975 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-04-17T11:34:44,975 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-04-17T11:34:44,978 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:34:44,979 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188084979
2022-04-17T11:34:44,979 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188084979
2022-04-17T11:34:44,979 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9003).
2022-04-17T11:34:45,004 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:34:45,008 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:34:45,010 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - [PID]15320
2022-04-17T11:34:45,010 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:45,010 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:34:45,010 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:34:45,010 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:45,011 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9008
2022-04-17T11:34:45,011 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:34:45,011 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9008
2022-04-17T11:34:45,011 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - [PID]3760
2022-04-17T11:34:45,012 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:45,012 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:34:45,012 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:45,014 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-04-17T11:34:45,015 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188085015
2022-04-17T11:34:45,012 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:34:45,015 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188085015
2022-04-17T11:34:45,014 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-04-17T11:34:45,015 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9008).
2022-04-17T11:34:45,023 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188085023
2022-04-17T11:34:45,023 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9007).
2022-04-17T11:34:45,023 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188085023
2022-04-17T11:34:45,040 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:34:45,047 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - [PID]16068
2022-04-17T11:34:45,048 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:45,048 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:34:45,048 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:34:45,048 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:45,050 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-04-17T11:34:45,050 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-04-17T11:34:45,048 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:34:45,052 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:34:45,053 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188085053
2022-04-17T11:34:45,053 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188085053
2022-04-17T11:34:45,053 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9005).
2022-04-17T11:34:45,065 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:34:45,065 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:34:45,070 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - [PID]17524
2022-04-17T11:34:45,072 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:34:45,073 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:45,073 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:45,073 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-04-17T11:34:45,073 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:34:45,073 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-04-17T11:34:45,078 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188085078
2022-04-17T11:34:45,078 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188085078
2022-04-17T11:34:45,078 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9000).
2022-04-17T11:34:45,092 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:34:45,140 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:34:45,141 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - [PID]12168
2022-04-17T11:34:45,142 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:45,142 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:34:45,142 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:45,142 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:34:45,143 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-04-17T11:34:45,143 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-04-17T11:34:45,146 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188085145
2022-04-17T11:34:45,146 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188085145
2022-04-17T11:34:45,146 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9006).
2022-04-17T11:34:45,158 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:34:45,166 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - [PID]588
2022-04-17T11:34:45,167 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:34:45,170 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:45,170 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:34:45,170 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:45,171 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-04-17T11:34:45,171 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:34:45,171 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-04-17T11:34:45,175 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188085174
2022-04-17T11:34:45,175 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188085174
2022-04-17T11:34:45,175 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9002).
2022-04-17T11:34:45,201 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:34:45,477 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:34:45,478 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:34:45,478 [INFO ] nioEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:45,478 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:34:45,478 [INFO ] nioEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:45,478 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:34:45,479 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:45,479 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:45,479 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:34:45,479 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:45,480 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:34:45,480 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:34:45,479 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:45,483 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:45,483 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:34:45,483 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:45,484 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:45,483 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:34:45,484 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:45,484 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:34:45,485 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stderr
2022-04-17T11:34:45,485 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stderr
2022-04-17T11:34:45,485 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stdout
2022-04-17T11:34:45,485 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-04-17T11:34:45,485 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stdout
2022-04-17T11:34:45,486 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-04-17T11:34:45,487 [INFO ] W-9004-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stdout
2022-04-17T11:34:45,487 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 1 seconds.
2022-04-17T11:34:45,487 [INFO ] W-9004-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stdout
2022-04-17T11:34:45,487 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 1 seconds.
2022-04-17T11:34:45,488 [INFO ] W-9004-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stderr
2022-04-17T11:34:45,488 [INFO ] W-9004-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stderr
2022-04-17T11:34:45,511 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:34:45,512 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:34:45,512 [INFO ] nioEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:45,512 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:34:45,512 [INFO ] nioEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:45,513 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:34:45,513 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:45,513 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:45,513 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:34:45,514 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:34:45,513 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:45,514 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:34:45,513 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:45,514 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:34:45,516 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:45,517 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:34:45,516 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:45,518 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:34:45,519 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:45,519 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:45,519 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-04-17T11:34:45,520 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stderr
2022-04-17T11:34:45,520 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stderr
2022-04-17T11:34:45,520 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stdout
2022-04-17T11:34:45,520 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-04-17T11:34:45,520 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stdout
2022-04-17T11:34:45,521 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 1 seconds.
2022-04-17T11:34:45,521 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 790, in exec_module
2022-04-17T11:34:45,521 [INFO ] W-9003-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stdout
2022-04-17T11:34:45,521 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 1 seconds.
2022-04-17T11:34:45,521 [INFO ] W-9003-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stdout
2022-04-17T11:34:45,524 [INFO ] W-9003-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stderr
2022-04-17T11:34:45,524 [INFO ] W-9003-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stderr
2022-04-17T11:34:45,543 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:34:45,544 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:34:45,544 [INFO ] nioEventLoopGroup-5-13 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:45,544 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:34:45,544 [INFO ] nioEventLoopGroup-5-13 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:45,545 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:45,545 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:34:45,545 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:45,545 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:45,545 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:34:45,545 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:45,546 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:34:45,548 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:45,548 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:45,548 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:45,548 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:34:45,548 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:45,548 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:34:45,549 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:34:45,549 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stderr
2022-04-17T11:34:45,549 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stderr
2022-04-17T11:34:45,550 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stdout
2022-04-17T11:34:45,549 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:34:45,550 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stdout
2022-04-17T11:34:45,550 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 1 seconds.
2022-04-17T11:34:45,550 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-04-17T11:34:45,551 [INFO ] W-9005-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stdout
2022-04-17T11:34:45,550 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 1 seconds.
2022-04-17T11:34:45,551 [INFO ] W-9005-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stdout
2022-04-17T11:34:45,552 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:34:45,552 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:34:45,553 [INFO ] nioEventLoopGroup-5-11 org.pytorch.serve.wlm.WorkerThread - 9008 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:45,553 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:34:45,553 [INFO ] nioEventLoopGroup-5-11 org.pytorch.serve.wlm.WorkerThread - 9008 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:45,553 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:34:45,554 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:45,554 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:45,554 [INFO ] W-9005-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stderr
2022-04-17T11:34:45,554 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:34:45,554 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:45,554 [INFO ] W-9005-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stderr
2022-04-17T11:34:45,554 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:34:45,554 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:45,555 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:34:45,557 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:45,557 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:45,557 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:45,557 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:34:45,557 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:45,557 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:34:45,558 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stderr
2022-04-17T11:34:45,558 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:34:45,558 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stderr
2022-04-17T11:34:45,559 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stdout
2022-04-17T11:34:45,558 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-04-17T11:34:45,559 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stdout
2022-04-17T11:34:45,559 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9008 in 1 seconds.
2022-04-17T11:34:45,559 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-04-17T11:34:45,559 [INFO ] W-9008-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stdout
2022-04-17T11:34:45,559 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9008 in 1 seconds.
2022-04-17T11:34:45,559 [INFO ] W-9008-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stdout
2022-04-17T11:34:45,561 [INFO ] W-9008-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stderr
2022-04-17T11:34:45,561 [INFO ] W-9008-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stderr
2022-04-17T11:34:45,564 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:34:45,565 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:34:45,565 [INFO ] nioEventLoopGroup-5-12 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:45,566 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:34:45,565 [INFO ] nioEventLoopGroup-5-12 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:45,566 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:34:45,567 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:34:45,567 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:34:45,567 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:45,568 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:34:45,567 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:45,568 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:34:45,568 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:34:45,569 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:34:45,569 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:34:45,569 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:34:45,570 [INFO ] nioEventLoopGroup-5-14 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:45,569 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-04-17T11:34:45,570 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:34:45,570 [INFO ] nioEventLoopGroup-5-14 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:45,570 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:34:45,571 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:45,570 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-04-17T11:34:45,568 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:45,571 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 790, in exec_module
2022-04-17T11:34:45,572 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-04-17T11:34:45,571 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:45,571 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:34:45,572 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:45,568 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:45,572 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\AppData\Local\Temp\models\b31932e975844d80809048017333897f\handler.py", line 3, in <module>
2022-04-17T11:34:45,575 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:45,575 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier as ImgClsfr
2022-04-17T11:34:45,573 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:34:45,572 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:45,575 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\torch_handler\image_classifier.py", line 8, in <module>
2022-04-17T11:34:45,577 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:45,577 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:34:45,575 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:45,578 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:45,577 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-04-17T11:34:45,577 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:45,579 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:45,578 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:45,578 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:34:45,579 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stderr
2022-04-17T11:34:45,579 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:45,580 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stderr
2022-04-17T11:34:45,578 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\torch_handler\vision_handler.py", line 11, in <module>
2022-04-17T11:34:45,579 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:34:45,580 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-04-17T11:34:45,579 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stderr
2022-04-17T11:34:45,580 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stdout
2022-04-17T11:34:45,580 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stderr
2022-04-17T11:34:45,581 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stdout
2022-04-17T11:34:45,580 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:34:45,580 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stdout
2022-04-17T11:34:45,581 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 1 seconds.
2022-04-17T11:34:45,580 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2022-04-17T11:34:45,581 [INFO ] W-9007-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stdout
2022-04-17T11:34:45,581 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-04-17T11:34:45,581 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stdout
2022-04-17T11:34:45,582 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-04-17T11:34:45,581 [INFO ] W-9007-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stdout
2022-04-17T11:34:45,581 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 1 seconds.
2022-04-17T11:34:45,583 [INFO ] W-9000-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stderr
2022-04-17T11:34:45,583 [INFO ] W-9007-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stderr
2022-04-17T11:34:45,582 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-04-17T11:34:45,582 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-04-17T11:34:45,583 [INFO ] W-9000-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stdout
2022-04-17T11:34:45,583 [INFO ] W-9007-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stderr
2022-04-17T11:34:45,583 [INFO ] W-9000-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stderr
2022-04-17T11:34:45,583 [INFO ] W-9000-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stdout
2022-04-17T11:34:45,618 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:34:45,618 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:34:45,618 [INFO ] nioEventLoopGroup-5-15 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:45,619 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:34:45,618 [INFO ] nioEventLoopGroup-5-15 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:45,619 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:45,619 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:34:45,619 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:45,619 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:34:45,620 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:45,620 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:34:45,620 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:45,620 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:34:45,622 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:45,622 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:45,622 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:34:45,623 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:45,623 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:45,623 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:34:45,623 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stderr
2022-04-17T11:34:45,623 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stderr
2022-04-17T11:34:45,623 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stdout
2022-04-17T11:34:45,623 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:34:45,623 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stdout
2022-04-17T11:34:45,624 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 1 seconds.
2022-04-17T11:34:45,624 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-04-17T11:34:45,624 [INFO ] W-9006-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stdout
2022-04-17T11:34:45,624 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 1 seconds.
2022-04-17T11:34:45,624 [INFO ] W-9006-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stdout
2022-04-17T11:34:45,626 [INFO ] W-9006-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stderr
2022-04-17T11:34:45,626 [INFO ] W-9006-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stderr
2022-04-17T11:34:45,636 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:34:45,637 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:34:45,637 [INFO ] nioEventLoopGroup-5-16 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:45,637 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:34:45,637 [INFO ] nioEventLoopGroup-5-16 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:45,637 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:45,637 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:34:45,637 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:45,638 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:34:45,638 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:45,638 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:34:45,638 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:45,638 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:34:45,640 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:45,640 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:45,640 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:45,640 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:34:45,640 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:45,640 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:34:45,641 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stderr
2022-04-17T11:34:45,641 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:34:45,641 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stderr
2022-04-17T11:34:45,641 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stdout
2022-04-17T11:34:45,641 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-04-17T11:34:45,641 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stdout
2022-04-17T11:34:45,642 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 1 seconds.
2022-04-17T11:34:45,642 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-04-17T11:34:45,642 [INFO ] W-9002-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stdout
2022-04-17T11:34:45,642 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 1 seconds.
2022-04-17T11:34:45,642 [INFO ] W-9002-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stdout
2022-04-17T11:34:45,643 [INFO ] W-9002-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stderr
2022-04-17T11:34:45,643 [INFO ] W-9002-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stderr
2022-04-17T11:34:46,488 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-04-17T11:34:46,488 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-04-17T11:34:46,524 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-04-17T11:34:46,524 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-04-17T11:34:46,552 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-04-17T11:34:46,552 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-04-17T11:34:46,562 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9008]
2022-04-17T11:34:46,562 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9008]
2022-04-17T11:34:46,584 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-04-17T11:34:46,584 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-04-17T11:34:46,584 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-04-17T11:34:46,584 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-04-17T11:34:46,627 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-04-17T11:34:46,627 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-04-17T11:34:46,645 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-04-17T11:34:46,645 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-04-17T11:34:48,259 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:34:48,261 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - [PID]15928
2022-04-17T11:34:48,261 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:48,261 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:34:48,261 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:48,262 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-04-17T11:34:48,262 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:34:48,262 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-04-17T11:34:48,266 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188088266
2022-04-17T11:34:48,266 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188088266
2022-04-17T11:34:48,267 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9004).
2022-04-17T11:34:48,295 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:34:48,305 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:34:48,307 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - [PID]17372
2022-04-17T11:34:48,308 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:48,308 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:34:48,308 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:48,309 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-04-17T11:34:48,308 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:34:48,309 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-04-17T11:34:48,313 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188088312
2022-04-17T11:34:48,312 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9003).
2022-04-17T11:34:48,313 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188088312
2022-04-17T11:34:48,340 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:34:48,341 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:34:48,342 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - [PID]17016
2022-04-17T11:34:48,343 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:48,343 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:34:48,343 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:48,344 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-04-17T11:34:48,343 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:34:48,344 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-04-17T11:34:48,348 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188088348
2022-04-17T11:34:48,348 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188088348
2022-04-17T11:34:48,348 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9005).
2022-04-17T11:34:48,390 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:34:48,406 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:34:48,408 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - [PID]17836
2022-04-17T11:34:48,409 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:48,409 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:34:48,409 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:48,410 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:34:48,411 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9008
2022-04-17T11:34:48,411 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9008
2022-04-17T11:34:48,415 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188088415
2022-04-17T11:34:48,414 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9008).
2022-04-17T11:34:48,415 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188088415
2022-04-17T11:34:48,451 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:34:48,474 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:34:48,476 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - [PID]13492
2022-04-17T11:34:48,478 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:48,477 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:34:48,478 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:48,478 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-04-17T11:34:48,478 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:34:48,478 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-04-17T11:34:48,482 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188088482
2022-04-17T11:34:48,482 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188088482
2022-04-17T11:34:48,482 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9000).
2022-04-17T11:34:48,514 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:34:48,575 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:34:48,577 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - [PID]2484
2022-04-17T11:34:48,578 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:48,577 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:34:48,578 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:48,578 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-04-17T11:34:48,578 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:34:48,578 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-04-17T11:34:48,582 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188088582
2022-04-17T11:34:48,582 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188088582
2022-04-17T11:34:48,582 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9007).
2022-04-17T11:34:48,608 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:34:48,610 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - [PID]11880
2022-04-17T11:34:48,611 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:48,611 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:34:48,611 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:48,611 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:34:48,612 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-04-17T11:34:48,612 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-04-17T11:34:48,618 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:34:48,620 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:34:48,620 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - [PID]11860
2022-04-17T11:34:48,621 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9002).
2022-04-17T11:34:48,621 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188088621
2022-04-17T11:34:48,621 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:34:48,622 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:48,621 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188088621
2022-04-17T11:34:48,622 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:48,628 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-04-17T11:34:48,622 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:34:48,628 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-04-17T11:34:48,632 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9006).
2022-04-17T11:34:48,636 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188088636
2022-04-17T11:34:48,636 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188088636
2022-04-17T11:34:48,670 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:34:48,670 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:34:48,964 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:34:48,965 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:34:48,965 [INFO ] nioEventLoopGroup-5-17 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:48,965 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:34:48,965 [INFO ] nioEventLoopGroup-5-17 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:48,966 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:48,966 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:48,966 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:34:48,967 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:48,967 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:48,969 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:48,967 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:34:48,970 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:34:48,969 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:48,970 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:48,970 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:34:48,970 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:48,971 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stderr
2022-04-17T11:34:48,971 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:34:48,971 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stderr
2022-04-17T11:34:48,972 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stdout
2022-04-17T11:34:48,971 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:34:48,972 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stdout
2022-04-17T11:34:48,973 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 2 seconds.
2022-04-17T11:34:48,972 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:34:48,973 [INFO ] W-9004-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stdout
2022-04-17T11:34:48,973 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 2 seconds.
2022-04-17T11:34:48,973 [INFO ] W-9004-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stdout
2022-04-17T11:34:48,978 [INFO ] W-9004-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stderr
2022-04-17T11:34:48,978 [INFO ] W-9004-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stderr
2022-04-17T11:34:49,011 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:34:49,012 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:34:49,012 [INFO ] nioEventLoopGroup-5-18 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:49,013 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:34:49,012 [INFO ] nioEventLoopGroup-5-18 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:49,013 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:34:49,014 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:49,014 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:34:49,014 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:49,014 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:34:49,014 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:49,015 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:34:49,014 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:49,015 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:34:49,018 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:49,018 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:49,018 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:34:49,019 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:49,019 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:34:49,019 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:49,019 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-04-17T11:34:49,020 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stderr
2022-04-17T11:34:49,020 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stderr
2022-04-17T11:34:49,020 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stdout
2022-04-17T11:34:49,020 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-04-17T11:34:49,020 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stdout
2022-04-17T11:34:49,021 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 2 seconds.
2022-04-17T11:34:49,021 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 790, in exec_module
2022-04-17T11:34:49,022 [INFO ] W-9003-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stdout
2022-04-17T11:34:49,021 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 2 seconds.
2022-04-17T11:34:49,022 [INFO ] W-9003-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stdout
2022-04-17T11:34:49,024 [INFO ] W-9003-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stderr
2022-04-17T11:34:49,024 [INFO ] W-9003-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stderr
2022-04-17T11:34:49,058 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:34:49,058 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:34:49,059 [INFO ] nioEventLoopGroup-5-19 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:49,059 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:34:49,059 [INFO ] nioEventLoopGroup-5-19 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:49,060 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:49,059 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:34:49,060 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:49,060 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:34:49,060 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:49,061 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:34:49,060 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:49,064 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:49,061 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:34:49,064 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:49,064 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:49,064 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:34:49,065 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:34:49,064 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:49,065 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:34:49,066 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stderr
2022-04-17T11:34:49,066 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-04-17T11:34:49,066 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-04-17T11:34:49,066 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stderr
2022-04-17T11:34:49,068 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stdout
2022-04-17T11:34:49,067 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 790, in exec_module
2022-04-17T11:34:49,068 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stdout
2022-04-17T11:34:49,068 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 2 seconds.
2022-04-17T11:34:49,068 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-04-17T11:34:49,069 [INFO ] W-9005-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stdout
2022-04-17T11:34:49,068 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 2 seconds.
2022-04-17T11:34:49,069 [INFO ] W-9005-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stdout
2022-04-17T11:34:49,072 [INFO ] W-9005-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stderr
2022-04-17T11:34:49,072 [INFO ] W-9005-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stderr
2022-04-17T11:34:49,101 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:34:49,102 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:34:49,102 [INFO ] nioEventLoopGroup-5-20 org.pytorch.serve.wlm.WorkerThread - 9008 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:49,103 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:34:49,102 [INFO ] nioEventLoopGroup-5-20 org.pytorch.serve.wlm.WorkerThread - 9008 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:49,104 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:49,103 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:34:49,104 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:49,104 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:34:49,104 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:49,104 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:34:49,104 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:49,109 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:49,105 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:34:49,109 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:49,109 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:49,109 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:34:49,109 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:49,110 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stderr
2022-04-17T11:34:49,110 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:34:49,110 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stderr
2022-04-17T11:34:49,111 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stdout
2022-04-17T11:34:49,110 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:34:49,111 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stdout
2022-04-17T11:34:49,112 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9008 in 2 seconds.
2022-04-17T11:34:49,111 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-04-17T11:34:49,112 [INFO ] W-9008-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stdout
2022-04-17T11:34:49,112 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9008 in 2 seconds.
2022-04-17T11:34:49,112 [INFO ] W-9008-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stdout
2022-04-17T11:34:49,114 [INFO ] W-9008-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stderr
2022-04-17T11:34:49,114 [INFO ] W-9008-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stderr
2022-04-17T11:34:49,140 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:34:49,140 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:34:49,140 [INFO ] nioEventLoopGroup-5-21 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:49,141 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:34:49,140 [INFO ] nioEventLoopGroup-5-21 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:49,141 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:34:49,142 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:49,142 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:34:49,142 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:49,142 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:34:49,143 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:49,143 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:34:49,143 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:49,143 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:34:49,146 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:49,146 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:49,146 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:34:49,147 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:49,147 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:49,147 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:34:49,148 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stderr
2022-04-17T11:34:49,147 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-04-17T11:34:49,148 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stderr
2022-04-17T11:34:49,149 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stdout
2022-04-17T11:34:49,148 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-04-17T11:34:49,149 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stdout
2022-04-17T11:34:49,149 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2022-04-17T11:34:49,149 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 790, in exec_module
2022-04-17T11:34:49,150 [INFO ] W-9000-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stdout
2022-04-17T11:34:49,149 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2022-04-17T11:34:49,150 [INFO ] W-9000-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stdout
2022-04-17T11:34:49,152 [INFO ] W-9000-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stderr
2022-04-17T11:34:49,152 [INFO ] W-9000-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stderr
2022-04-17T11:34:49,198 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:34:49,199 [INFO ] nioEventLoopGroup-5-22 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:49,199 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:34:49,199 [INFO ] nioEventLoopGroup-5-22 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:49,200 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:49,200 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:34:49,200 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:49,201 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:34:49,201 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:49,201 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:34:49,201 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:49,202 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:34:49,204 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:49,204 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:49,205 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:49,204 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:34:49,205 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:49,205 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stderr
2022-04-17T11:34:49,205 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:34:49,205 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stderr
2022-04-17T11:34:49,206 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stdout
2022-04-17T11:34:49,206 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:34:49,206 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stdout
2022-04-17T11:34:49,207 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 2 seconds.
2022-04-17T11:34:49,206 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:34:49,207 [INFO ] W-9007-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stdout
2022-04-17T11:34:49,207 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 2 seconds.
2022-04-17T11:34:49,207 [INFO ] W-9007-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stdout
2022-04-17T11:34:49,209 [INFO ] W-9007-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stderr
2022-04-17T11:34:49,209 [INFO ] W-9007-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stderr
2022-04-17T11:34:49,234 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:34:49,235 [INFO ] nioEventLoopGroup-5-24 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:49,235 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:34:49,235 [INFO ] nioEventLoopGroup-5-24 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:49,236 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:49,235 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:34:49,236 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:49,236 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:34:49,236 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:49,237 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:34:49,236 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:49,239 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:49,237 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:34:49,239 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:49,240 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:49,239 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:34:49,240 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:49,240 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stderr
2022-04-17T11:34:49,240 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:34:49,240 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stderr
2022-04-17T11:34:49,241 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stdout
2022-04-17T11:34:49,240 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:34:49,241 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stdout
2022-04-17T11:34:49,241 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 2 seconds.
2022-04-17T11:34:49,241 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:34:49,242 [INFO ] W-9006-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stdout
2022-04-17T11:34:49,241 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 2 seconds.
2022-04-17T11:34:49,242 [INFO ] W-9006-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stdout
2022-04-17T11:34:49,244 [INFO ] W-9006-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stderr
2022-04-17T11:34:49,244 [INFO ] W-9006-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stderr
2022-04-17T11:34:49,248 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:34:49,248 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:34:49,249 [INFO ] nioEventLoopGroup-5-23 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:49,249 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:34:49,249 [INFO ] nioEventLoopGroup-5-23 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:49,250 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:49,250 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:34:49,250 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:49,250 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:34:49,251 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:49,251 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:34:49,251 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:49,251 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:34:49,253 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:49,253 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:49,254 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:49,253 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:34:49,254 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:49,254 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stderr
2022-04-17T11:34:49,254 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:34:49,254 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stderr
2022-04-17T11:34:49,255 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stdout
2022-04-17T11:34:49,255 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:34:49,255 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stdout
2022-04-17T11:34:49,256 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 2 seconds.
2022-04-17T11:34:49,255 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-04-17T11:34:49,256 [INFO ] W-9002-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stdout
2022-04-17T11:34:49,256 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 2 seconds.
2022-04-17T11:34:49,256 [INFO ] W-9002-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stdout
2022-04-17T11:34:49,258 [INFO ] W-9002-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stderr
2022-04-17T11:34:49,258 [INFO ] W-9002-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stderr
2022-04-17T11:34:50,975 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-04-17T11:34:50,975 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-04-17T11:34:51,022 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-04-17T11:34:51,022 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-04-17T11:34:51,070 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-04-17T11:34:51,070 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-04-17T11:34:51,113 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9008]
2022-04-17T11:34:51,113 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9008]
2022-04-17T11:34:51,151 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-04-17T11:34:51,151 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-04-17T11:34:51,210 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-04-17T11:34:51,210 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-04-17T11:34:51,243 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-04-17T11:34:51,243 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-04-17T11:34:51,258 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-04-17T11:34:51,258 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-04-17T11:34:52,399 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:34:52,400 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - [PID]7864
2022-04-17T11:34:52,401 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:52,401 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:34:52,401 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:52,402 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:34:52,402 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-04-17T11:34:52,402 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-04-17T11:34:52,404 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188092404
2022-04-17T11:34:52,404 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188092404
2022-04-17T11:34:52,405 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9004).
2022-04-17T11:34:52,406 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:34:52,519 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:34:52,520 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - [PID]14408
2022-04-17T11:34:52,521 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:52,521 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:34:52,521 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:52,522 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-04-17T11:34:52,522 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:34:52,522 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-04-17T11:34:52,524 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188092524
2022-04-17T11:34:52,524 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188092524
2022-04-17T11:34:52,525 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9003).
2022-04-17T11:34:52,526 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:34:52,621 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:34:52,623 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - [PID]14356
2022-04-17T11:34:52,623 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:52,623 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:34:52,623 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:52,624 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-04-17T11:34:52,624 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:34:52,624 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-04-17T11:34:52,627 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188092627
2022-04-17T11:34:52,627 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188092627
2022-04-17T11:34:52,627 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9005).
2022-04-17T11:34:52,628 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:34:52,713 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:34:52,715 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - [PID]15972
2022-04-17T11:34:52,715 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:34:52,716 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:52,716 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:34:52,716 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:52,717 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9008
2022-04-17T11:34:52,717 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9008
2022-04-17T11:34:52,720 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188092720
2022-04-17T11:34:52,720 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9008).
2022-04-17T11:34:52,720 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188092720
2022-04-17T11:34:52,721 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:34:52,745 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:34:52,747 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - [PID]15800
2022-04-17T11:34:52,748 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:52,748 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:34:52,748 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:52,748 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:34:52,749 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-04-17T11:34:52,749 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-04-17T11:34:52,751 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188092751
2022-04-17T11:34:52,751 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9000).
2022-04-17T11:34:52,751 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188092751
2022-04-17T11:34:52,753 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:34:52,863 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:34:52,865 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - [PID]3100
2022-04-17T11:34:52,866 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:52,866 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:34:52,866 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:52,867 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-04-17T11:34:52,866 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:34:52,867 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-04-17T11:34:52,870 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188092870
2022-04-17T11:34:52,870 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9007).
2022-04-17T11:34:52,870 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188092870
2022-04-17T11:34:52,872 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:34:52,889 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:34:52,891 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - [PID]16696
2022-04-17T11:34:52,892 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:52,892 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:34:52,892 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:52,893 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-04-17T11:34:52,892 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:34:52,893 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-04-17T11:34:52,895 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188092895
2022-04-17T11:34:52,895 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9002).
2022-04-17T11:34:52,895 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188092895
2022-04-17T11:34:52,897 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:34:52,948 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:34:52,948 [INFO ] nioEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:52,948 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:34:52,948 [INFO ] nioEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:52,949 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:34:52,950 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:52,950 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:34:52,950 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:52,950 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:34:52,950 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:34:52,952 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:34:52,952 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:52,953 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:34:52,953 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - [PID]16068
2022-04-17T11:34:52,954 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:52,953 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:34:52,952 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:52,954 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:52,957 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-04-17T11:34:52,954 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:34:52,957 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-04-17T11:34:52,956 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:34:52,957 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:34:52,958 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:34:52,959 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-04-17T11:34:52,959 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-04-17T11:34:52,960 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9006).
2022-04-17T11:34:52,960 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188092960
2022-04-17T11:34:52,960 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 790, in exec_module
2022-04-17T11:34:52,960 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188092960
2022-04-17T11:34:52,961 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:52,961 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-04-17T11:34:52,961 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:52,962 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:52,962 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\AppData\Local\Temp\models\b31932e975844d80809048017333897f\handler.py", line 3, in <module>
2022-04-17T11:34:52,962 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:34:52,962 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:52,965 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stderr
2022-04-17T11:34:52,963 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier as ImgClsfr
2022-04-17T11:34:52,965 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\torch_handler\image_classifier.py", line 8, in <module>
2022-04-17T11:34:52,965 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stderr
2022-04-17T11:34:52,966 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stdout
2022-04-17T11:34:52,966 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-04-17T11:34:52,966 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stdout
2022-04-17T11:34:52,967 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 3 seconds.
2022-04-17T11:34:52,967 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\torch_handler\vision_handler.py", line 11, in <module>
2022-04-17T11:34:52,967 [INFO ] W-9004-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stdout
2022-04-17T11:34:52,967 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 3 seconds.
2022-04-17T11:34:52,967 [INFO ] W-9004-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stdout
2022-04-17T11:34:52,972 [INFO ] W-9004-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stderr
2022-04-17T11:34:52,972 [INFO ] W-9004-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stderr
2022-04-17T11:34:53,092 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:34:53,092 [INFO ] nioEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:53,092 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:34:53,092 [INFO ] nioEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:53,093 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:34:53,093 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:53,093 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:53,093 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:34:53,094 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:53,094 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:34:53,094 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:53,097 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:53,095 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:34:53,097 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:53,097 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:53,097 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:34:53,097 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:53,098 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stderr
2022-04-17T11:34:53,098 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:34:53,098 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stderr
2022-04-17T11:34:53,099 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stdout
2022-04-17T11:34:53,098 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:34:53,099 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stdout
2022-04-17T11:34:53,100 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 3 seconds.
2022-04-17T11:34:53,099 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:34:53,100 [INFO ] W-9003-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stdout
2022-04-17T11:34:53,100 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 3 seconds.
2022-04-17T11:34:53,100 [INFO ] W-9003-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stdout
2022-04-17T11:34:53,102 [INFO ] W-9003-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stderr
2022-04-17T11:34:53,102 [INFO ] W-9003-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stderr
2022-04-17T11:34:53,194 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:34:53,195 [INFO ] nioEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:53,195 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:34:53,195 [INFO ] nioEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:53,195 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:53,195 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:34:53,195 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:53,196 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:34:53,196 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:53,196 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:34:53,196 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:53,197 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:34:53,199 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:53,199 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:34:53,199 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:53,200 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:53,200 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:34:53,200 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:53,201 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stderr
2022-04-17T11:34:53,200 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:34:53,201 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stderr
2022-04-17T11:34:53,201 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stdout
2022-04-17T11:34:53,201 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:34:53,201 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stdout
2022-04-17T11:34:53,202 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 3 seconds.
2022-04-17T11:34:53,202 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-04-17T11:34:53,202 [INFO ] W-9005-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stdout
2022-04-17T11:34:53,202 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 3 seconds.
2022-04-17T11:34:53,202 [INFO ] W-9005-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stdout
2022-04-17T11:34:53,204 [INFO ] W-9005-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stderr
2022-04-17T11:34:53,204 [INFO ] W-9005-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stderr
2022-04-17T11:34:53,251 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:34:53,252 [INFO ] nioEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9008 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:53,252 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:34:53,252 [INFO ] nioEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9008 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:53,253 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:53,252 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:34:53,253 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:53,253 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:34:53,253 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:53,254 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:34:53,253 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:53,254 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:34:53,256 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:53,256 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:53,257 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:53,256 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:34:53,257 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:53,257 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:34:53,258 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stderr
2022-04-17T11:34:53,258 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:34:53,258 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:34:53,259 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-04-17T11:34:53,259 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-04-17T11:34:53,258 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stderr
2022-04-17T11:34:53,261 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stdout
2022-04-17T11:34:53,260 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 790, in exec_module
2022-04-17T11:34:53,261 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stdout
2022-04-17T11:34:53,261 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9008 in 3 seconds.
2022-04-17T11:34:53,261 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-04-17T11:34:53,262 [INFO ] W-9008-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stdout
2022-04-17T11:34:53,261 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9008 in 3 seconds.
2022-04-17T11:34:53,262 [INFO ] W-9008-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stdout
2022-04-17T11:34:53,262 [INFO ] W-9008-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stderr
2022-04-17T11:34:53,262 [INFO ] W-9008-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stderr
2022-04-17T11:34:53,288 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:34:53,288 [INFO ] nioEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:53,288 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:34:53,288 [INFO ] nioEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:53,289 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:53,288 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:34:53,289 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:53,289 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:34:53,289 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:53,290 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:34:53,289 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:53,292 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:53,290 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:34:53,292 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:53,293 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:53,292 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:34:53,293 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:53,293 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:34:53,294 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stderr
2022-04-17T11:34:53,294 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stderr
2022-04-17T11:34:53,294 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stdout
2022-04-17T11:34:53,294 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:34:53,294 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stdout
2022-04-17T11:34:53,295 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2022-04-17T11:34:53,294 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:34:53,295 [INFO ] W-9000-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stdout
2022-04-17T11:34:53,295 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2022-04-17T11:34:53,295 [INFO ] W-9000-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stdout
2022-04-17T11:34:53,296 [INFO ] W-9000-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stderr
2022-04-17T11:34:53,296 [INFO ] W-9000-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stderr
2022-04-17T11:34:53,373 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:34:53,374 [INFO ] nioEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:53,374 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:34:53,374 [INFO ] nioEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:53,374 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:53,374 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:34:53,374 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:53,375 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:34:53,375 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:53,375 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:34:53,375 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:53,377 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:53,376 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:34:53,377 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:53,378 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:53,378 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:34:53,378 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:53,378 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:34:53,379 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stderr
2022-04-17T11:34:53,379 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stderr
2022-04-17T11:34:53,379 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stdout
2022-04-17T11:34:53,379 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:34:53,379 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stdout
2022-04-17T11:34:53,380 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 3 seconds.
2022-04-17T11:34:53,379 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:34:53,380 [INFO ] W-9007-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stdout
2022-04-17T11:34:53,380 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 3 seconds.
2022-04-17T11:34:53,380 [INFO ] W-9007-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stdout
2022-04-17T11:34:53,381 [INFO ] W-9007-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stderr
2022-04-17T11:34:53,381 [INFO ] W-9007-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stderr
2022-04-17T11:34:53,387 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:34:53,387 [INFO ] nioEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:53,387 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:34:53,387 [INFO ] nioEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:53,388 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:53,388 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:34:53,388 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:53,388 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:34:53,389 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:53,389 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:34:53,389 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:53,390 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:34:53,392 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:53,392 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:53,392 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:53,392 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:34:53,392 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:53,393 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stderr
2022-04-17T11:34:53,393 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:34:53,393 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stderr
2022-04-17T11:34:53,394 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stdout
2022-04-17T11:34:53,393 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:34:53,394 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stdout
2022-04-17T11:34:53,394 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 3 seconds.
2022-04-17T11:34:53,394 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:34:53,395 [INFO ] W-9002-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stdout
2022-04-17T11:34:53,394 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 3 seconds.
2022-04-17T11:34:53,395 [INFO ] W-9002-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stdout
2022-04-17T11:34:53,396 [INFO ] W-9002-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stderr
2022-04-17T11:34:53,396 [INFO ] W-9002-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stderr
2022-04-17T11:34:53,437 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:34:53,437 [INFO ] nioEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:53,437 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:34:53,437 [INFO ] nioEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:53,438 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:53,438 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:34:53,438 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:53,439 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:53,438 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:34:53,439 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:53,439 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:34:53,441 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:53,441 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:53,442 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:53,441 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:34:53,442 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:53,442 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:34:53,443 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stderr
2022-04-17T11:34:53,443 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:34:53,443 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stderr
2022-04-17T11:34:53,443 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stdout
2022-04-17T11:34:53,443 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:34:53,443 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stdout
2022-04-17T11:34:53,444 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:34:53,444 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 3 seconds.
2022-04-17T11:34:53,444 [INFO ] W-9006-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stdout
2022-04-17T11:34:53,444 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 3 seconds.
2022-04-17T11:34:53,444 [INFO ] W-9006-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stdout
2022-04-17T11:34:53,446 [INFO ] W-9006-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stderr
2022-04-17T11:34:53,446 [INFO ] W-9006-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stderr
2022-04-17T11:34:55,969 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-04-17T11:34:55,969 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-04-17T11:34:56,100 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-04-17T11:34:56,100 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-04-17T11:34:56,203 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-04-17T11:34:56,203 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-04-17T11:34:56,263 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9008]
2022-04-17T11:34:56,263 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9008]
2022-04-17T11:34:56,297 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-04-17T11:34:56,297 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-04-17T11:34:56,380 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-04-17T11:34:56,380 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-04-17T11:34:56,396 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-04-17T11:34:56,396 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-04-17T11:34:56,448 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-04-17T11:34:56,448 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-04-17T11:34:57,100 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:34:57,101 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - [PID]11704
2022-04-17T11:34:57,102 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:57,102 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:34:57,102 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:57,102 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:34:57,103 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-04-17T11:34:57,103 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-04-17T11:34:57,106 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188097106
2022-04-17T11:34:57,105 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9004).
2022-04-17T11:34:57,106 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188097106
2022-04-17T11:34:57,107 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:34:57,288 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:34:57,289 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - [PID]15544
2022-04-17T11:34:57,290 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:57,290 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:34:57,290 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:34:57,290 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:57,291 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-04-17T11:34:57,291 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-04-17T11:34:57,294 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188097294
2022-04-17T11:34:57,294 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188097294
2022-04-17T11:34:57,294 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9003).
2022-04-17T11:34:57,295 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:34:57,523 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:34:57,524 [INFO ] nioEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:57,523 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:34:57,524 [INFO ] nioEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:57,524 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:57,524 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:34:57,524 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:57,524 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:34:57,525 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:57,525 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:34:57,525 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:57,525 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:34:57,528 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:57,528 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:34:57,528 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:57,529 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:57,528 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:34:57,529 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:57,529 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:34:57,530 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:34:57,530 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stderr
2022-04-17T11:34:57,530 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-04-17T11:34:57,530 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stderr
2022-04-17T11:34:57,530 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-04-17T11:34:57,531 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stdout
2022-04-17T11:34:57,531 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stdout
2022-04-17T11:34:57,532 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 5 seconds.
2022-04-17T11:34:57,531 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 790, in exec_module
2022-04-17T11:34:57,532 [INFO ] W-9004-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stdout
2022-04-17T11:34:57,532 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 5 seconds.
2022-04-17T11:34:57,532 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:34:57,532 [INFO ] W-9004-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stdout
2022-04-17T11:34:57,534 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - [PID]7420
2022-04-17T11:34:57,535 [INFO ] W-9004-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stderr
2022-04-17T11:34:57,535 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:57,535 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:34:57,535 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:34:57,535 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:57,535 [INFO ] W-9004-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stderr
2022-04-17T11:34:57,536 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-04-17T11:34:57,536 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-04-17T11:34:57,538 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188097538
2022-04-17T11:34:57,538 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9005).
2022-04-17T11:34:57,538 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188097538
2022-04-17T11:34:57,540 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:34:57,607 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:34:57,608 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - [PID]812
2022-04-17T11:34:57,609 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:57,609 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:34:57,609 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:57,610 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9008
2022-04-17T11:34:57,609 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:34:57,610 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9008
2022-04-17T11:34:57,612 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188097612
2022-04-17T11:34:57,612 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188097612
2022-04-17T11:34:57,612 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9008).
2022-04-17T11:34:57,614 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:34:57,633 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:34:57,635 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - [PID]15548
2022-04-17T11:34:57,636 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:57,636 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:34:57,636 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:57,637 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-04-17T11:34:57,636 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:34:57,637 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-04-17T11:34:57,639 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188097639
2022-04-17T11:34:57,639 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188097639
2022-04-17T11:34:57,639 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9000).
2022-04-17T11:34:57,640 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:34:57,747 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:34:57,747 [INFO ] nioEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:57,747 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:34:57,747 [INFO ] nioEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:57,748 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:57,748 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:34:57,748 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:57,749 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:34:57,749 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:57,750 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:34:57,749 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:57,750 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:34:57,752 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:57,752 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:57,753 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:57,752 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:34:57,753 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:57,753 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stderr
2022-04-17T11:34:57,753 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:34:57,753 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stderr
2022-04-17T11:34:57,754 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stdout
2022-04-17T11:34:57,754 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:34:57,754 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stdout
2022-04-17T11:34:57,755 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 5 seconds.
2022-04-17T11:34:57,754 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:34:57,755 [INFO ] W-9003-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stdout
2022-04-17T11:34:57,755 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 5 seconds.
2022-04-17T11:34:57,755 [INFO ] W-9003-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stdout
2022-04-17T11:34:57,758 [INFO ] W-9003-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stderr
2022-04-17T11:34:57,758 [INFO ] W-9003-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stderr
2022-04-17T11:34:57,796 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:34:57,798 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - [PID]17728
2022-04-17T11:34:57,799 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:57,799 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:34:57,799 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:57,800 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-04-17T11:34:57,799 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:34:57,800 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-04-17T11:34:57,802 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188097802
2022-04-17T11:34:57,802 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188097802
2022-04-17T11:34:57,803 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9007).
2022-04-17T11:34:57,804 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:34:57,806 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:34:57,807 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - [PID]1956
2022-04-17T11:34:57,808 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:57,808 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:34:57,808 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:57,809 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:34:57,809 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-04-17T11:34:57,809 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-04-17T11:34:57,812 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188097812
2022-04-17T11:34:57,812 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188097812
2022-04-17T11:34:57,812 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9002).
2022-04-17T11:34:57,813 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:34:57,843 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:34:57,844 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - [PID]8680
2022-04-17T11:34:57,845 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:57,845 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:34:57,845 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:34:57,846 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-04-17T11:34:57,845 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:34:57,846 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-04-17T11:34:57,847 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188097847
2022-04-17T11:34:57,847 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188097847
2022-04-17T11:34:57,848 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9006).
2022-04-17T11:34:57,849 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:34:58,023 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:34:58,023 [INFO ] nioEventLoopGroup-5-11 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:58,023 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:34:58,023 [INFO ] nioEventLoopGroup-5-11 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:58,024 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:34:58,025 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:58,024 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:34:58,025 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:58,025 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:34:58,025 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:58,026 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:34:58,025 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:58,028 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:58,026 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:34:58,028 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:58,029 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:58,028 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:34:58,029 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:58,029 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:34:58,030 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stderr
2022-04-17T11:34:58,030 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stderr
2022-04-17T11:34:58,030 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stdout
2022-04-17T11:34:58,030 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:34:58,030 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stdout
2022-04-17T11:34:58,031 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 5 seconds.
2022-04-17T11:34:58,031 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-04-17T11:34:58,031 [INFO ] W-9005-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stdout
2022-04-17T11:34:58,031 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 5 seconds.
2022-04-17T11:34:58,031 [INFO ] W-9005-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stdout
2022-04-17T11:34:58,033 [INFO ] W-9005-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stderr
2022-04-17T11:34:58,033 [INFO ] W-9005-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stderr
2022-04-17T11:34:58,079 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:34:58,079 [INFO ] nioEventLoopGroup-5-12 org.pytorch.serve.wlm.WorkerThread - 9008 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:58,079 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:34:58,079 [INFO ] nioEventLoopGroup-5-12 org.pytorch.serve.wlm.WorkerThread - 9008 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:58,080 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:58,080 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:34:58,080 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:58,080 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:34:58,081 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:58,081 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:34:58,081 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:58,081 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:34:58,084 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:58,084 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:34:58,084 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:58,084 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:58,084 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:34:58,084 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:58,085 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stderr
2022-04-17T11:34:58,085 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:34:58,085 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stderr
2022-04-17T11:34:58,086 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stdout
2022-04-17T11:34:58,086 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:34:58,086 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stdout
2022-04-17T11:34:58,087 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9008 in 5 seconds.
2022-04-17T11:34:58,086 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-04-17T11:34:58,087 [INFO ] W-9008-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stdout
2022-04-17T11:34:58,087 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9008 in 5 seconds.
2022-04-17T11:34:58,087 [INFO ] W-9008-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stdout
2022-04-17T11:34:58,088 [INFO ] W-9008-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stderr
2022-04-17T11:34:58,088 [INFO ] W-9008-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stderr
2022-04-17T11:34:58,119 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:34:58,120 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:34:58,120 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:34:58,121 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:34:58,122 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:34:58,122 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:34:58,122 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:34:58,123 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:34:58,123 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:34:58,124 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:34:58,124 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-04-17T11:34:58,124 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-04-17T11:34:58,125 [INFO ] nioEventLoopGroup-5-13 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:58,125 [INFO ] nioEventLoopGroup-5-13 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:58,125 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 790, in exec_module
2022-04-17T11:34:58,126 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-04-17T11:34:58,126 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\AppData\Local\Temp\models\b31932e975844d80809048017333897f\handler.py", line 3, in <module>
2022-04-17T11:34:58,126 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier as ImgClsfr
2022-04-17T11:34:58,127 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:58,127 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\torch_handler\image_classifier.py", line 8, in <module>
2022-04-17T11:34:58,127 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2022-04-17T11:34:58,127 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\torch_handler\vision_handler.py", line 11, in <module>
2022-04-17T11:34:58,128 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2022-04-17T11:34:58,128 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2022-04-17T11:34:58,129 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - 
2022-04-17T11:34:58,129 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-04-17T11:34:58,129 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - 
2022-04-17T11:34:58,130 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:34:58,130 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-04-17T11:34:58,130 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -     worker.run_server()
2022-04-17T11:34:58,131 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-04-17T11:34:58,132 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-04-17T11:34:58,132 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-04-17T11:34:58,133 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-04-17T11:34:58,133 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py", line 95, in load_model
2022-04-17T11:34:58,134 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -     service = model_loader.load(model_name, model_dir, handler, gpu,
2022-04-17T11:34:58,127 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:58,136 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 85, in load
2022-04-17T11:34:58,136 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:58,137 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2022-04-17T11:34:58,136 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:58,139 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:58,137 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 128, in _load_default_handler
2022-04-17T11:34:58,139 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:58,139 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:58,139 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, 'ts.torch_handler')
2022-04-17T11:34:58,139 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:58,140 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stderr
2022-04-17T11:34:58,140 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:34:58,140 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stderr
2022-04-17T11:34:58,141 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stdout
2022-04-17T11:34:58,141 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stdout
2022-04-17T11:34:58,141 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2022-04-17T11:34:58,141 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2022-04-17T11:34:58,142 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:34:58,142 [INFO ] W-9000-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stdout
2022-04-17T11:34:58,142 [INFO ] W-9000-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stdout
2022-04-17T11:34:58,156 [INFO ] W-9000-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stderr
2022-04-17T11:34:58,156 [INFO ] W-9000-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stderr
2022-04-17T11:34:58,303 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:34:58,304 [INFO ] nioEventLoopGroup-5-14 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:58,304 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:34:58,304 [INFO ] nioEventLoopGroup-5-14 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:58,305 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:58,304 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:34:58,305 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:58,305 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:34:58,305 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:58,305 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:34:58,305 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:58,306 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:34:58,308 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:58,308 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:34:58,308 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:58,309 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:58,309 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:34:58,309 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:58,309 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:34:58,310 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stderr
2022-04-17T11:34:58,310 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stderr
2022-04-17T11:34:58,310 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stdout
2022-04-17T11:34:58,310 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:34:58,310 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stdout
2022-04-17T11:34:58,311 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 5 seconds.
2022-04-17T11:34:58,311 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-04-17T11:34:58,311 [INFO ] W-9007-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stdout
2022-04-17T11:34:58,311 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 5 seconds.
2022-04-17T11:34:58,311 [INFO ] W-9007-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stdout
2022-04-17T11:34:58,313 [INFO ] W-9007-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stderr
2022-04-17T11:34:58,313 [INFO ] W-9007-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stderr
2022-04-17T11:34:58,314 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:34:58,315 [INFO ] nioEventLoopGroup-5-15 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:58,315 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:34:58,315 [INFO ] nioEventLoopGroup-5-15 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:58,315 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:34:58,316 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:34:58,316 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:58,317 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:34:58,317 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:34:58,316 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:58,318 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:34:58,318 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:58,318 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:34:58,318 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:58,319 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:34:58,321 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:58,321 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:34:58,321 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:58,322 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:58,321 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-04-17T11:34:58,322 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:58,323 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stderr
2022-04-17T11:34:58,322 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-04-17T11:34:58,323 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stderr
2022-04-17T11:34:58,323 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stdout
2022-04-17T11:34:58,323 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stdout
2022-04-17T11:34:58,324 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 5 seconds.
2022-04-17T11:34:58,323 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 790, in exec_module
2022-04-17T11:34:58,324 [INFO ] W-9002-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stdout
2022-04-17T11:34:58,324 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 5 seconds.
2022-04-17T11:34:58,324 [INFO ] W-9002-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stdout
2022-04-17T11:34:58,327 [INFO ] W-9002-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stderr
2022-04-17T11:34:58,327 [INFO ] W-9002-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stderr
2022-04-17T11:34:58,359 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:34:58,359 [INFO ] nioEventLoopGroup-5-16 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:58,359 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:34:58,359 [INFO ] nioEventLoopGroup-5-16 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-04-17T11:34:58,360 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:58,360 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:34:58,360 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:34:58,360 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:34:58,361 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:58,361 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:34:58,361 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:34:58,363 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:58,361 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:34:58,363 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:34:58,364 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:58,364 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:34:58,364 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:34:58,364 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:34:58,365 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stderr
2022-04-17T11:34:58,365 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stderr
2022-04-17T11:34:58,365 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stdout
2022-04-17T11:34:58,365 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:34:58,365 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stdout
2022-04-17T11:34:58,366 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 5 seconds.
2022-04-17T11:34:58,365 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:34:58,366 [INFO ] W-9006-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stdout
2022-04-17T11:34:58,366 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 5 seconds.
2022-04-17T11:34:58,366 [INFO ] W-9006-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stdout
2022-04-17T11:34:58,368 [INFO ] W-9006-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stderr
2022-04-17T11:34:58,368 [INFO ] W-9006-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stderr
2022-04-17T11:35:02,534 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-04-17T11:35:02,534 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-04-17T11:35:02,757 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-04-17T11:35:02,757 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-04-17T11:35:03,034 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-04-17T11:35:03,034 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-04-17T11:35:03,090 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9008]
2022-04-17T11:35:03,090 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9008]
2022-04-17T11:35:03,144 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-04-17T11:35:03,144 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-04-17T11:35:03,314 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-04-17T11:35:03,314 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-04-17T11:35:03,327 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-04-17T11:35:03,327 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-04-17T11:35:03,370 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-04-17T11:35:03,370 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-04-17T11:35:03,568 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:35:03,569 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - [PID]11056
2022-04-17T11:35:03,570 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:03,570 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:35:03,570 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:03,570 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:35:03,571 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-04-17T11:35:03,571 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-04-17T11:35:03,574 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188103574
2022-04-17T11:35:03,573 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9004).
2022-04-17T11:35:03,574 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188103574
2022-04-17T11:35:03,575 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:35:03,941 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:35:03,942 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - [PID]11048
2022-04-17T11:35:03,942 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:03,942 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:35:03,942 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:03,942 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:35:03,943 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-04-17T11:35:03,943 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-04-17T11:35:03,946 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188103946
2022-04-17T11:35:03,946 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9003).
2022-04-17T11:35:03,946 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188103946
2022-04-17T11:35:03,948 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:35:04,030 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:35:04,031 [INFO ] nioEventLoopGroup-5-17 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:04,031 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:35:04,031 [INFO ] nioEventLoopGroup-5-17 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:04,032 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:04,032 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:35:04,032 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:04,032 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:35:04,032 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:04,033 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:35:04,033 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:35:04,033 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:35:04,032 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:04,035 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:35:04,036 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:04,036 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:35:04,036 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:04,037 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:04,036 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:35:04,037 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-04-17T11:35:04,037 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:04,039 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-04-17T11:35:04,039 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stderr
2022-04-17T11:35:04,039 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stderr
2022-04-17T11:35:04,040 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stdout
2022-04-17T11:35:04,039 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 790, in exec_module
2022-04-17T11:35:04,040 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stdout
2022-04-17T11:35:04,040 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 8 seconds.
2022-04-17T11:35:04,040 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-04-17T11:35:04,040 [INFO ] W-9004-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stdout
2022-04-17T11:35:04,040 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 8 seconds.
2022-04-17T11:35:04,040 [INFO ] W-9004-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stdout
2022-04-17T11:35:04,043 [INFO ] W-9004-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stderr
2022-04-17T11:35:04,043 [INFO ] W-9004-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stderr
2022-04-17T11:35:04,389 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:35:04,390 [INFO ] nioEventLoopGroup-5-18 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:04,390 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:35:04,390 [INFO ] nioEventLoopGroup-5-18 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:04,391 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:04,390 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:35:04,391 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:04,391 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:35:04,391 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:04,391 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:35:04,391 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:04,392 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:35:04,394 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:04,394 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:35:04,394 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:04,395 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:04,395 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:35:04,395 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:04,396 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stderr
2022-04-17T11:35:04,396 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:35:04,396 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stderr
2022-04-17T11:35:04,397 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stdout
2022-04-17T11:35:04,396 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:35:04,397 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stdout
2022-04-17T11:35:04,397 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-04-17T11:35:04,398 [INFO ] W-9003-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stdout
2022-04-17T11:35:04,398 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 8 seconds.
2022-04-17T11:35:04,398 [INFO ] W-9003-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stdout
2022-04-17T11:35:04,398 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 8 seconds.
2022-04-17T11:35:04,402 [INFO ] W-9003-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stderr
2022-04-17T11:35:04,402 [INFO ] W-9003-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stderr
2022-04-17T11:35:04,491 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:35:04,492 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - [PID]18092
2022-04-17T11:35:04,493 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:04,493 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:35:04,493 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:04,494 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-04-17T11:35:04,493 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:35:04,494 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-04-17T11:35:04,496 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188104496
2022-04-17T11:35:04,496 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9005).
2022-04-17T11:35:04,496 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188104496
2022-04-17T11:35:04,498 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:35:04,592 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:35:04,593 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - [PID]7996
2022-04-17T11:35:04,594 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:04,594 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:35:04,594 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:04,594 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9008
2022-04-17T11:35:04,594 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:35:04,594 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9008
2022-04-17T11:35:04,597 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188104597
2022-04-17T11:35:04,597 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9008).
2022-04-17T11:35:04,597 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188104597
2022-04-17T11:35:04,599 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:35:04,676 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:35:04,677 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - [PID]18192
2022-04-17T11:35:04,678 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:04,678 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:35:04,678 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:04,679 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-04-17T11:35:04,678 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:35:04,679 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-04-17T11:35:04,681 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188104681
2022-04-17T11:35:04,681 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9000).
2022-04-17T11:35:04,681 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188104681
2022-04-17T11:35:04,683 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:35:04,777 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:35:04,778 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - [PID]15472
2022-04-17T11:35:04,779 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:04,779 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:35:04,779 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:04,779 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-04-17T11:35:04,779 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:35:04,779 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-04-17T11:35:04,781 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188104781
2022-04-17T11:35:04,781 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188104781
2022-04-17T11:35:04,781 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9007).
2022-04-17T11:35:04,783 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:35:04,809 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:35:04,810 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - [PID]14360
2022-04-17T11:35:04,811 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:04,811 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:35:04,811 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:04,812 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-04-17T11:35:04,812 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:35:04,812 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-04-17T11:35:04,814 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188104814
2022-04-17T11:35:04,814 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188104814
2022-04-17T11:35:04,814 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9002).
2022-04-17T11:35:04,816 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:35:04,850 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:35:04,852 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - [PID]14160
2022-04-17T11:35:04,852 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:04,852 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:35:04,852 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:04,853 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-04-17T11:35:04,852 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:35:04,853 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-04-17T11:35:04,855 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188104855
2022-04-17T11:35:04,855 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188104855
2022-04-17T11:35:04,855 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9006).
2022-04-17T11:35:04,856 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:35:04,948 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:35:04,949 [INFO ] nioEventLoopGroup-5-19 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:04,949 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:35:04,949 [INFO ] nioEventLoopGroup-5-19 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:04,950 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:04,949 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:35:04,950 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:04,950 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:35:04,950 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:04,951 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:35:04,950 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:04,953 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:04,951 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:35:04,953 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:04,954 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:04,953 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:35:04,954 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:04,954 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:35:04,954 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stderr
2022-04-17T11:35:04,955 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:35:04,954 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stderr
2022-04-17T11:35:04,955 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stdout
2022-04-17T11:35:04,955 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:35:04,955 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stdout
2022-04-17T11:35:04,956 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 8 seconds.
2022-04-17T11:35:04,956 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-04-17T11:35:04,956 [INFO ] W-9005-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stdout
2022-04-17T11:35:04,956 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 8 seconds.
2022-04-17T11:35:04,956 [INFO ] W-9005-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stdout
2022-04-17T11:35:04,959 [INFO ] W-9005-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stderr
2022-04-17T11:35:04,959 [INFO ] W-9005-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stderr
2022-04-17T11:35:05,049 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:35:05,049 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:35:05,050 [INFO ] nioEventLoopGroup-5-20 org.pytorch.serve.wlm.WorkerThread - 9008 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:05,050 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:35:05,050 [INFO ] nioEventLoopGroup-5-20 org.pytorch.serve.wlm.WorkerThread - 9008 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:05,051 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:05,050 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:35:05,051 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:05,051 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:35:05,051 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:05,051 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:35:05,051 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:05,054 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:05,052 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:35:05,054 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:05,054 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:05,054 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:35:05,054 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:05,055 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stderr
2022-04-17T11:35:05,055 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:35:05,055 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stderr
2022-04-17T11:35:05,056 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stdout
2022-04-17T11:35:05,055 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:35:05,056 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stdout
2022-04-17T11:35:05,056 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9008 in 8 seconds.
2022-04-17T11:35:05,056 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-04-17T11:35:05,057 [INFO ] W-9008-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stdout
2022-04-17T11:35:05,056 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9008 in 8 seconds.
2022-04-17T11:35:05,057 [INFO ] W-9008-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stdout
2022-04-17T11:35:05,058 [INFO ] W-9008-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stderr
2022-04-17T11:35:05,058 [INFO ] W-9008-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stderr
2022-04-17T11:35:05,119 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:35:05,120 [INFO ] nioEventLoopGroup-5-21 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:05,120 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:35:05,120 [INFO ] nioEventLoopGroup-5-21 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:05,121 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:05,120 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:35:05,121 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:05,121 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:35:05,121 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:05,121 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:05,124 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:05,122 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:35:05,124 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:05,124 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:05,124 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:35:05,124 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:05,124 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:35:05,125 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stderr
2022-04-17T11:35:05,125 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stderr
2022-04-17T11:35:05,125 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stdout
2022-04-17T11:35:05,125 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:35:05,125 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stdout
2022-04-17T11:35:05,126 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2022-04-17T11:35:05,126 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:35:05,126 [INFO ] W-9000-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stdout
2022-04-17T11:35:05,126 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2022-04-17T11:35:05,126 [INFO ] W-9000-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stdout
2022-04-17T11:35:05,128 [INFO ] W-9000-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stderr
2022-04-17T11:35:05,128 [INFO ] W-9000-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stderr
2022-04-17T11:35:05,198 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:35:05,198 [INFO ] nioEventLoopGroup-5-22 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:05,198 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:35:05,198 [INFO ] nioEventLoopGroup-5-22 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:05,199 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:05,199 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:35:05,199 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:05,200 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:35:05,200 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:05,200 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:35:05,200 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:05,201 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:35:05,203 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:05,203 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:05,203 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:05,203 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:35:05,203 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:05,203 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:35:05,204 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stderr
2022-04-17T11:35:05,204 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stderr
2022-04-17T11:35:05,204 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stdout
2022-04-17T11:35:05,204 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:35:05,204 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stdout
2022-04-17T11:35:05,205 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 8 seconds.
2022-04-17T11:35:05,205 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:35:05,205 [INFO ] W-9007-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stdout
2022-04-17T11:35:05,205 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 8 seconds.
2022-04-17T11:35:05,205 [INFO ] W-9007-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stdout
2022-04-17T11:35:05,206 [INFO ] W-9007-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stderr
2022-04-17T11:35:05,206 [INFO ] W-9007-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stderr
2022-04-17T11:35:05,224 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:35:05,224 [INFO ] nioEventLoopGroup-5-23 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:05,224 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:35:05,224 [INFO ] nioEventLoopGroup-5-23 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:05,225 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:05,225 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:35:05,225 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:05,225 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:35:05,226 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:05,226 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:35:05,226 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:05,226 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:35:05,228 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:05,228 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:05,229 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:05,228 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:35:05,229 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:05,229 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:35:05,230 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stderr
2022-04-17T11:35:05,230 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stderr
2022-04-17T11:35:05,230 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stdout
2022-04-17T11:35:05,230 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:35:05,230 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stdout
2022-04-17T11:35:05,231 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 8 seconds.
2022-04-17T11:35:05,230 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:35:05,231 [INFO ] W-9002-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stdout
2022-04-17T11:35:05,231 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 8 seconds.
2022-04-17T11:35:05,231 [INFO ] W-9002-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stdout
2022-04-17T11:35:05,232 [INFO ] W-9002-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stderr
2022-04-17T11:35:05,232 [INFO ] W-9002-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stderr
2022-04-17T11:35:05,272 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:35:05,272 [INFO ] nioEventLoopGroup-5-24 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:05,272 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:35:05,272 [INFO ] nioEventLoopGroup-5-24 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:05,273 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:05,273 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:35:05,273 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:05,274 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:35:05,274 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:05,274 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:35:05,274 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:05,274 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:35:05,276 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:05,276 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:35:05,276 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:05,277 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:05,277 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:35:05,277 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:05,278 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stderr
2022-04-17T11:35:05,277 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:35:05,278 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stderr
2022-04-17T11:35:05,278 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stdout
2022-04-17T11:35:05,278 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:35:05,278 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stdout
2022-04-17T11:35:05,279 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 8 seconds.
2022-04-17T11:35:05,279 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-04-17T11:35:05,279 [INFO ] W-9006-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stdout
2022-04-17T11:35:05,279 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 8 seconds.
2022-04-17T11:35:05,279 [INFO ] W-9006-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stdout
2022-04-17T11:35:05,281 [INFO ] W-9006-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stderr
2022-04-17T11:35:05,281 [INFO ] W-9006-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stderr
2022-04-17T11:35:12,042 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-04-17T11:35:12,042 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-04-17T11:35:12,401 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-04-17T11:35:12,401 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-04-17T11:35:12,935 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:35:12,936 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - [PID]17504
2022-04-17T11:35:12,936 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:12,936 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:35:12,936 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:12,937 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-04-17T11:35:12,936 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:35:12,937 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-04-17T11:35:12,938 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188112938
2022-04-17T11:35:12,938 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188112938
2022-04-17T11:35:12,938 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9004).
2022-04-17T11:35:12,939 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:35:12,958 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-04-17T11:35:12,958 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-04-17T11:35:13,058 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9008]
2022-04-17T11:35:13,058 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9008]
2022-04-17T11:35:13,129 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-04-17T11:35:13,129 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-04-17T11:35:13,209 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-04-17T11:35:13,209 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-04-17T11:35:13,234 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-04-17T11:35:13,234 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-04-17T11:35:13,282 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-04-17T11:35:13,282 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-04-17T11:35:13,293 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:35:13,294 [INFO ] nioEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:13,294 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:35:13,294 [INFO ] nioEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:13,294 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:35:13,295 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:35:13,295 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:13,295 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:35:13,295 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:13,296 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:35:13,296 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:13,296 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:35:13,296 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:35:13,296 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:13,299 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:35:13,300 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:35:13,300 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:13,300 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:13,300 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-04-17T11:35:13,301 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:13,301 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-04-17T11:35:13,301 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:13,302 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stderr
2022-04-17T11:35:13,302 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 790, in exec_module
2022-04-17T11:35:13,302 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stderr
2022-04-17T11:35:13,303 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stdout
2022-04-17T11:35:13,302 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-04-17T11:35:13,303 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stdout
2022-04-17T11:35:13,303 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\AppData\Local\Temp\models\b31932e975844d80809048017333897f\handler.py", line 3, in <module>
2022-04-17T11:35:13,303 [INFO ] W-9004-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stdout
2022-04-17T11:35:13,303 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 13 seconds.
2022-04-17T11:35:13,303 [INFO ] W-9004-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stdout
2022-04-17T11:35:13,303 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 13 seconds.
2022-04-17T11:35:13,314 [INFO ] W-9004-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stderr
2022-04-17T11:35:13,314 [INFO ] W-9004-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stderr
2022-04-17T11:35:13,682 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:35:13,684 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - [PID]15872
2022-04-17T11:35:13,685 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:13,685 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:35:13,685 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:13,685 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-04-17T11:35:13,685 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:35:13,685 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-04-17T11:35:13,687 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188113687
2022-04-17T11:35:13,688 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9003).
2022-04-17T11:35:13,687 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188113687
2022-04-17T11:35:13,689 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:35:14,151 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:35:14,152 [INFO ] nioEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:14,152 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:35:14,152 [INFO ] nioEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:14,153 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:14,153 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:35:14,153 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:14,153 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:35:14,154 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:14,154 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:35:14,154 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:14,156 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:14,154 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:35:14,156 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:14,157 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:14,156 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:35:14,157 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:14,157 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:35:14,158 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stderr
2022-04-17T11:35:14,158 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stderr
2022-04-17T11:35:14,158 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stdout
2022-04-17T11:35:14,158 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:35:14,158 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stdout
2022-04-17T11:35:14,159 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 13 seconds.
2022-04-17T11:35:14,158 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:35:14,159 [INFO ] W-9003-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stdout
2022-04-17T11:35:14,159 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 13 seconds.
2022-04-17T11:35:14,159 [INFO ] W-9003-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stdout
2022-04-17T11:35:14,161 [INFO ] W-9003-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stderr
2022-04-17T11:35:14,161 [INFO ] W-9003-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stderr
2022-04-17T11:35:14,661 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:35:14,668 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - [PID]18156
2022-04-17T11:35:14,669 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:35:14,669 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:14,669 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:35:14,669 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:14,675 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-04-17T11:35:14,675 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-04-17T11:35:14,679 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9005).
2022-04-17T11:35:14,680 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188114680
2022-04-17T11:35:14,680 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188114680
2022-04-17T11:35:14,691 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:35:15,023 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:35:15,024 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - [PID]17560
2022-04-17T11:35:15,025 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:15,025 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:35:15,025 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:15,026 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9008
2022-04-17T11:35:15,025 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:35:15,026 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9008
2022-04-17T11:35:15,028 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188115028
2022-04-17T11:35:15,028 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188115028
2022-04-17T11:35:15,028 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9008).
2022-04-17T11:35:15,030 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:35:15,100 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:35:15,101 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - [PID]8448
2022-04-17T11:35:15,102 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:15,102 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:35:15,102 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:15,103 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-04-17T11:35:15,103 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:35:15,103 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-04-17T11:35:15,106 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188115106
2022-04-17T11:35:15,106 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9000).
2022-04-17T11:35:15,106 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188115106
2022-04-17T11:35:15,108 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:35:15,133 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:35:15,135 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - [PID]9532
2022-04-17T11:35:15,136 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:15,136 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:35:15,136 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:15,136 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:35:15,137 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-04-17T11:35:15,137 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-04-17T11:35:15,139 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188115139
2022-04-17T11:35:15,139 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9002).
2022-04-17T11:35:15,139 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188115139
2022-04-17T11:35:15,141 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:35:15,184 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:35:15,186 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - [PID]6744
2022-04-17T11:35:15,187 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:15,187 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:35:15,187 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:15,188 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-04-17T11:35:15,187 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:35:15,188 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-04-17T11:35:15,190 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188115190
2022-04-17T11:35:15,190 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188115190
2022-04-17T11:35:15,190 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9007).
2022-04-17T11:35:15,191 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:35:15,271 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:35:15,273 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - [PID]17452
2022-04-17T11:35:15,274 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:15,273 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:35:15,274 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:15,274 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:35:15,275 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-04-17T11:35:15,275 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-04-17T11:35:15,277 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188115277
2022-04-17T11:35:15,277 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9006).
2022-04-17T11:35:15,277 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188115277
2022-04-17T11:35:15,279 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:35:15,356 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:35:15,356 [INFO ] nioEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:15,356 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:35:15,356 [INFO ] nioEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:15,358 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:15,357 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:35:15,358 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:15,358 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:35:15,358 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:15,359 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:35:15,358 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:15,359 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:35:15,362 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:15,362 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:35:15,362 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:15,363 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:15,362 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:35:15,363 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:15,364 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stderr
2022-04-17T11:35:15,363 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:35:15,364 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stderr
2022-04-17T11:35:15,364 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stdout
2022-04-17T11:35:15,364 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:35:15,364 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stdout
2022-04-17T11:35:15,365 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 13 seconds.
2022-04-17T11:35:15,364 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-04-17T11:35:15,365 [INFO ] W-9005-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stdout
2022-04-17T11:35:15,365 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 13 seconds.
2022-04-17T11:35:15,365 [INFO ] W-9005-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stdout
2022-04-17T11:35:15,368 [INFO ] W-9005-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stderr
2022-04-17T11:35:15,368 [INFO ] W-9005-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stderr
2022-04-17T11:35:15,534 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:35:15,535 [INFO ] nioEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9008 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:15,535 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:35:15,535 [INFO ] nioEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9008 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:15,535 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:15,535 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:35:15,535 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:15,536 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:35:15,536 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:15,536 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:35:15,536 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:15,539 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:15,537 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:35:15,539 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:15,540 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:15,539 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:35:15,540 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:15,540 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stderr
2022-04-17T11:35:15,540 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:35:15,540 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stderr
2022-04-17T11:35:15,541 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stdout
2022-04-17T11:35:15,540 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:35:15,541 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stdout
2022-04-17T11:35:15,542 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9008 in 13 seconds.
2022-04-17T11:35:15,541 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:35:15,542 [INFO ] W-9008-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stdout
2022-04-17T11:35:15,542 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9008 in 13 seconds.
2022-04-17T11:35:15,542 [INFO ] W-9008-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stdout
2022-04-17T11:35:15,544 [INFO ] W-9008-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stderr
2022-04-17T11:35:15,544 [INFO ] W-9008-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stderr
2022-04-17T11:35:15,590 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:35:15,591 [INFO ] nioEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:15,591 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:35:15,591 [INFO ] nioEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:15,592 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:15,591 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:35:15,592 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:15,592 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:35:15,592 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:15,592 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:15,595 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:15,593 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:35:15,595 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:15,596 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:15,595 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:35:15,596 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:15,596 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:35:15,597 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stderr
2022-04-17T11:35:15,597 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stderr
2022-04-17T11:35:15,597 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stdout
2022-04-17T11:35:15,597 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:35:15,597 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stdout
2022-04-17T11:35:15,598 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.
2022-04-17T11:35:15,597 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:35:15,598 [INFO ] W-9000-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stdout
2022-04-17T11:35:15,598 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.
2022-04-17T11:35:15,598 [INFO ] W-9000-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stdout
2022-04-17T11:35:15,600 [INFO ] W-9000-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stderr
2022-04-17T11:35:15,600 [INFO ] W-9000-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stderr
2022-04-17T11:35:15,619 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:35:15,620 [INFO ] nioEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:15,620 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:35:15,620 [INFO ] nioEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:15,621 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:15,620 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:35:15,621 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:15,622 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:15,621 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:35:15,622 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:15,624 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:15,622 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:35:15,624 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:15,625 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:15,624 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:35:15,625 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:15,625 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stderr
2022-04-17T11:35:15,625 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:35:15,625 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stderr
2022-04-17T11:35:15,626 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stdout
2022-04-17T11:35:15,626 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:35:15,626 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stdout
2022-04-17T11:35:15,627 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 13 seconds.
2022-04-17T11:35:15,626 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:35:15,627 [INFO ] W-9002-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stdout
2022-04-17T11:35:15,627 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 13 seconds.
2022-04-17T11:35:15,627 [INFO ] W-9002-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stdout
2022-04-17T11:35:15,629 [INFO ] W-9002-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stderr
2022-04-17T11:35:15,629 [INFO ] W-9002-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stderr
2022-04-17T11:35:15,664 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:35:15,665 [INFO ] nioEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:15,665 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:35:15,665 [INFO ] nioEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:15,665 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:15,665 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:35:15,665 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:15,666 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:35:15,666 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:15,666 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:15,668 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:15,666 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:35:15,668 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:15,669 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:15,669 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:35:15,669 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:15,670 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stderr
2022-04-17T11:35:15,669 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:35:15,670 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stderr
2022-04-17T11:35:15,670 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stdout
2022-04-17T11:35:15,670 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:35:15,670 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stdout
2022-04-17T11:35:15,671 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 13 seconds.
2022-04-17T11:35:15,671 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:35:15,671 [INFO ] W-9007-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stdout
2022-04-17T11:35:15,671 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 13 seconds.
2022-04-17T11:35:15,671 [INFO ] W-9007-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stdout
2022-04-17T11:35:15,673 [INFO ] W-9007-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stderr
2022-04-17T11:35:15,673 [INFO ] W-9007-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stderr
2022-04-17T11:35:15,737 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:35:15,737 [INFO ] nioEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:15,737 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:35:15,737 [INFO ] nioEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:15,738 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:15,738 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:35:15,738 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:15,738 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:35:15,739 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:15,739 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:35:15,739 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:15,740 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:35:15,742 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:15,742 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:15,742 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:15,742 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:35:15,742 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:15,743 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stderr
2022-04-17T11:35:15,743 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:35:15,743 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stderr
2022-04-17T11:35:15,744 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stdout
2022-04-17T11:35:15,743 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:35:15,744 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stdout
2022-04-17T11:35:15,744 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 13 seconds.
2022-04-17T11:35:15,744 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:35:15,745 [INFO ] W-9006-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stdout
2022-04-17T11:35:15,744 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 13 seconds.
2022-04-17T11:35:15,745 [INFO ] W-9006-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stdout
2022-04-17T11:35:15,746 [INFO ] W-9006-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stderr
2022-04-17T11:35:15,746 [INFO ] W-9006-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stderr
2022-04-17T11:35:26,306 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-04-17T11:35:26,306 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-04-17T11:35:27,104 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:35:27,105 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - [PID]16188
2022-04-17T11:35:27,105 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:27,105 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:35:27,105 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:27,105 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-04-17T11:35:27,105 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:35:27,105 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-04-17T11:35:27,107 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188127107
2022-04-17T11:35:27,107 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188127107
2022-04-17T11:35:27,107 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9004).
2022-04-17T11:35:27,108 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:35:27,161 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-04-17T11:35:27,161 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-04-17T11:35:27,358 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:35:27,358 [INFO ] nioEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:27,358 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:35:27,358 [INFO ] nioEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:27,359 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:27,358 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:35:27,359 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:27,359 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:35:27,359 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:27,359 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:35:27,359 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:27,361 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:27,360 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:35:27,361 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:27,361 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:27,361 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:35:27,361 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:27,362 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stderr
2022-04-17T11:35:27,362 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:35:27,362 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stderr
2022-04-17T11:35:27,362 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stdout
2022-04-17T11:35:27,362 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:35:27,362 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stdout
2022-04-17T11:35:27,363 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 21 seconds.
2022-04-17T11:35:27,363 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:35:27,363 [INFO ] W-9004-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stdout
2022-04-17T11:35:27,363 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 21 seconds.
2022-04-17T11:35:27,363 [INFO ] W-9004-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stdout
2022-04-17T11:35:27,364 [INFO ] W-9004-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stderr
2022-04-17T11:35:27,364 [INFO ] W-9004-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stderr
2022-04-17T11:35:27,980 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:35:27,983 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - [PID]17672
2022-04-17T11:35:27,984 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:27,984 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:35:27,984 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:27,985 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-04-17T11:35:27,984 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:35:27,985 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-04-17T11:35:27,987 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188127987
2022-04-17T11:35:27,987 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188127987
2022-04-17T11:35:27,987 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9003).
2022-04-17T11:35:27,988 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:35:28,301 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:35:28,302 [INFO ] nioEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:28,301 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:35:28,302 [INFO ] nioEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:28,302 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:28,302 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:35:28,302 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:28,302 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:35:28,303 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:28,303 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:35:28,303 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:28,305 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:28,303 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:35:28,305 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:28,305 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:28,305 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:35:28,305 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:28,306 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:35:28,306 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stderr
2022-04-17T11:35:28,306 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stderr
2022-04-17T11:35:28,306 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stdout
2022-04-17T11:35:28,306 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:35:28,306 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stdout
2022-04-17T11:35:28,307 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 21 seconds.
2022-04-17T11:35:28,307 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:35:28,307 [INFO ] W-9003-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stdout
2022-04-17T11:35:28,307 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 21 seconds.
2022-04-17T11:35:28,307 [INFO ] W-9003-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stdout
2022-04-17T11:35:28,309 [INFO ] W-9003-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stderr
2022-04-17T11:35:28,309 [INFO ] W-9003-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stderr
2022-04-17T11:35:28,368 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-04-17T11:35:28,368 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-04-17T11:35:28,544 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9008]
2022-04-17T11:35:28,544 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9008]
2022-04-17T11:35:28,600 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-04-17T11:35:28,600 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-04-17T11:35:28,630 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-04-17T11:35:28,630 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-04-17T11:35:28,675 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-04-17T11:35:28,675 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-04-17T11:35:28,747 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-04-17T11:35:28,747 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-04-17T11:35:30,250 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:35:30,254 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - [PID]16624
2022-04-17T11:35:30,255 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:30,255 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:35:30,255 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:30,256 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-04-17T11:35:30,256 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:35:30,256 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-04-17T11:35:30,259 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188130258
2022-04-17T11:35:30,259 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188130258
2022-04-17T11:35:30,259 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9005).
2022-04-17T11:35:30,260 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:35:30,596 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:35:30,598 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - [PID]15704
2022-04-17T11:35:30,599 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:30,598 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:35:30,599 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:30,599 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9008
2022-04-17T11:35:30,599 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:35:30,599 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9008
2022-04-17T11:35:30,602 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188130602
2022-04-17T11:35:30,602 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9008).
2022-04-17T11:35:30,602 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188130602
2022-04-17T11:35:30,604 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:35:30,631 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:35:30,633 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - [PID]10660
2022-04-17T11:35:30,634 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:30,634 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:35:30,634 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:35:30,634 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:30,635 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-04-17T11:35:30,635 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-04-17T11:35:30,638 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188130638
2022-04-17T11:35:30,638 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9000).
2022-04-17T11:35:30,638 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188130638
2022-04-17T11:35:30,640 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:35:30,679 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:35:30,680 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - [PID]17772
2022-04-17T11:35:30,681 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:30,681 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:35:30,681 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:30,682 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-04-17T11:35:30,681 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:35:30,682 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-04-17T11:35:30,687 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188130687
2022-04-17T11:35:30,687 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9002).
2022-04-17T11:35:30,687 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188130687
2022-04-17T11:35:30,689 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:35:30,713 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:35:30,715 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - [PID]8596
2022-04-17T11:35:30,715 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:30,715 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:35:30,715 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:30,718 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-04-17T11:35:30,716 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:35:30,718 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-04-17T11:35:30,720 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188130720
2022-04-17T11:35:30,720 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188130720
2022-04-17T11:35:30,720 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9007).
2022-04-17T11:35:30,721 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:35:30,801 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:35:30,802 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - [PID]15696
2022-04-17T11:35:30,803 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:30,803 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:35:30,803 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:35:30,803 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:30,804 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-04-17T11:35:30,804 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-04-17T11:35:30,807 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188130807
2022-04-17T11:35:30,807 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9006).
2022-04-17T11:35:30,807 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188130807
2022-04-17T11:35:30,808 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:35:30,858 [INFO ] nioEventLoopGroup-5-11 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:30,857 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:35:30,858 [INFO ] nioEventLoopGroup-5-11 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:30,859 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:30,858 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:35:30,859 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:30,860 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:30,859 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:35:30,860 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:30,860 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:35:30,862 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:30,862 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:30,863 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:30,862 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:35:30,863 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:30,863 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stderr
2022-04-17T11:35:30,863 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:35:30,863 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stderr
2022-04-17T11:35:30,864 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stdout
2022-04-17T11:35:30,864 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:35:30,864 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stdout
2022-04-17T11:35:30,865 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 21 seconds.
2022-04-17T11:35:30,864 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:35:30,865 [INFO ] W-9005-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stdout
2022-04-17T11:35:30,865 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 21 seconds.
2022-04-17T11:35:30,865 [INFO ] W-9005-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stdout
2022-04-17T11:35:30,868 [INFO ] W-9005-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stderr
2022-04-17T11:35:30,868 [INFO ] W-9005-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stderr
2022-04-17T11:35:31,130 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:35:31,131 [INFO ] nioEventLoopGroup-5-12 org.pytorch.serve.wlm.WorkerThread - 9008 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:31,130 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:35:31,131 [INFO ] nioEventLoopGroup-5-12 org.pytorch.serve.wlm.WorkerThread - 9008 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:31,131 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:31,131 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:31,131 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:35:31,132 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:31,132 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:31,135 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:31,132 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:35:31,135 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:31,135 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:31,135 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:31,136 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stderr
2022-04-17T11:35:31,135 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:35:31,136 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stderr
2022-04-17T11:35:31,137 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stdout
2022-04-17T11:35:31,137 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stdout
2022-04-17T11:35:31,137 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9008 in 21 seconds.
2022-04-17T11:35:31,136 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:35:31,137 [INFO ] W-9008-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stdout
2022-04-17T11:35:31,137 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9008 in 21 seconds.
2022-04-17T11:35:31,137 [INFO ] W-9008-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stdout
2022-04-17T11:35:31,143 [INFO ] W-9008-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stderr
2022-04-17T11:35:31,143 [INFO ] W-9008-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stderr
2022-04-17T11:35:31,178 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:35:31,178 [INFO ] nioEventLoopGroup-5-13 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:31,178 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:35:31,178 [INFO ] nioEventLoopGroup-5-13 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:31,179 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:35:31,179 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:35:31,180 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:35:31,180 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:35:31,181 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:31,181 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:35:31,181 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:31,181 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:35:31,182 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:31,182 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:35:31,182 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:31,182 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:35:31,185 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:31,185 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-04-17T11:35:31,185 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:31,186 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:31,185 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-04-17T11:35:31,186 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:31,187 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stderr
2022-04-17T11:35:31,187 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 790, in exec_module
2022-04-17T11:35:31,187 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stderr
2022-04-17T11:35:31,188 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stdout
2022-04-17T11:35:31,187 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-04-17T11:35:31,188 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stdout
2022-04-17T11:35:31,189 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.
2022-04-17T11:35:31,188 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\AppData\Local\Temp\models\b31932e975844d80809048017333897f\handler.py", line 3, in <module>
2022-04-17T11:35:31,189 [INFO ] W-9000-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stdout
2022-04-17T11:35:31,189 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.
2022-04-17T11:35:31,189 [INFO ] W-9000-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stdout
2022-04-17T11:35:31,191 [INFO ] W-9000-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stderr
2022-04-17T11:35:31,191 [INFO ] W-9000-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stderr
2022-04-17T11:35:31,219 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:35:31,220 [INFO ] nioEventLoopGroup-5-14 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:31,220 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:35:31,220 [INFO ] nioEventLoopGroup-5-14 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:31,220 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:31,220 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:35:31,220 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:31,221 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:35:31,221 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:31,221 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:35:31,221 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:31,222 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:35:31,224 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:31,224 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:31,224 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:31,224 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:35:31,224 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:31,225 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:35:31,225 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stderr
2022-04-17T11:35:31,225 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stderr
2022-04-17T11:35:31,226 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stdout
2022-04-17T11:35:31,225 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:35:31,226 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stdout
2022-04-17T11:35:31,227 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 21 seconds.
2022-04-17T11:35:31,226 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:35:31,227 [INFO ] W-9002-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stdout
2022-04-17T11:35:31,227 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 21 seconds.
2022-04-17T11:35:31,227 [INFO ] W-9002-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stdout
2022-04-17T11:35:31,230 [INFO ] W-9002-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stderr
2022-04-17T11:35:31,230 [INFO ] W-9002-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stderr
2022-04-17T11:35:31,259 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:35:31,260 [INFO ] nioEventLoopGroup-5-15 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:31,260 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:35:31,260 [INFO ] nioEventLoopGroup-5-15 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:31,261 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:31,260 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:35:31,261 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:31,261 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:31,261 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:35:31,261 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:31,263 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:31,262 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:35:31,263 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:31,264 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:31,264 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:35:31,264 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:31,265 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stderr
2022-04-17T11:35:31,264 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:35:31,265 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stderr
2022-04-17T11:35:31,265 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stdout
2022-04-17T11:35:31,265 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:35:31,265 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stdout
2022-04-17T11:35:31,266 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 21 seconds.
2022-04-17T11:35:31,265 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:35:31,266 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 21 seconds.
2022-04-17T11:35:31,267 [INFO ] W-9007-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stdout
2022-04-17T11:35:31,267 [INFO ] W-9007-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stdout
2022-04-17T11:35:31,269 [INFO ] W-9007-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stderr
2022-04-17T11:35:31,269 [INFO ] W-9007-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stderr
2022-04-17T11:35:31,351 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:35:31,352 [INFO ] nioEventLoopGroup-5-16 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:31,352 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:35:31,352 [INFO ] nioEventLoopGroup-5-16 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:31,352 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:35:31,353 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:31,353 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:35:31,353 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:31,353 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:35:31,354 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:31,354 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:35:31,354 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:31,355 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:35:31,358 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:31,357 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:35:31,358 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:31,358 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:31,358 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:35:31,358 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:31,359 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:35:31,359 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stderr
2022-04-17T11:35:31,359 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stderr
2022-04-17T11:35:31,360 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stdout
2022-04-17T11:35:31,359 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-04-17T11:35:31,360 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stdout
2022-04-17T11:35:31,361 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 21 seconds.
2022-04-17T11:35:31,360 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-04-17T11:35:31,361 [INFO ] W-9006-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stdout
2022-04-17T11:35:31,361 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 21 seconds.
2022-04-17T11:35:31,361 [INFO ] W-9006-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stdout
2022-04-17T11:35:31,363 [INFO ] W-9006-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stderr
2022-04-17T11:35:31,363 [INFO ] W-9006-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stderr
2022-04-17T11:35:39,584 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-04-17T11:35:39,584 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-04-17T11:35:39,717 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:1650188139
2022-04-17T11:35:39,718 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:80.98338317871094|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:1650188139
2022-04-17T11:35:39,718 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:374.8652458190918|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:1650188139
2022-04-17T11:35:39,718 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:82.2|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:1650188139
2022-04-17T11:35:39,719 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:5788.953125|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:1650188139
2022-04-17T11:35:39,719 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:10400.7109375|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:1650188139
2022-04-17T11:35:39,719 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:64.2|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:1650188139
2022-04-17T11:35:39,762 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - --- Logging error ---
Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\_pswindows.py", line 688, in wrapper
    return fun(self, *args, **kwargs)
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\_pswindows.py", line 942, in create_time
    user, system, created = cext.proc_times(self.pid)
ProcessLookupError: [Errno 3] assume no such process (originated from GetExitCodeProcess != STILL_ACTIVE)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\__init__.py", line 361, in _init
    self.create_time()
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\__init__.py", line 717, in create_time
    self._create_time = self._proc.create_time()
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\_pswindows.py", line 690, in wrapper
    raise convert_oserror(err, pid=self.pid, name=self._name)
psutil.NoSuchProcess: process no longer exists (pid=15696)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\process_memory_metric.py", line 20, in get_cpu_usage
    process = psutil.Process(int(pid))
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\__init__.py", line 332, in __init__
    self._init(pid)
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\__init__.py", line 373, in _init
    raise NoSuchProcess(pid, msg='process PID not found')
psutil.NoSuchProcess: process PID not found (pid=15696)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\logging\__init__.py", line 1087, in emit
    self.flush()
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\logging\__init__.py", line 1067, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\metric_collector.py", line 29, in <module>
    check_process_mem_usage(sys.stdin)
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\process_memory_metric.py", line 22, in get_cpu_usage
    logging.error("Failed get process for pid: %s", pid, exc_info=True)
Message: 'Failed get process for pid: %s'
Arguments: ('15696',)
--- Logging error ---
Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\logging\__init__.py", line 1087, in emit
    self.flush()
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\logging\__init__.py", line 1067, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\metric_collector.py", line 29, in <module>
    check_process_mem_usage(sys.stdin)
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
Message: '%s:%d'
Arguments: ('15696', 0)
--- Logging error ---
Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\_pswindows.py", line 688, in wrapper
    return fun(self, *args, **kwargs)
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\_pswindows.py", line 942, in create_time
    user, system, created = cext.proc_times(self.pid)
ProcessLookupError: [Errno 3] assume no such process (originated from GetExitCodeProcess != STILL_ACTIVE)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\__init__.py", line 361, in _init
    self.create_time()
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\__init__.py", line 717, in create_time
    self._create_time = self._proc.create_time()
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\_pswindows.py", line 690, in wrapper
    raise convert_oserror(err, pid=self.pid, name=self._name)
psutil.NoSuchProcess: process no longer exists (pid=10660)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\process_memory_metric.py", line 20, in get_cpu_usage
    process = psutil.Process(int(pid))
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\__init__.py", line 332, in __init__
    self._init(pid)
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\__init__.py", line 373, in _init
    raise NoSuchProcess(pid, msg='process PID not found')
psutil.NoSuchProcess: process PID not found (pid=10660)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\logging\__init__.py", line 1087, in emit
    self.flush()
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\logging\__init__.py", line 1067, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\metric_collector.py", line 29, in <module>
    check_process_mem_usage(sys.stdin)
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\process_memory_metric.py", line 22, in get_cpu_usage
    logging.error("Failed get process for pid: %s", pid, exc_info=True)
Message: 'Failed get process for pid: %s'
Arguments: ('10660',)
--- Logging error ---
Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\logging\__init__.py", line 1087, in emit
    self.flush()
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\logging\__init__.py", line 1067, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\metric_collector.py", line 29, in <module>
    check_process_mem_usage(sys.stdin)
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
Message: '%s:%d'
Arguments: ('10660', 0)
--- Logging error ---
Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\_pswindows.py", line 688, in wrapper
    return fun(self, *args, **kwargs)
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\_pswindows.py", line 942, in create_time
    user, system, created = cext.proc_times(self.pid)
ProcessLookupError: [Errno 3] assume no such process (originated from GetExitCodeProcess != STILL_ACTIVE)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\__init__.py", line 361, in _init
    self.create_time()
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\__init__.py", line 717, in create_time
    self._create_time = self._proc.create_time()
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\_pswindows.py", line 690, in wrapper
    raise convert_oserror(err, pid=self.pid, name=self._name)
psutil.NoSuchProcess: process no longer exists (pid=8596)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\process_memory_metric.py", line 20, in get_cpu_usage
    process = psutil.Process(int(pid))
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\__init__.py", line 332, in __init__
    self._init(pid)
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\__init__.py", line 373, in _init
    raise NoSuchProcess(pid, msg='process PID not found')
psutil.NoSuchProcess: process PID not found (pid=8596)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\logging\__init__.py", line 1087, in emit
    self.flush()
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\logging\__init__.py", line 1067, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\metric_collector.py", line 29, in <module>
    check_process_mem_usage(sys.stdin)
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\process_memory_metric.py", line 22, in get_cpu_usage
    logging.error("Failed get process for pid: %s", pid, exc_info=True)
Message: 'Failed get process for pid: %s'
Arguments: ('8596',)
--- Logging error ---
Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\logging\__init__.py", line 1087, in emit
    self.flush()
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\logging\__init__.py", line 1067, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\metric_collector.py", line 29, in <module>
    check_process_mem_usage(sys.stdin)
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
Message: '%s:%d'
Arguments: ('8596', 0)
--- Logging error ---
Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\_pswindows.py", line 688, in wrapper
    return fun(self, *args, **kwargs)
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\_pswindows.py", line 942, in create_time
    user, system, created = cext.proc_times(self.pid)
ProcessLookupError: [Errno 3] assume no such process (originated from GetExitCodeProcess != STILL_ACTIVE)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\__init__.py", line 361, in _init
    self.create_time()
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\__init__.py", line 717, in create_time
    self._create_time = self._proc.create_time()
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\_pswindows.py", line 690, in wrapper
    raise convert_oserror(err, pid=self.pid, name=self._name)
psutil.NoSuchProcess: process no longer exists (pid=17672)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\process_memory_metric.py", line 20, in get_cpu_usage
    process = psutil.Process(int(pid))
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\__init__.py", line 332, in __init__
    self._init(pid)
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\__init__.py", line 373, in _init
    raise NoSuchProcess(pid, msg='process PID not found')
psutil.NoSuchProcess: process PID not found (pid=17672)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\logging\__init__.py", line 1087, in emit
    self.flush()
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\logging\__init__.py", line 1067, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\metric_collector.py", line 29, in <module>
    check_process_mem_usage(sys.stdin)
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\process_memory_metric.py", line 22, in get_cpu_usage
    logging.error("Failed get process for pid: %s", pid, exc_info=True)
Message: 'Failed get process for pid: %s'
Arguments: ('17672',)
--- Logging error ---
Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\logging\__init__.py", line 1087, in emit
    self.flush()
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\logging\__init__.py", line 1067, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\metric_collector.py", line 29, in <module>
    check_process_mem_usage(sys.stdin)
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
Message: '%s:%d'
Arguments: ('17672', 0)
--- Logging error ---
Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\_pswindows.py", line 688, in wrapper
    return fun(self, *args, **kwargs)
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\_pswindows.py", line 942, in create_time
    user, system, created = cext.proc_times(self.pid)
ProcessLookupError: [Errno 3] assume no such process (originated from GetExitCodeProcess != STILL_ACTIVE)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\__init__.py", line 361, in _init
    self.create_time()
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\__init__.py", line 717, in create_time
    self._create_time = self._proc.create_time()
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\_pswindows.py", line 690, in wrapper
    raise convert_oserror(err, pid=self.pid, name=self._name)
psutil.NoSuchProcess: process no longer exists (pid=15704)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\process_memory_metric.py", line 20, in get_cpu_usage
    process = psutil.Process(int(pid))
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\__init__.py", line 332, in __init__
    self._init(pid)
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\__init__.py", line 373, in _init
    raise NoSuchProcess(pid, msg='process PID not found')
psutil.NoSuchProcess: process PID not found (pid=15704)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\logging\__init__.py", line 1087, in emit
    self.flush()
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\logging\__init__.py", line 1067, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\metric_collector.py", line 29, in <module>
    check_process_mem_usage(sys.stdin)
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\process_memory_metric.py", line 22, in get_cpu_usage
    logging.error("Failed get process for pid: %s", pid, exc_info=True)
Message: 'Failed get process for pid: %s'
Arguments: ('15704',)
--- Logging error ---
Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\logging\__init__.py", line 1087, in emit
    self.flush()
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\logging\__init__.py", line 1067, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\metric_collector.py", line 29, in <module>
    check_process_mem_usage(sys.stdin)
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
Message: '%s:%d'
Arguments: ('15704', 0)
--- Logging error ---
Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\_pswindows.py", line 688, in wrapper
    return fun(self, *args, **kwargs)
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\_pswindows.py", line 942, in create_time
    user, system, created = cext.proc_times(self.pid)
ProcessLookupError: [Errno 3] assume no such process (originated from GetExitCodeProcess != STILL_ACTIVE)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\__init__.py", line 361, in _init
    self.create_time()
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\__init__.py", line 717, in create_time
    self._create_time = self._proc.create_time()
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\_pswindows.py", line 690, in wrapper
    raise convert_oserror(err, pid=self.pid, name=self._name)
psutil.NoSuchProcess: process no longer exists (pid=17772)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\process_memory_metric.py", line 20, in get_cpu_usage
    process = psutil.Process(int(pid))
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\__init__.py", line 332, in __init__
    self._init(pid)
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\__init__.py", line 373, in _init
    raise NoSuchProcess(pid, msg='process PID not found')
psutil.NoSuchProcess: process PID not found (pid=17772)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\logging\__init__.py", line 1087, in emit
    self.flush()
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\logging\__init__.py", line 1067, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\metric_collector.py", line 29, in <module>
    check_process_mem_usage(sys.stdin)
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\process_memory_metric.py", line 22, in get_cpu_usage
    logging.error("Failed get process for pid: %s", pid, exc_info=True)
Message: 'Failed get process for pid: %s'
Arguments: ('17772',)
--- Logging error ---
Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\logging\__init__.py", line 1087, in emit
    self.flush()
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\logging\__init__.py", line 1067, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\metric_collector.py", line 29, in <module>
    check_process_mem_usage(sys.stdin)
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
Message: '%s:%d'
Arguments: ('17772', 0)
--- Logging error ---
Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\_pswindows.py", line 688, in wrapper
    return fun(self, *args, **kwargs)
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\_pswindows.py", line 942, in create_time
    user, system, created = cext.proc_times(self.pid)
ProcessLookupError: [Errno 3] assume no such process (originated from GetExitCodeProcess != STILL_ACTIVE)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\__init__.py", line 361, in _init
    self.create_time()
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\__init__.py", line 717, in create_time
    self._create_time = self._proc.create_time()
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\_pswindows.py", line 690, in wrapper
    raise convert_oserror(err, pid=self.pid, name=self._name)
psutil.NoSuchProcess: process no longer exists (pid=16188)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\process_memory_metric.py", line 20, in get_cpu_usage
    process = psutil.Process(int(pid))
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\__init__.py", line 332, in __init__
    self._init(pid)
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\__init__.py", line 373, in _init
    raise NoSuchProcess(pid, msg='process PID not found')
psutil.NoSuchProcess: process PID not found (pid=16188)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\logging\__init__.py", line 1087, in emit
    self.flush()
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\logging\__init__.py", line 1067, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\metric_collector.py", line 29, in <module>
    check_process_mem_usage(sys.stdin)
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\process_memory_metric.py", line 22, in get_cpu_usage
    logging.error("Failed get process for pid: %s", pid, exc_info=True)
Message: 'Failed get process for pid: %s'
Arguments: ('16188',)
--- Logging error ---
Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\logging\__init__.py", line 1087, in emit
    self.flush()
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\logging\__init__.py", line 1067, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\metric_collector.py", line 29, in <module>
    check_process_mem_usage(sys.stdin)
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
Message: '%s:%d'
Arguments: ('16188', 0)
Exception ignored in: <_io.TextIOWrapper name='<stdout>' mode='w' encoding='cp1252'>
OSError: [Errno 22] Invalid argument

2022-04-17T11:35:39,762 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - --- Logging error ---
Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\_pswindows.py", line 688, in wrapper
    return fun(self, *args, **kwargs)
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\_pswindows.py", line 942, in create_time
    user, system, created = cext.proc_times(self.pid)
ProcessLookupError: [Errno 3] assume no such process (originated from GetExitCodeProcess != STILL_ACTIVE)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\__init__.py", line 361, in _init
    self.create_time()
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\__init__.py", line 717, in create_time
    self._create_time = self._proc.create_time()
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\_pswindows.py", line 690, in wrapper
    raise convert_oserror(err, pid=self.pid, name=self._name)
psutil.NoSuchProcess: process no longer exists (pid=15696)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\process_memory_metric.py", line 20, in get_cpu_usage
    process = psutil.Process(int(pid))
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\__init__.py", line 332, in __init__
    self._init(pid)
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\__init__.py", line 373, in _init
    raise NoSuchProcess(pid, msg='process PID not found')
psutil.NoSuchProcess: process PID not found (pid=15696)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\logging\__init__.py", line 1087, in emit
    self.flush()
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\logging\__init__.py", line 1067, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\metric_collector.py", line 29, in <module>
    check_process_mem_usage(sys.stdin)
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\process_memory_metric.py", line 22, in get_cpu_usage
    logging.error("Failed get process for pid: %s", pid, exc_info=True)
Message: 'Failed get process for pid: %s'
Arguments: ('15696',)
--- Logging error ---
Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\logging\__init__.py", line 1087, in emit
    self.flush()
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\logging\__init__.py", line 1067, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\metric_collector.py", line 29, in <module>
    check_process_mem_usage(sys.stdin)
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
Message: '%s:%d'
Arguments: ('15696', 0)
--- Logging error ---
Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\_pswindows.py", line 688, in wrapper
    return fun(self, *args, **kwargs)
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\_pswindows.py", line 942, in create_time
    user, system, created = cext.proc_times(self.pid)
ProcessLookupError: [Errno 3] assume no such process (originated from GetExitCodeProcess != STILL_ACTIVE)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\__init__.py", line 361, in _init
    self.create_time()
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\__init__.py", line 717, in create_time
    self._create_time = self._proc.create_time()
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\_pswindows.py", line 690, in wrapper
    raise convert_oserror(err, pid=self.pid, name=self._name)
psutil.NoSuchProcess: process no longer exists (pid=10660)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\process_memory_metric.py", line 20, in get_cpu_usage
    process = psutil.Process(int(pid))
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\__init__.py", line 332, in __init__
    self._init(pid)
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\__init__.py", line 373, in _init
    raise NoSuchProcess(pid, msg='process PID not found')
psutil.NoSuchProcess: process PID not found (pid=10660)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\logging\__init__.py", line 1087, in emit
    self.flush()
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\logging\__init__.py", line 1067, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\metric_collector.py", line 29, in <module>
    check_process_mem_usage(sys.stdin)
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\process_memory_metric.py", line 22, in get_cpu_usage
    logging.error("Failed get process for pid: %s", pid, exc_info=True)
Message: 'Failed get process for pid: %s'
Arguments: ('10660',)
--- Logging error ---
Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\logging\__init__.py", line 1087, in emit
    self.flush()
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\logging\__init__.py", line 1067, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\metric_collector.py", line 29, in <module>
    check_process_mem_usage(sys.stdin)
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
Message: '%s:%d'
Arguments: ('10660', 0)
--- Logging error ---
Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\_pswindows.py", line 688, in wrapper
    return fun(self, *args, **kwargs)
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\_pswindows.py", line 942, in create_time
    user, system, created = cext.proc_times(self.pid)
ProcessLookupError: [Errno 3] assume no such process (originated from GetExitCodeProcess != STILL_ACTIVE)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\__init__.py", line 361, in _init
    self.create_time()
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\__init__.py", line 717, in create_time
    self._create_time = self._proc.create_time()
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\_pswindows.py", line 690, in wrapper
    raise convert_oserror(err, pid=self.pid, name=self._name)
psutil.NoSuchProcess: process no longer exists (pid=8596)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\process_memory_metric.py", line 20, in get_cpu_usage
    process = psutil.Process(int(pid))
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\__init__.py", line 332, in __init__
    self._init(pid)
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\__init__.py", line 373, in _init
    raise NoSuchProcess(pid, msg='process PID not found')
psutil.NoSuchProcess: process PID not found (pid=8596)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\logging\__init__.py", line 1087, in emit
    self.flush()
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\logging\__init__.py", line 1067, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\metric_collector.py", line 29, in <module>
    check_process_mem_usage(sys.stdin)
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\process_memory_metric.py", line 22, in get_cpu_usage
    logging.error("Failed get process for pid: %s", pid, exc_info=True)
Message: 'Failed get process for pid: %s'
Arguments: ('8596',)
--- Logging error ---
Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\logging\__init__.py", line 1087, in emit
    self.flush()
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\logging\__init__.py", line 1067, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\metric_collector.py", line 29, in <module>
    check_process_mem_usage(sys.stdin)
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
Message: '%s:%d'
Arguments: ('8596', 0)
--- Logging error ---
Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\_pswindows.py", line 688, in wrapper
    return fun(self, *args, **kwargs)
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\_pswindows.py", line 942, in create_time
    user, system, created = cext.proc_times(self.pid)
ProcessLookupError: [Errno 3] assume no such process (originated from GetExitCodeProcess != STILL_ACTIVE)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\__init__.py", line 361, in _init
    self.create_time()
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\__init__.py", line 717, in create_time
    self._create_time = self._proc.create_time()
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\_pswindows.py", line 690, in wrapper
    raise convert_oserror(err, pid=self.pid, name=self._name)
psutil.NoSuchProcess: process no longer exists (pid=17672)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\process_memory_metric.py", line 20, in get_cpu_usage
    process = psutil.Process(int(pid))
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\__init__.py", line 332, in __init__
    self._init(pid)
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\__init__.py", line 373, in _init
    raise NoSuchProcess(pid, msg='process PID not found')
psutil.NoSuchProcess: process PID not found (pid=17672)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\logging\__init__.py", line 1087, in emit
    self.flush()
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\logging\__init__.py", line 1067, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\metric_collector.py", line 29, in <module>
    check_process_mem_usage(sys.stdin)
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\process_memory_metric.py", line 22, in get_cpu_usage
    logging.error("Failed get process for pid: %s", pid, exc_info=True)
Message: 'Failed get process for pid: %s'
Arguments: ('17672',)
--- Logging error ---
Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\logging\__init__.py", line 1087, in emit
    self.flush()
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\logging\__init__.py", line 1067, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\metric_collector.py", line 29, in <module>
    check_process_mem_usage(sys.stdin)
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
Message: '%s:%d'
Arguments: ('17672', 0)
--- Logging error ---
Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\_pswindows.py", line 688, in wrapper
    return fun(self, *args, **kwargs)
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\_pswindows.py", line 942, in create_time
    user, system, created = cext.proc_times(self.pid)
ProcessLookupError: [Errno 3] assume no such process (originated from GetExitCodeProcess != STILL_ACTIVE)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\__init__.py", line 361, in _init
    self.create_time()
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\__init__.py", line 717, in create_time
    self._create_time = self._proc.create_time()
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\_pswindows.py", line 690, in wrapper
    raise convert_oserror(err, pid=self.pid, name=self._name)
psutil.NoSuchProcess: process no longer exists (pid=15704)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\process_memory_metric.py", line 20, in get_cpu_usage
    process = psutil.Process(int(pid))
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\__init__.py", line 332, in __init__
    self._init(pid)
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\__init__.py", line 373, in _init
    raise NoSuchProcess(pid, msg='process PID not found')
psutil.NoSuchProcess: process PID not found (pid=15704)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\logging\__init__.py", line 1087, in emit
    self.flush()
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\logging\__init__.py", line 1067, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\metric_collector.py", line 29, in <module>
    check_process_mem_usage(sys.stdin)
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\process_memory_metric.py", line 22, in get_cpu_usage
    logging.error("Failed get process for pid: %s", pid, exc_info=True)
Message: 'Failed get process for pid: %s'
Arguments: ('15704',)
--- Logging error ---
Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\logging\__init__.py", line 1087, in emit
    self.flush()
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\logging\__init__.py", line 1067, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\metric_collector.py", line 29, in <module>
    check_process_mem_usage(sys.stdin)
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
Message: '%s:%d'
Arguments: ('15704', 0)
--- Logging error ---
Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\_pswindows.py", line 688, in wrapper
    return fun(self, *args, **kwargs)
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\_pswindows.py", line 942, in create_time
    user, system, created = cext.proc_times(self.pid)
ProcessLookupError: [Errno 3] assume no such process (originated from GetExitCodeProcess != STILL_ACTIVE)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\__init__.py", line 361, in _init
    self.create_time()
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\__init__.py", line 717, in create_time
    self._create_time = self._proc.create_time()
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\_pswindows.py", line 690, in wrapper
    raise convert_oserror(err, pid=self.pid, name=self._name)
psutil.NoSuchProcess: process no longer exists (pid=17772)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\process_memory_metric.py", line 20, in get_cpu_usage
    process = psutil.Process(int(pid))
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\__init__.py", line 332, in __init__
    self._init(pid)
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\__init__.py", line 373, in _init
    raise NoSuchProcess(pid, msg='process PID not found')
psutil.NoSuchProcess: process PID not found (pid=17772)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\logging\__init__.py", line 1087, in emit
    self.flush()
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\logging\__init__.py", line 1067, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\metric_collector.py", line 29, in <module>
    check_process_mem_usage(sys.stdin)
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\process_memory_metric.py", line 22, in get_cpu_usage
    logging.error("Failed get process for pid: %s", pid, exc_info=True)
Message: 'Failed get process for pid: %s'
Arguments: ('17772',)
--- Logging error ---
Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\logging\__init__.py", line 1087, in emit
    self.flush()
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\logging\__init__.py", line 1067, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\metric_collector.py", line 29, in <module>
    check_process_mem_usage(sys.stdin)
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
Message: '%s:%d'
Arguments: ('17772', 0)
--- Logging error ---
Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\_pswindows.py", line 688, in wrapper
    return fun(self, *args, **kwargs)
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\_pswindows.py", line 942, in create_time
    user, system, created = cext.proc_times(self.pid)
ProcessLookupError: [Errno 3] assume no such process (originated from GetExitCodeProcess != STILL_ACTIVE)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\__init__.py", line 361, in _init
    self.create_time()
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\__init__.py", line 717, in create_time
    self._create_time = self._proc.create_time()
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\_pswindows.py", line 690, in wrapper
    raise convert_oserror(err, pid=self.pid, name=self._name)
psutil.NoSuchProcess: process no longer exists (pid=16188)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\process_memory_metric.py", line 20, in get_cpu_usage
    process = psutil.Process(int(pid))
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\__init__.py", line 332, in __init__
    self._init(pid)
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\psutil\__init__.py", line 373, in _init
    raise NoSuchProcess(pid, msg='process PID not found')
psutil.NoSuchProcess: process PID not found (pid=16188)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\logging\__init__.py", line 1087, in emit
    self.flush()
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\logging\__init__.py", line 1067, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\metric_collector.py", line 29, in <module>
    check_process_mem_usage(sys.stdin)
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\process_memory_metric.py", line 22, in get_cpu_usage
    logging.error("Failed get process for pid: %s", pid, exc_info=True)
Message: 'Failed get process for pid: %s'
Arguments: ('16188',)
--- Logging error ---
Traceback (most recent call last):
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\logging\__init__.py", line 1087, in emit
    self.flush()
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\logging\__init__.py", line 1067, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\metric_collector.py", line 29, in <module>
    check_process_mem_usage(sys.stdin)
  File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
Message: '%s:%d'
Arguments: ('16188', 0)
Exception ignored in: <_io.TextIOWrapper name='<stdout>' mode='w' encoding='cp1252'>
OSError: [Errno 22] Invalid argument

2022-04-17T11:35:48,364 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-04-17T11:35:48,364 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-04-17T11:35:49,185 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:35:49,185 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - [PID]17140
2022-04-17T11:35:49,186 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:49,186 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:35:49,186 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:49,186 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-04-17T11:35:49,186 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:35:49,186 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-04-17T11:35:49,187 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188149187
2022-04-17T11:35:49,187 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188149187
2022-04-17T11:35:49,187 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9004).
2022-04-17T11:35:49,188 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:35:49,309 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-04-17T11:35:49,309 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-04-17T11:35:49,431 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:35:49,432 [INFO ] nioEventLoopGroup-5-17 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:49,432 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:35:49,432 [INFO ] nioEventLoopGroup-5-17 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:49,432 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:49,432 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:35:49,432 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:49,433 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:49,432 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:35:49,433 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:49,435 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:49,433 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:35:49,435 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:49,435 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:49,435 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:35:49,435 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:49,436 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stderr
2022-04-17T11:35:49,435 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:35:49,436 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stderr
2022-04-17T11:35:49,436 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stdout
2022-04-17T11:35:49,436 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stdout
2022-04-17T11:35:49,436 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:35:49,437 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 34 seconds.
2022-04-17T11:35:49,437 [INFO ] W-9004-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stdout
2022-04-17T11:35:49,437 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 34 seconds.
2022-04-17T11:35:49,437 [INFO ] W-9004-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stdout
2022-04-17T11:35:49,439 [INFO ] W-9004-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stderr
2022-04-17T11:35:49,439 [INFO ] W-9004-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stderr
2022-04-17T11:35:50,197 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:35:50,197 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - [PID]17324
2022-04-17T11:35:50,198 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:50,198 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:35:50,198 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:50,198 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-04-17T11:35:50,198 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:35:50,198 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-04-17T11:35:50,200 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188150200
2022-04-17T11:35:50,200 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188150200
2022-04-17T11:35:50,200 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9003).
2022-04-17T11:35:50,201 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:35:50,451 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:35:50,452 [INFO ] nioEventLoopGroup-5-18 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:50,452 [INFO ] nioEventLoopGroup-5-18 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:50,452 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:50,452 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:35:50,452 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:50,453 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:50,452 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:35:50,453 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:50,454 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:50,453 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:35:50,454 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:50,455 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:50,455 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:35:50,455 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:50,455 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:35:50,455 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stderr
2022-04-17T11:35:50,455 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stderr
2022-04-17T11:35:50,456 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stdout
2022-04-17T11:35:50,456 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:35:50,456 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stdout
2022-04-17T11:35:50,456 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 34 seconds.
2022-04-17T11:35:50,456 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:35:50,456 [INFO ] W-9003-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stdout
2022-04-17T11:35:50,456 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 34 seconds.
2022-04-17T11:35:50,456 [INFO ] W-9003-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stdout
2022-04-17T11:35:50,458 [INFO ] W-9003-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stderr
2022-04-17T11:35:50,458 [INFO ] W-9003-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stderr
2022-04-17T11:35:51,866 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-04-17T11:35:51,866 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-04-17T11:35:52,139 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9008]
2022-04-17T11:35:52,139 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9008]
2022-04-17T11:35:52,189 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-04-17T11:35:52,189 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-04-17T11:35:52,229 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-04-17T11:35:52,229 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-04-17T11:35:52,268 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-04-17T11:35:52,268 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-04-17T11:35:52,363 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-04-17T11:35:52,363 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-04-17T11:35:52,808 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:35:52,810 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - [PID]18412
2022-04-17T11:35:52,810 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:35:52,811 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:35:52,811 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:52,811 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:52,812 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-04-17T11:35:52,812 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-04-17T11:35:52,814 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188152814
2022-04-17T11:35:52,814 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188152814
2022-04-17T11:35:52,814 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9005).
2022-04-17T11:35:52,815 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:35:53,123 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:35:53,124 [INFO ] nioEventLoopGroup-5-19 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:53,124 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:35:53,124 [INFO ] nioEventLoopGroup-5-19 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:53,125 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:53,124 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:35:53,125 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:53,125 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:35:53,125 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:53,125 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:35:53,125 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:53,126 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:35:53,127 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:53,127 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:53,128 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:53,127 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:35:53,128 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:53,128 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:35:53,128 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stderr
2022-04-17T11:35:53,128 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stderr
2022-04-17T11:35:53,129 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stdout
2022-04-17T11:35:53,129 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:35:53,129 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stdout
2022-04-17T11:35:53,129 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 34 seconds.
2022-04-17T11:35:53,129 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:35:53,130 [INFO ] W-9005-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stdout
2022-04-17T11:35:53,129 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 34 seconds.
2022-04-17T11:35:53,130 [INFO ] W-9005-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stdout
2022-04-17T11:35:53,131 [INFO ] W-9005-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stderr
2022-04-17T11:35:53,131 [INFO ] W-9005-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stderr
2022-04-17T11:35:53,198 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:35:53,199 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - [PID]18180
2022-04-17T11:35:53,199 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:53,199 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:35:53,199 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:53,200 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:35:53,200 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9008
2022-04-17T11:35:53,200 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9008
2022-04-17T11:35:53,202 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188153202
2022-04-17T11:35:53,202 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188153202
2022-04-17T11:35:53,202 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9008).
2022-04-17T11:35:53,203 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:35:53,271 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:35:53,272 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - [PID]17556
2022-04-17T11:35:53,273 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:53,273 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:35:53,273 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:53,274 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-04-17T11:35:53,273 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:35:53,274 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-04-17T11:35:53,275 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188153275
2022-04-17T11:35:53,275 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188153275
2022-04-17T11:35:53,275 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9000).
2022-04-17T11:35:53,276 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:35:53,304 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:35:53,306 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - [PID]17116
2022-04-17T11:35:53,306 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:53,306 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:35:53,306 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:53,307 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-04-17T11:35:53,307 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:35:53,307 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-04-17T11:35:53,309 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188153309
2022-04-17T11:35:53,309 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188153309
2022-04-17T11:35:53,309 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9002).
2022-04-17T11:35:53,311 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:35:53,325 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:35:53,326 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - [PID]5812
2022-04-17T11:35:53,327 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:53,327 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:35:53,327 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:53,327 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-04-17T11:35:53,327 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:35:53,327 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-04-17T11:35:53,329 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188153329
2022-04-17T11:35:53,329 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188153329
2022-04-17T11:35:53,329 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9007).
2022-04-17T11:35:53,330 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:35:53,421 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:35:53,422 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - [PID]684
2022-04-17T11:35:53,422 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:53,422 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:35:53,422 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:35:53,423 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-04-17T11:35:53,423 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:35:53,423 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-04-17T11:35:53,425 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188153425
2022-04-17T11:35:53,425 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188153425
2022-04-17T11:35:53,425 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9006).
2022-04-17T11:35:53,426 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:35:53,515 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:35:53,515 [INFO ] nioEventLoopGroup-5-20 org.pytorch.serve.wlm.WorkerThread - 9008 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:53,515 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:35:53,515 [INFO ] nioEventLoopGroup-5-20 org.pytorch.serve.wlm.WorkerThread - 9008 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:53,516 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:35:53,516 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:53,516 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:53,516 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:35:53,517 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:53,517 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:35:53,517 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:53,519 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:53,519 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:35:53,519 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:53,519 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:53,519 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:35:53,519 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:53,520 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stderr
2022-04-17T11:35:53,520 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:35:53,520 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stderr
2022-04-17T11:35:53,521 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stdout
2022-04-17T11:35:53,520 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:35:53,521 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stdout
2022-04-17T11:35:53,521 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9008 in 34 seconds.
2022-04-17T11:35:53,521 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:35:53,521 [INFO ] W-9008-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stdout
2022-04-17T11:35:53,521 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9008 in 34 seconds.
2022-04-17T11:35:53,521 [INFO ] W-9008-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stdout
2022-04-17T11:35:53,523 [INFO ] W-9008-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stderr
2022-04-17T11:35:53,523 [INFO ] W-9008-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stderr
2022-04-17T11:35:53,587 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:35:53,587 [INFO ] nioEventLoopGroup-5-21 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:53,587 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:35:53,587 [INFO ] nioEventLoopGroup-5-21 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:53,588 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:53,587 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:35:53,588 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:53,588 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:35:53,588 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:53,588 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:35:53,588 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:53,590 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:53,588 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:35:53,590 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:53,590 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:53,590 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:35:53,590 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:53,590 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:35:53,591 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stderr
2022-04-17T11:35:53,591 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stderr
2022-04-17T11:35:53,591 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:35:53,591 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stdout
2022-04-17T11:35:53,591 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stdout
2022-04-17T11:35:53,591 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 34 seconds.
2022-04-17T11:35:53,591 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:35:53,591 [INFO ] W-9000-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stdout
2022-04-17T11:35:53,591 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 34 seconds.
2022-04-17T11:35:53,591 [INFO ] W-9000-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stdout
2022-04-17T11:35:53,593 [INFO ] W-9000-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stderr
2022-04-17T11:35:53,593 [INFO ] W-9000-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stderr
2022-04-17T11:35:53,613 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:35:53,614 [INFO ] nioEventLoopGroup-5-22 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:53,614 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:35:53,614 [INFO ] nioEventLoopGroup-5-22 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:53,614 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:53,614 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:35:53,614 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:53,614 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:35:53,614 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:53,614 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:53,615 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:35:53,616 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:53,616 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:53,617 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:53,616 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:35:53,617 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:53,617 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:35:53,618 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stderr
2022-04-17T11:35:53,618 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:35:53,618 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stderr
2022-04-17T11:35:53,618 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stdout
2022-04-17T11:35:53,618 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:35:53,618 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stdout
2022-04-17T11:35:53,619 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 34 seconds.
2022-04-17T11:35:53,618 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:35:53,619 [INFO ] W-9002-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stdout
2022-04-17T11:35:53,619 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 34 seconds.
2022-04-17T11:35:53,619 [INFO ] W-9002-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stdout
2022-04-17T11:35:53,620 [INFO ] W-9002-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stderr
2022-04-17T11:35:53,620 [INFO ] W-9002-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stderr
2022-04-17T11:35:53,635 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:35:53,636 [INFO ] nioEventLoopGroup-5-23 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:53,636 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:35:53,636 [INFO ] nioEventLoopGroup-5-23 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:53,636 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:53,636 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:35:53,636 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:53,636 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:35:53,637 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:53,637 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:35:53,637 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:53,637 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:35:53,638 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:53,638 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:53,638 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:53,638 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:35:53,638 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:53,639 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:35:53,639 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stderr
2022-04-17T11:35:53,639 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:35:53,639 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stderr
2022-04-17T11:35:53,639 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stdout
2022-04-17T11:35:53,639 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:35:53,639 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stdout
2022-04-17T11:35:53,640 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 34 seconds.
2022-04-17T11:35:53,640 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-04-17T11:35:53,640 [INFO ] W-9007-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stdout
2022-04-17T11:35:53,640 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 34 seconds.
2022-04-17T11:35:53,640 [INFO ] W-9007-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stdout
2022-04-17T11:35:53,641 [INFO ] W-9007-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stderr
2022-04-17T11:35:53,641 [INFO ] W-9007-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stderr
2022-04-17T11:35:53,709 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:35:53,709 [INFO ] nioEventLoopGroup-5-24 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:53,709 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:35:53,709 [INFO ] nioEventLoopGroup-5-24 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-04-17T11:35:53,710 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:53,709 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:35:53,710 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:35:53,710 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:53,710 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:35:53,710 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:35:53,712 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:53,710 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:35:53,712 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:35:53,712 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:53,712 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:35:53,712 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:35:53,713 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stderr
2022-04-17T11:35:53,712 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:35:53,713 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stderr
2022-04-17T11:35:53,713 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stdout
2022-04-17T11:35:53,713 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:35:53,713 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stdout
2022-04-17T11:35:53,713 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 34 seconds.
2022-04-17T11:35:53,713 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:35:53,714 [INFO ] W-9006-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stdout
2022-04-17T11:35:53,713 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 34 seconds.
2022-04-17T11:35:53,714 [INFO ] W-9006-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stdout
2022-04-17T11:35:53,714 [INFO ] W-9006-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stderr
2022-04-17T11:35:53,714 [INFO ] W-9006-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stderr
2022-04-17T11:36:02,642 [INFO ] W-9006-foodnet_1.0 ACCESS_LOG - /[0:0:0:0:0:0:0:1]:55599 "GET /ping HTTP/1.1" 200 6
2022-04-17T11:36:02,643 [INFO ] W-9006-foodnet_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:null
2022-04-17T11:36:08,471 [INFO ] W-9006-foodnet_1.0 ACCESS_LOG - /[0:0:0:0:0:0:0:1]:55604 "GET /ping HTTP/1.1" 200 0
2022-04-17T11:36:08,472 [INFO ] W-9006-foodnet_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:null
2022-04-17T11:36:23,439 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-04-17T11:36:23,439 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-04-17T11:36:24,266 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:36:24,267 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - [PID]8980
2022-04-17T11:36:24,267 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:36:24,267 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:36:24,268 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:36:24,267 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:36:24,268 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-04-17T11:36:24,268 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-04-17T11:36:24,270 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188184270
2022-04-17T11:36:24,270 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188184270
2022-04-17T11:36:24,270 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9004).
2022-04-17T11:36:24,270 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:36:24,458 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-04-17T11:36:24,458 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-04-17T11:36:24,511 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:36:24,512 [INFO ] nioEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-04-17T11:36:24,511 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:36:24,512 [INFO ] nioEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-04-17T11:36:24,512 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:36:24,512 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:36:24,512 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:36:24,512 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:36:24,512 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:36:24,513 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:36:24,512 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:36:24,514 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:36:24,513 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:36:24,514 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:36:24,515 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:36:24,514 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:36:24,515 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:36:24,515 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:36:24,515 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stderr
2022-04-17T11:36:24,515 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stderr
2022-04-17T11:36:24,516 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stdout
2022-04-17T11:36:24,515 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:36:24,516 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stdout
2022-04-17T11:36:24,516 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 55 seconds.
2022-04-17T11:36:24,516 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:36:24,516 [INFO ] W-9004-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stdout
2022-04-17T11:36:24,516 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 55 seconds.
2022-04-17T11:36:24,516 [INFO ] W-9004-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stdout
2022-04-17T11:36:24,518 [INFO ] W-9004-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stderr
2022-04-17T11:36:24,518 [INFO ] W-9004-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stderr
2022-04-17T11:36:25,280 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:36:25,281 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - [PID]15660
2022-04-17T11:36:25,281 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:36:25,281 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:36:25,281 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:36:25,282 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-04-17T11:36:25,282 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:36:25,282 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-04-17T11:36:25,284 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188185284
2022-04-17T11:36:25,284 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9003).
2022-04-17T11:36:25,284 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188185284
2022-04-17T11:36:25,285 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:36:25,535 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:36:25,535 [INFO ] nioEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-04-17T11:36:25,535 [INFO ] nioEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-04-17T11:36:25,536 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:36:25,535 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:36:25,536 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:36:25,536 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:36:25,536 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:36:25,536 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:36:25,538 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:36:25,536 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:36:25,538 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:36:25,538 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:36:25,538 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:36:25,538 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:36:25,539 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stderr
2022-04-17T11:36:25,538 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:36:25,539 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stderr
2022-04-17T11:36:25,539 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stdout
2022-04-17T11:36:25,539 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:36:25,539 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stdout
2022-04-17T11:36:25,540 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 55 seconds.
2022-04-17T11:36:25,539 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:36:25,540 [INFO ] W-9003-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stdout
2022-04-17T11:36:25,540 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 55 seconds.
2022-04-17T11:36:25,540 [INFO ] W-9003-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stdout
2022-04-17T11:36:25,541 [INFO ] W-9003-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stderr
2022-04-17T11:36:25,541 [INFO ] W-9003-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stderr
2022-04-17T11:36:27,132 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-04-17T11:36:27,132 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-04-17T11:36:27,522 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9008]
2022-04-17T11:36:27,522 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9008]
2022-04-17T11:36:27,593 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-04-17T11:36:27,593 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-04-17T11:36:27,620 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-04-17T11:36:27,620 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-04-17T11:36:27,642 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-04-17T11:36:27,642 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-04-17T11:36:27,715 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-04-17T11:36:27,715 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-04-17T11:36:28,091 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:36:28,093 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - [PID]17236
2022-04-17T11:36:28,093 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:36:28,093 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:36:28,093 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:36:28,094 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-04-17T11:36:28,094 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:36:28,094 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-04-17T11:36:28,096 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188188096
2022-04-17T11:36:28,096 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188188096
2022-04-17T11:36:28,096 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9005).
2022-04-17T11:36:28,098 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:36:28,444 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:36:28,445 [INFO ] nioEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-04-17T11:36:28,445 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:36:28,445 [INFO ] nioEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-04-17T11:36:28,446 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:36:28,445 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:36:28,446 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:36:28,446 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:36:28,446 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:36:28,446 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:36:28,446 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:36:28,448 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:36:28,448 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:36:28,448 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:36:28,448 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:36:28,448 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:36:28,449 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stderr
2022-04-17T11:36:28,449 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:36:28,449 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stderr
2022-04-17T11:36:28,449 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stdout
2022-04-17T11:36:28,449 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:36:28,449 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stdout
2022-04-17T11:36:28,450 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 55 seconds.
2022-04-17T11:36:28,450 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:36:28,450 [INFO ] W-9005-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stdout
2022-04-17T11:36:28,450 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 55 seconds.
2022-04-17T11:36:28,450 [INFO ] W-9005-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stdout
2022-04-17T11:36:28,453 [INFO ] W-9005-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stderr
2022-04-17T11:36:28,453 [INFO ] W-9005-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stderr
2022-04-17T11:36:28,622 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:36:28,622 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - [PID]14752
2022-04-17T11:36:28,623 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:36:28,623 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:36:28,623 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:36:28,624 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9008
2022-04-17T11:36:28,623 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:36:28,624 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9008
2022-04-17T11:36:28,625 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188188625
2022-04-17T11:36:28,625 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188188625
2022-04-17T11:36:28,625 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9008).
2022-04-17T11:36:28,626 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:36:28,722 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:36:28,723 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - [PID]7268
2022-04-17T11:36:28,724 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:36:28,724 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:36:28,724 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:36:28,724 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-04-17T11:36:28,724 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:36:28,724 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-04-17T11:36:28,726 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188188726
2022-04-17T11:36:28,726 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188188726
2022-04-17T11:36:28,726 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9000).
2022-04-17T11:36:28,727 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:36:28,738 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:36:28,739 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - [PID]3652
2022-04-17T11:36:28,740 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:36:28,740 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:36:28,740 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:36:28,740 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-04-17T11:36:28,740 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:36:28,740 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-04-17T11:36:28,742 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188188742
2022-04-17T11:36:28,742 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188188742
2022-04-17T11:36:28,743 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9002).
2022-04-17T11:36:28,744 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:36:28,753 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:36:28,754 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - [PID]16780
2022-04-17T11:36:28,755 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:36:28,755 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:36:28,755 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:36:28,755 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-04-17T11:36:28,755 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:36:28,755 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-04-17T11:36:28,757 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188188757
2022-04-17T11:36:28,757 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188188757
2022-04-17T11:36:28,757 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9007).
2022-04-17T11:36:28,758 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:36:28,823 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:36:28,824 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - [PID]10284
2022-04-17T11:36:28,824 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:36:28,824 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:36:28,824 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:36:28,825 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-04-17T11:36:28,825 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:36:28,825 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-04-17T11:36:28,827 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188188827
2022-04-17T11:36:28,827 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188188827
2022-04-17T11:36:28,827 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9006).
2022-04-17T11:36:28,828 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:36:28,941 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:36:28,941 [INFO ] nioEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9008 Worker disconnected. WORKER_STARTED
2022-04-17T11:36:28,941 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:36:28,941 [INFO ] nioEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9008 Worker disconnected. WORKER_STARTED
2022-04-17T11:36:28,942 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:36:28,942 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:36:28,942 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:36:28,942 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:36:28,942 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:36:28,942 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:36:28,944 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:36:28,943 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:36:28,944 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:36:28,944 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:36:28,944 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:36:28,944 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:36:28,945 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stderr
2022-04-17T11:36:28,945 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:36:28,945 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stderr
2022-04-17T11:36:28,945 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stdout
2022-04-17T11:36:28,945 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:36:28,945 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stdout
2022-04-17T11:36:28,946 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9008 in 55 seconds.
2022-04-17T11:36:28,945 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:36:28,946 [INFO ] W-9008-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stdout
2022-04-17T11:36:28,946 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9008 in 55 seconds.
2022-04-17T11:36:28,946 [INFO ] W-9008-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stdout
2022-04-17T11:36:28,947 [INFO ] W-9008-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stderr
2022-04-17T11:36:28,947 [INFO ] W-9008-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stderr
2022-04-17T11:36:29,034 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:36:29,034 [INFO ] nioEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-04-17T11:36:29,034 [INFO ] nioEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-04-17T11:36:29,035 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:36:29,035 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:36:29,035 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:36:29,035 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:36:29,036 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:36:29,036 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:36:29,036 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:36:29,036 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:36:29,038 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:36:29,038 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:36:29,038 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:36:29,038 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:36:29,038 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:36:29,038 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:36:29,039 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:36:29,039 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stderr
2022-04-17T11:36:29,039 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stderr
2022-04-17T11:36:29,039 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stdout
2022-04-17T11:36:29,039 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:36:29,039 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stdout
2022-04-17T11:36:29,040 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 55 seconds.
2022-04-17T11:36:29,040 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:36:29,040 [INFO ] W-9000-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stdout
2022-04-17T11:36:29,040 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 55 seconds.
2022-04-17T11:36:29,040 [INFO ] W-9000-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stdout
2022-04-17T11:36:29,042 [INFO ] W-9000-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stderr
2022-04-17T11:36:29,042 [INFO ] W-9000-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stderr
2022-04-17T11:36:29,048 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:36:29,049 [INFO ] nioEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-04-17T11:36:29,049 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:36:29,049 [INFO ] nioEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-04-17T11:36:29,049 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:36:29,049 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:36:29,049 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:36:29,050 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:36:29,050 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:36:29,050 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:36:29,050 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:36:29,050 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:36:29,052 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:36:29,052 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:36:29,052 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:36:29,052 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:36:29,052 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:36:29,052 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:36:29,053 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stderr
2022-04-17T11:36:29,053 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:36:29,053 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stderr
2022-04-17T11:36:29,053 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stdout
2022-04-17T11:36:29,053 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:36:29,053 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stdout
2022-04-17T11:36:29,054 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 55 seconds.
2022-04-17T11:36:29,053 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-04-17T11:36:29,054 [INFO ] W-9002-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stdout
2022-04-17T11:36:29,054 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 55 seconds.
2022-04-17T11:36:29,054 [INFO ] W-9002-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stdout
2022-04-17T11:36:29,055 [INFO ] W-9002-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stderr
2022-04-17T11:36:29,055 [INFO ] W-9002-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stderr
2022-04-17T11:36:29,062 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:36:29,062 [INFO ] nioEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-04-17T11:36:29,062 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:36:29,062 [INFO ] nioEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-04-17T11:36:29,063 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:36:29,062 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:36:29,063 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:36:29,063 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:36:29,063 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:36:29,063 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:36:29,063 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:36:29,065 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:36:29,064 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:36:29,065 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:36:29,065 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:36:29,065 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:36:29,065 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:36:29,066 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stderr
2022-04-17T11:36:29,066 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:36:29,066 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stderr
2022-04-17T11:36:29,066 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stdout
2022-04-17T11:36:29,066 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:36:29,067 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:36:29,067 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-04-17T11:36:29,066 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stdout
2022-04-17T11:36:29,068 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 55 seconds.
2022-04-17T11:36:29,067 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-04-17T11:36:29,068 [INFO ] W-9007-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stdout
2022-04-17T11:36:29,068 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 55 seconds.
2022-04-17T11:36:29,068 [INFO ] W-9007-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stdout
2022-04-17T11:36:29,069 [INFO ] W-9007-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stderr
2022-04-17T11:36:29,069 [INFO ] W-9007-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stderr
2022-04-17T11:36:29,123 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:36:29,124 [INFO ] nioEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-04-17T11:36:29,124 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:36:29,124 [INFO ] nioEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-04-17T11:36:29,124 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:36:29,124 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:36:29,124 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:36:29,124 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:36:29,125 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:36:29,125 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:36:29,125 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:36:29,126 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:36:29,125 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:36:29,126 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:36:29,127 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:36:29,127 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:36:29,127 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:36:29,127 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:36:29,127 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stderr
2022-04-17T11:36:29,127 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stderr
2022-04-17T11:36:29,128 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stdout
2022-04-17T11:36:29,127 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:36:29,128 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stdout
2022-04-17T11:36:29,128 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 55 seconds.
2022-04-17T11:36:29,128 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:36:29,128 [INFO ] W-9006-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stdout
2022-04-17T11:36:29,128 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 55 seconds.
2022-04-17T11:36:29,128 [INFO ] W-9006-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stdout
2022-04-17T11:36:29,129 [INFO ] W-9006-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stderr
2022-04-17T11:36:29,129 [INFO ] W-9006-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stderr
2022-04-17T11:37:04,168 [INFO ] W-9006-foodnet_1.0 ACCESS_LOG - /[0:0:0:0:0:0:0:1]:55637 "GET /ping HTTP/1.1" 200 1
2022-04-17T11:37:04,168 [INFO ] W-9006-foodnet_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:null
2022-04-17T11:37:19,519 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-04-17T11:37:19,519 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-04-17T11:37:20,348 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:37:20,349 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - [PID]16712
2022-04-17T11:37:20,350 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:37:20,350 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:37:20,350 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:37:20,350 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-04-17T11:37:20,350 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:37:20,350 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-04-17T11:37:20,352 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188240352
2022-04-17T11:37:20,352 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188240352
2022-04-17T11:37:20,352 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9004).
2022-04-17T11:37:20,353 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:37:20,541 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-04-17T11:37:20,541 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-04-17T11:37:20,608 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:37:20,609 [INFO ] nioEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-04-17T11:37:20,609 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:37:20,609 [INFO ] nioEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-04-17T11:37:20,609 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:37:20,609 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:37:20,609 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:37:20,610 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:37:20,610 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:37:20,610 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:37:20,611 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:37:20,610 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:37:20,611 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:37:20,612 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:37:20,612 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:37:20,612 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:37:20,612 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:37:20,613 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stderr
2022-04-17T11:37:20,613 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stderr
2022-04-17T11:37:20,613 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stdout
2022-04-17T11:37:20,613 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:37:20,613 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stdout
2022-04-17T11:37:20,613 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 89 seconds.
2022-04-17T11:37:20,613 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:37:20,614 [INFO ] W-9004-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stdout
2022-04-17T11:37:20,613 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 89 seconds.
2022-04-17T11:37:20,614 [INFO ] W-9004-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stdout
2022-04-17T11:37:20,614 [INFO ] W-9004-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stderr
2022-04-17T11:37:20,614 [INFO ] W-9004-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stderr
2022-04-17T11:37:21,331 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:37:21,332 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - [PID]16672
2022-04-17T11:37:21,332 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:37:21,332 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:37:21,332 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:37:21,332 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:37:21,333 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-04-17T11:37:21,333 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-04-17T11:37:21,334 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188241334
2022-04-17T11:37:21,334 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188241334
2022-04-17T11:37:21,334 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9003).
2022-04-17T11:37:21,335 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:37:21,582 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:37:21,583 [INFO ] nioEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-04-17T11:37:21,583 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:37:21,583 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:37:21,583 [INFO ] nioEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-04-17T11:37:21,584 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:37:21,583 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:37:21,584 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:37:21,584 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:37:21,584 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:37:21,584 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:37:21,586 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:37:21,584 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:37:21,586 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:37:21,586 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:37:21,586 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:37:21,586 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:37:21,587 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stderr
2022-04-17T11:37:21,587 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:37:21,587 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stderr
2022-04-17T11:37:21,587 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stdout
2022-04-17T11:37:21,587 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:37:21,587 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stdout
2022-04-17T11:37:21,588 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 89 seconds.
2022-04-17T11:37:21,588 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:37:21,588 [INFO ] W-9003-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stdout
2022-04-17T11:37:21,588 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 89 seconds.
2022-04-17T11:37:21,588 [INFO ] W-9003-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stdout
2022-04-17T11:37:21,589 [INFO ] W-9003-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stderr
2022-04-17T11:37:21,589 [INFO ] W-9003-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stderr
2022-04-17T11:37:23,452 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-04-17T11:37:23,452 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-04-17T11:37:23,948 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9008]
2022-04-17T11:37:23,948 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9008]
2022-04-17T11:37:24,043 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-04-17T11:37:24,043 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-04-17T11:37:24,057 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-04-17T11:37:24,057 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-04-17T11:37:24,071 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-04-17T11:37:24,071 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-04-17T11:37:24,131 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-04-17T11:37:24,131 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-04-17T11:37:24,364 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:37:24,365 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - [PID]17136
2022-04-17T11:37:24,366 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:37:24,366 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:37:24,366 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:37:24,366 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-04-17T11:37:24,366 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:37:24,366 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-04-17T11:37:24,368 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188244368
2022-04-17T11:37:24,368 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188244368
2022-04-17T11:37:24,368 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9005).
2022-04-17T11:37:24,369 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:37:24,687 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:37:24,688 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:37:24,688 [INFO ] nioEventLoopGroup-5-11 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-04-17T11:37:24,688 [INFO ] nioEventLoopGroup-5-11 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-04-17T11:37:24,689 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:37:24,688 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:37:24,689 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:37:24,689 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:37:24,689 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:37:24,690 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:37:24,689 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:37:24,690 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:37:24,692 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:37:24,692 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:37:24,692 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:37:24,692 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:37:24,692 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:37:24,692 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:37:24,693 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stderr
2022-04-17T11:37:24,693 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stderr
2022-04-17T11:37:24,693 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stdout
2022-04-17T11:37:24,693 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:37:24,693 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stdout
2022-04-17T11:37:24,693 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:37:24,694 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 89 seconds.
2022-04-17T11:37:24,694 [INFO ] W-9005-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stdout
2022-04-17T11:37:24,694 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 89 seconds.
2022-04-17T11:37:24,694 [INFO ] W-9005-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stdout
2022-04-17T11:37:24,696 [INFO ] W-9005-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stderr
2022-04-17T11:37:24,696 [INFO ] W-9005-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stderr
2022-04-17T11:37:24,995 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:37:24,995 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - [PID]11156
2022-04-17T11:37:24,996 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:37:24,996 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:37:24,996 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:37:24,997 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9008
2022-04-17T11:37:24,996 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:37:24,997 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9008
2022-04-17T11:37:24,998 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188244998
2022-04-17T11:37:24,998 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188244998
2022-04-17T11:37:24,998 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9008).
2022-04-17T11:37:25,000 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:37:25,078 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:37:25,079 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - [PID]17540
2022-04-17T11:37:25,080 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:37:25,080 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:37:25,080 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:37:25,080 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-04-17T11:37:25,080 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:37:25,080 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-04-17T11:37:25,082 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188245082
2022-04-17T11:37:25,082 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188245082
2022-04-17T11:37:25,082 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9000).
2022-04-17T11:37:25,083 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:37:25,098 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:37:25,099 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - [PID]13636
2022-04-17T11:37:25,099 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:37:25,099 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:37:25,099 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:37:25,100 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-04-17T11:37:25,100 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:37:25,100 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-04-17T11:37:25,102 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188245102
2022-04-17T11:37:25,102 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188245102
2022-04-17T11:37:25,102 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9002).
2022-04-17T11:37:25,104 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:37:25,121 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:37:25,122 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - [PID]4620
2022-04-17T11:37:25,123 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:37:25,123 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:37:25,123 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:37:25,123 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-04-17T11:37:25,123 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:37:25,123 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-04-17T11:37:25,125 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188245125
2022-04-17T11:37:25,125 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188245125
2022-04-17T11:37:25,125 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9007).
2022-04-17T11:37:25,126 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:37:25,164 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:37:25,165 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - [PID]9952
2022-04-17T11:37:25,165 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:37:25,165 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:37:25,165 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:37:25,165 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-04-17T11:37:25,165 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:37:25,165 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-04-17T11:37:25,167 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188245167
2022-04-17T11:37:25,167 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188245167
2022-04-17T11:37:25,168 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9006).
2022-04-17T11:37:25,169 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:37:25,324 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:37:25,325 [INFO ] nioEventLoopGroup-5-12 org.pytorch.serve.wlm.WorkerThread - 9008 Worker disconnected. WORKER_STARTED
2022-04-17T11:37:25,325 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:37:25,325 [INFO ] nioEventLoopGroup-5-12 org.pytorch.serve.wlm.WorkerThread - 9008 Worker disconnected. WORKER_STARTED
2022-04-17T11:37:25,326 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:37:25,325 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:37:25,326 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:37:25,326 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:37:25,326 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:37:25,326 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:37:25,326 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:37:25,328 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:37:25,327 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:37:25,328 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:37:25,329 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:37:25,328 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:37:25,329 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:37:25,329 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:37:25,329 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stderr
2022-04-17T11:37:25,329 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stderr
2022-04-17T11:37:25,330 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stdout
2022-04-17T11:37:25,329 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:37:25,330 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stdout
2022-04-17T11:37:25,330 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9008 in 89 seconds.
2022-04-17T11:37:25,330 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:37:25,330 [INFO ] W-9008-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stdout
2022-04-17T11:37:25,330 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9008 in 89 seconds.
2022-04-17T11:37:25,330 [INFO ] W-9008-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stdout
2022-04-17T11:37:25,333 [INFO ] W-9008-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stderr
2022-04-17T11:37:25,333 [INFO ] W-9008-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stderr
2022-04-17T11:37:25,403 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:37:25,404 [INFO ] nioEventLoopGroup-5-13 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-04-17T11:37:25,404 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:37:25,404 [INFO ] nioEventLoopGroup-5-13 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-04-17T11:37:25,404 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:37:25,404 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:37:25,404 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:37:25,405 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:37:25,405 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:37:25,405 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:37:25,405 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:37:25,407 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:37:25,406 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:37:25,407 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:37:25,407 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:37:25,407 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:37:25,407 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:37:25,407 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:37:25,408 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stderr
2022-04-17T11:37:25,408 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stderr
2022-04-17T11:37:25,408 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stdout
2022-04-17T11:37:25,408 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:37:25,408 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stdout
2022-04-17T11:37:25,409 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 89 seconds.
2022-04-17T11:37:25,408 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:37:25,409 [INFO ] W-9000-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stdout
2022-04-17T11:37:25,409 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 89 seconds.
2022-04-17T11:37:25,409 [INFO ] W-9000-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stdout
2022-04-17T11:37:25,410 [INFO ] W-9000-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stderr
2022-04-17T11:37:25,410 [INFO ] W-9000-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stderr
2022-04-17T11:37:25,423 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:37:25,424 [INFO ] nioEventLoopGroup-5-14 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-04-17T11:37:25,423 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:37:25,424 [INFO ] nioEventLoopGroup-5-14 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-04-17T11:37:25,424 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:37:25,424 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:37:25,424 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:37:25,424 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:37:25,424 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:37:25,425 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:37:25,424 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:37:25,426 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:37:25,425 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:37:25,426 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:37:25,426 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:37:25,426 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:37:25,426 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:37:25,427 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stderr
2022-04-17T11:37:25,426 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:37:25,427 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stderr
2022-04-17T11:37:25,427 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stdout
2022-04-17T11:37:25,427 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:37:25,427 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stdout
2022-04-17T11:37:25,428 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 89 seconds.
2022-04-17T11:37:25,427 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:37:25,428 [INFO ] W-9002-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stdout
2022-04-17T11:37:25,428 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 89 seconds.
2022-04-17T11:37:25,428 [INFO ] W-9002-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stdout
2022-04-17T11:37:25,429 [INFO ] W-9002-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stderr
2022-04-17T11:37:25,429 [INFO ] W-9002-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stderr
2022-04-17T11:37:25,444 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:37:25,445 [INFO ] nioEventLoopGroup-5-15 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-04-17T11:37:25,445 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:37:25,445 [INFO ] nioEventLoopGroup-5-15 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-04-17T11:37:25,446 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:37:25,445 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:37:25,446 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:37:25,446 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:37:25,446 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:37:25,446 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:37:25,446 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:37:25,448 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:37:25,447 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:37:25,448 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:37:25,448 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:37:25,448 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:37:25,448 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:37:25,448 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:37:25,449 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stderr
2022-04-17T11:37:25,449 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stderr
2022-04-17T11:37:25,449 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stdout
2022-04-17T11:37:25,449 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:37:25,449 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stdout
2022-04-17T11:37:25,450 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 89 seconds.
2022-04-17T11:37:25,449 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:37:25,450 [INFO ] W-9007-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stdout
2022-04-17T11:37:25,450 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 89 seconds.
2022-04-17T11:37:25,450 [INFO ] W-9007-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stdout
2022-04-17T11:37:25,452 [INFO ] W-9007-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stderr
2022-04-17T11:37:25,452 [INFO ] W-9007-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stderr
2022-04-17T11:37:25,486 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:37:25,486 [INFO ] nioEventLoopGroup-5-16 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-04-17T11:37:25,486 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:37:25,486 [INFO ] nioEventLoopGroup-5-16 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-04-17T11:37:25,487 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:37:25,486 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:37:25,487 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:37:25,487 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:37:25,487 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:37:25,487 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:37:25,487 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:37:25,488 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:37:25,487 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:37:25,488 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:37:25,489 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:37:25,489 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:37:25,489 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:37:25,489 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:37:25,489 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stderr
2022-04-17T11:37:25,489 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:37:25,489 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stderr
2022-04-17T11:37:25,490 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stdout
2022-04-17T11:37:25,489 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:37:25,490 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stdout
2022-04-17T11:37:25,490 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 89 seconds.
2022-04-17T11:37:25,490 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-04-17T11:37:25,490 [INFO ] W-9006-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stdout
2022-04-17T11:37:25,490 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 89 seconds.
2022-04-17T11:37:25,490 [INFO ] W-9006-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stdout
2022-04-17T11:37:25,491 [INFO ] W-9006-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stderr
2022-04-17T11:37:25,491 [INFO ] W-9006-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stderr
2022-04-17T11:38:49,615 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-04-17T11:38:49,615 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-04-17T11:38:50,591 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-04-17T11:38:50,591 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-04-17T11:38:50,687 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:38:50,688 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - [PID]15644
2022-04-17T11:38:50,689 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:38:50,688 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:38:50,689 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:38:50,689 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-04-17T11:38:50,689 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:38:50,689 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-04-17T11:38:50,692 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188330692
2022-04-17T11:38:50,692 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9004).
2022-04-17T11:38:50,692 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188330692
2022-04-17T11:38:50,693 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:38:51,049 [INFO ] nioEventLoopGroup-5-17 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-04-17T11:38:51,049 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:38:51,049 [INFO ] nioEventLoopGroup-5-17 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-04-17T11:38:51,050 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:38:51,049 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:38:51,050 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:38:51,050 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:38:51,050 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:38:51,051 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:38:51,050 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:38:51,053 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:38:51,051 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:38:51,053 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:38:51,054 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:38:51,053 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:38:51,054 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:38:51,054 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stderr
2022-04-17T11:38:51,054 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:38:51,054 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stderr
2022-04-17T11:38:51,055 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stdout
2022-04-17T11:38:51,055 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:38:51,055 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stdout
2022-04-17T11:38:51,056 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 144 seconds.
2022-04-17T11:38:51,055 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:38:51,056 [INFO ] W-9004-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stdout
2022-04-17T11:38:51,056 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 144 seconds.
2022-04-17T11:38:51,056 [INFO ] W-9004-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stdout
2022-04-17T11:38:51,057 [INFO ] W-9004-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stderr
2022-04-17T11:38:51,057 [INFO ] W-9004-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stderr
2022-04-17T11:38:51,671 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:38:51,672 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - [PID]16424
2022-04-17T11:38:51,673 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:38:51,673 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:38:51,673 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:38:51,674 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-04-17T11:38:51,673 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:38:51,674 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-04-17T11:38:51,675 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188331675
2022-04-17T11:38:51,675 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188331675
2022-04-17T11:38:51,676 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9003).
2022-04-17T11:38:51,676 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:38:51,998 [INFO ] nioEventLoopGroup-5-18 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-04-17T11:38:51,998 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:38:51,998 [INFO ] nioEventLoopGroup-5-18 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-04-17T11:38:51,999 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:38:51,999 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:38:51,999 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:38:51,999 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:38:51,999 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:38:51,999 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:38:52,001 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:38:52,000 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:38:52,001 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:38:52,002 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:38:52,002 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:38:52,002 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:38:52,003 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stderr
2022-04-17T11:38:52,002 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:38:52,003 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stderr
2022-04-17T11:38:52,003 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stdout
2022-04-17T11:38:52,003 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:38:52,003 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stdout
2022-04-17T11:38:52,003 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 144 seconds.
2022-04-17T11:38:52,003 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:38:52,004 [INFO ] W-9003-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stdout
2022-04-17T11:38:52,003 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 144 seconds.
2022-04-17T11:38:52,004 [INFO ] W-9003-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stdout
2022-04-17T11:38:52,005 [INFO ] W-9003-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stderr
2022-04-17T11:38:52,005 [INFO ] W-9003-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stderr
2022-04-17T11:38:53,694 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-04-17T11:38:53,694 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-04-17T11:38:54,331 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9008]
2022-04-17T11:38:54,331 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9008]
2022-04-17T11:38:54,411 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-04-17T11:38:54,411 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-04-17T11:38:54,429 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-04-17T11:38:54,429 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-04-17T11:38:54,452 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-04-17T11:38:54,452 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-04-17T11:38:54,493 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-04-17T11:38:54,493 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-04-17T11:38:54,786 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:38:54,788 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - [PID]8384
2022-04-17T11:38:54,789 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:38:54,789 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:38:54,789 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:38:54,790 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-04-17T11:38:54,789 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:38:54,790 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-04-17T11:38:54,792 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188334792
2022-04-17T11:38:54,792 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9005).
2022-04-17T11:38:54,792 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188334792
2022-04-17T11:38:54,795 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:38:55,361 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:38:55,362 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:38:55,363 [INFO ] nioEventLoopGroup-5-19 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-04-17T11:38:55,363 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:38:55,363 [INFO ] nioEventLoopGroup-5-19 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-04-17T11:38:55,364 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:38:55,363 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:38:55,364 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:38:55,364 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:38:55,365 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:38:55,365 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:38:55,365 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:38:55,369 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:38:55,366 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:38:55,369 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:38:55,370 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:38:55,369 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:38:55,370 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:38:55,371 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stderr
2022-04-17T11:38:55,370 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:38:55,371 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stderr
2022-04-17T11:38:55,371 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stdout
2022-04-17T11:38:55,371 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:38:55,371 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stdout
2022-04-17T11:38:55,372 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 144 seconds.
2022-04-17T11:38:55,372 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-04-17T11:38:55,373 [INFO ] W-9005-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stdout
2022-04-17T11:38:55,372 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 144 seconds.
2022-04-17T11:38:55,373 [INFO ] W-9005-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stdout
2022-04-17T11:38:55,381 [INFO ] W-9005-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stderr
2022-04-17T11:38:55,381 [INFO ] W-9005-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stderr
2022-04-17T11:38:56,519 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:38:56,521 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - [PID]1496
2022-04-17T11:38:56,522 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:38:56,522 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:38:56,522 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:38:56,523 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9008
2022-04-17T11:38:56,523 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:38:56,523 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9008
2022-04-17T11:38:56,526 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188336526
2022-04-17T11:38:56,526 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188336526
2022-04-17T11:38:56,526 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9008).
2022-04-17T11:38:56,528 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:38:56,641 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:38:56,642 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - [PID]13824
2022-04-17T11:38:56,643 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:38:56,643 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:38:56,643 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:38:56,644 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-04-17T11:38:56,643 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:38:56,644 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-04-17T11:38:56,647 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188336647
2022-04-17T11:38:56,647 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188336647
2022-04-17T11:38:56,647 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9000).
2022-04-17T11:38:56,649 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:38:56,683 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:38:56,685 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - [PID]16844
2022-04-17T11:38:56,686 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:38:56,686 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:38:56,686 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:38:56,687 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-04-17T11:38:56,686 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:38:56,687 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-04-17T11:38:56,690 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188336690
2022-04-17T11:38:56,690 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188336690
2022-04-17T11:38:56,690 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9002).
2022-04-17T11:38:56,691 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:38:56,735 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:38:56,736 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - [PID]17188
2022-04-17T11:38:56,737 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:38:56,737 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:38:56,737 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:38:56,739 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-04-17T11:38:56,738 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:38:56,739 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-04-17T11:38:56,742 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188336742
2022-04-17T11:38:56,742 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188336742
2022-04-17T11:38:56,742 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9007).
2022-04-17T11:38:56,743 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:38:56,763 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:38:56,766 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - [PID]11876
2022-04-17T11:38:56,767 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:38:56,767 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:38:56,767 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:38:56,767 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:38:56,769 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-04-17T11:38:56,769 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-04-17T11:38:56,772 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188336771
2022-04-17T11:38:56,772 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188336771
2022-04-17T11:38:56,772 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9006).
2022-04-17T11:38:56,774 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:38:57,178 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:38:57,179 [INFO ] nioEventLoopGroup-5-20 org.pytorch.serve.wlm.WorkerThread - 9008 Worker disconnected. WORKER_STARTED
2022-04-17T11:38:57,178 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:38:57,179 [INFO ] nioEventLoopGroup-5-20 org.pytorch.serve.wlm.WorkerThread - 9008 Worker disconnected. WORKER_STARTED
2022-04-17T11:38:57,179 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:38:57,180 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:38:57,180 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:38:57,181 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:38:57,181 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:38:57,182 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:38:57,180 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:38:57,182 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:38:57,184 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:38:57,185 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:38:57,185 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:38:57,184 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:38:57,189 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:38:57,186 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-04-17T11:38:57,189 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:38:57,190 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:38:57,189 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-04-17T11:38:57,190 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:38:57,192 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stderr
2022-04-17T11:38:57,191 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 790, in exec_module
2022-04-17T11:38:57,192 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
2022-04-17T11:38:57,192 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stderr
2022-04-17T11:38:57,193 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stdout
2022-04-17T11:38:57,192 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\AppData\Local\Temp\models\b31932e975844d80809048017333897f\handler.py", line 3, in <module>
2022-04-17T11:38:57,193 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stdout
2022-04-17T11:38:57,194 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9008 in 144 seconds.
2022-04-17T11:38:57,193 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier as ImgClsfr
2022-04-17T11:38:57,194 [INFO ] W-9008-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stdout
2022-04-17T11:38:57,194 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9008 in 144 seconds.
2022-04-17T11:38:57,194 [INFO ] W-9008-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stdout
2022-04-17T11:38:57,197 [INFO ] W-9008-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stderr
2022-04-17T11:38:57,197 [INFO ] W-9008-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stderr
2022-04-17T11:38:57,291 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:38:57,292 [INFO ] nioEventLoopGroup-5-21 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-04-17T11:38:57,292 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:38:57,292 [INFO ] nioEventLoopGroup-5-21 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-04-17T11:38:57,293 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:38:57,293 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:38:57,293 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:38:57,294 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:38:57,294 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:38:57,294 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:38:57,297 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:38:57,295 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:38:57,297 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:38:57,298 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:38:57,298 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:38:57,298 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:38:57,299 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stderr
2022-04-17T11:38:57,298 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:38:57,299 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stderr
2022-04-17T11:38:57,300 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stdout
2022-04-17T11:38:57,299 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:38:57,300 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stdout
2022-04-17T11:38:57,301 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 144 seconds.
2022-04-17T11:38:57,300 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:38:57,301 [INFO ] W-9000-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stdout
2022-04-17T11:38:57,301 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 144 seconds.
2022-04-17T11:38:57,301 [INFO ] W-9000-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stdout
2022-04-17T11:38:57,304 [INFO ] W-9000-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stderr
2022-04-17T11:38:57,304 [INFO ] W-9000-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stderr
2022-04-17T11:38:57,334 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:38:57,334 [INFO ] nioEventLoopGroup-5-22 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-04-17T11:38:57,334 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:38:57,334 [INFO ] nioEventLoopGroup-5-22 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-04-17T11:38:57,336 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:38:57,335 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:38:57,336 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:38:57,336 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:38:57,336 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:38:57,337 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:38:57,336 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:38:57,340 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:38:57,337 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:38:57,340 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:38:57,341 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:38:57,341 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:38:57,341 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:38:57,342 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stderr
2022-04-17T11:38:57,342 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:38:57,342 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stderr
2022-04-17T11:38:57,343 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stdout
2022-04-17T11:38:57,342 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:38:57,343 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stdout
2022-04-17T11:38:57,344 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 144 seconds.
2022-04-17T11:38:57,343 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:38:57,344 [INFO ] W-9002-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stdout
2022-04-17T11:38:57,344 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 144 seconds.
2022-04-17T11:38:57,344 [INFO ] W-9002-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stdout
2022-04-17T11:38:57,348 [INFO ] W-9002-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stderr
2022-04-17T11:38:57,348 [INFO ] W-9002-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stderr
2022-04-17T11:38:57,377 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:38:57,377 [INFO ] nioEventLoopGroup-5-23 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-04-17T11:38:57,377 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:38:57,377 [INFO ] nioEventLoopGroup-5-23 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-04-17T11:38:57,378 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:38:57,378 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:38:57,378 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:38:57,379 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:38:57,378 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:38:57,379 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:38:57,381 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:38:57,379 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:38:57,381 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:38:57,382 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:38:57,382 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:38:57,382 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:38:57,383 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stderr
2022-04-17T11:38:57,383 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:38:57,383 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stderr
2022-04-17T11:38:57,384 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stdout
2022-04-17T11:38:57,383 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:38:57,384 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stdout
2022-04-17T11:38:57,385 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 144 seconds.
2022-04-17T11:38:57,384 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:38:57,385 [INFO ] W-9007-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stdout
2022-04-17T11:38:57,385 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 144 seconds.
2022-04-17T11:38:57,385 [INFO ] W-9007-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stdout
2022-04-17T11:38:57,386 [INFO ] W-9007-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stderr
2022-04-17T11:38:57,386 [INFO ] W-9007-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stderr
2022-04-17T11:38:57,398 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:38:57,399 [INFO ] nioEventLoopGroup-5-24 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-04-17T11:38:57,399 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:38:57,399 [INFO ] nioEventLoopGroup-5-24 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-04-17T11:38:57,400 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:38:57,399 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:38:57,400 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:38:57,400 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:38:57,400 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:38:57,400 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:38:57,404 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:38:57,401 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:38:57,404 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:38:57,404 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:38:57,404 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:38:57,404 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:38:57,406 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stderr
2022-04-17T11:38:57,405 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:38:57,406 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stderr
2022-04-17T11:38:57,406 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stdout
2022-04-17T11:38:57,406 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:38:57,406 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stdout
2022-04-17T11:38:57,407 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 144 seconds.
2022-04-17T11:38:57,407 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:38:57,407 [INFO ] W-9006-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stdout
2022-04-17T11:38:57,407 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 144 seconds.
2022-04-17T11:38:57,407 [INFO ] W-9006-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stdout
2022-04-17T11:38:57,408 [INFO ] W-9006-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stderr
2022-04-17T11:38:57,408 [INFO ] W-9006-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stderr
2022-04-17T11:41:15,058 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-04-17T11:41:15,058 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-04-17T11:41:15,849 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:41:15,850 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - [PID]12152
2022-04-17T11:41:15,851 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:41:15,851 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:41:15,851 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:41:15,851 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-04-17T11:41:15,851 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:41:15,851 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-04-17T11:41:15,853 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188475853
2022-04-17T11:41:15,853 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9004).
2022-04-17T11:41:15,853 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188475853
2022-04-17T11:41:15,854 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:41:16,006 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-04-17T11:41:16,006 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-04-17T11:41:16,086 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:41:16,087 [INFO ] nioEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-04-17T11:41:16,087 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:41:16,087 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:41:16,087 [INFO ] nioEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-04-17T11:41:16,088 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:41:16,087 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:41:16,088 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:41:16,088 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:41:16,088 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:41:16,088 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:41:16,088 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:41:16,090 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:41:16,088 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:41:16,090 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:41:16,090 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:41:16,090 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:41:16,090 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:41:16,091 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stderr
2022-04-17T11:41:16,090 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:41:16,091 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stderr
2022-04-17T11:41:16,091 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stdout
2022-04-17T11:41:16,091 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:41:16,091 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stdout
2022-04-17T11:41:16,092 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 233 seconds.
2022-04-17T11:41:16,091 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-04-17T11:41:16,092 [INFO ] W-9004-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stdout
2022-04-17T11:41:16,092 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 233 seconds.
2022-04-17T11:41:16,092 [INFO ] W-9004-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stdout
2022-04-17T11:41:16,093 [INFO ] W-9004-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stderr
2022-04-17T11:41:16,093 [INFO ] W-9004-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stderr
2022-04-17T11:41:16,787 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:41:16,787 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - [PID]7804
2022-04-17T11:41:16,788 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:41:16,788 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:41:16,788 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:41:16,788 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-04-17T11:41:16,788 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:41:16,788 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-04-17T11:41:16,790 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188476790
2022-04-17T11:41:16,790 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188476790
2022-04-17T11:41:16,790 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9003).
2022-04-17T11:41:16,790 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:41:17,018 [INFO ] nioEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-04-17T11:41:17,018 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:41:17,018 [INFO ] nioEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-04-17T11:41:17,018 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:41:17,019 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:41:17,019 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:41:17,019 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:41:17,020 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:41:17,019 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:41:17,020 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:41:17,020 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:41:17,022 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:41:17,020 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:41:17,022 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:41:17,022 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:41:17,022 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:41:17,022 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:41:17,022 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:41:17,023 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stderr
2022-04-17T11:41:17,023 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stderr
2022-04-17T11:41:17,023 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stdout
2022-04-17T11:41:17,023 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:41:17,023 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stdout
2022-04-17T11:41:17,023 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 233 seconds.
2022-04-17T11:41:17,023 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:41:17,024 [INFO ] W-9003-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stdout
2022-04-17T11:41:17,023 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 233 seconds.
2022-04-17T11:41:17,024 [INFO ] W-9003-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stdout
2022-04-17T11:41:17,024 [INFO ] W-9003-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stderr
2022-04-17T11:41:17,024 [INFO ] W-9003-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stderr
2022-04-17T11:41:19,374 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-04-17T11:41:19,374 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-04-17T11:41:20,153 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:41:20,154 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - [PID]17184
2022-04-17T11:41:20,154 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:41:20,154 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:41:20,154 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:41:20,155 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-04-17T11:41:20,155 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:41:20,155 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-04-17T11:41:20,156 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188480156
2022-04-17T11:41:20,156 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188480156
2022-04-17T11:41:20,156 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9005).
2022-04-17T11:41:20,157 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:41:20,386 [INFO ] nioEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-04-17T11:41:20,385 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:41:20,386 [INFO ] nioEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-04-17T11:41:20,386 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:41:20,386 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:41:20,386 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:41:20,387 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:41:20,387 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:41:20,387 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:41:20,387 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:41:20,389 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:41:20,389 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:41:20,389 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:41:20,389 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:41:20,389 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:41:20,390 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:41:20,390 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:41:20,390 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:41:20,390 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:41:20,390 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:41:20,391 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stderr
2022-04-17T11:41:20,391 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-04-17T11:41:20,391 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stderr
2022-04-17T11:41:20,391 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stdout
2022-04-17T11:41:20,391 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-04-17T11:41:20,391 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stdout
2022-04-17T11:41:20,391 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 233 seconds.
2022-04-17T11:41:20,391 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 790, in exec_module
2022-04-17T11:41:20,392 [INFO ] W-9005-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stdout
2022-04-17T11:41:20,391 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 233 seconds.
2022-04-17T11:41:20,392 [INFO ] W-9005-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stdout
2022-04-17T11:41:20,392 [INFO ] W-9005-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stderr
2022-04-17T11:41:20,392 [INFO ] W-9005-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stderr
2022-04-17T11:41:21,196 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9008]
2022-04-17T11:41:21,196 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9008]
2022-04-17T11:41:21,304 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-04-17T11:41:21,304 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-04-17T11:41:21,347 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-04-17T11:41:21,347 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-04-17T11:41:21,388 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-04-17T11:41:21,388 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-04-17T11:41:21,410 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-04-17T11:41:21,410 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-04-17T11:41:22,109 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:41:22,110 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - [PID]17712
2022-04-17T11:41:22,110 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:41:22,110 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:41:22,110 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:41:22,111 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9008
2022-04-17T11:41:22,110 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:41:22,111 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9008
2022-04-17T11:41:22,112 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188482112
2022-04-17T11:41:22,112 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9008).
2022-04-17T11:41:22,112 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188482112
2022-04-17T11:41:22,114 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:41:22,194 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:41:22,195 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - [PID]17512
2022-04-17T11:41:22,195 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:41:22,195 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:41:22,195 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:41:22,196 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-04-17T11:41:22,195 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:41:22,196 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-04-17T11:41:22,197 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188482197
2022-04-17T11:41:22,197 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188482197
2022-04-17T11:41:22,197 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9000).
2022-04-17T11:41:22,198 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:41:22,221 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:41:22,222 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - [PID]2280
2022-04-17T11:41:22,222 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:41:22,222 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:41:22,222 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:41:22,223 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-04-17T11:41:22,222 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:41:22,223 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-04-17T11:41:22,224 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188482224
2022-04-17T11:41:22,224 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188482224
2022-04-17T11:41:22,224 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9002).
2022-04-17T11:41:22,226 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:41:22,261 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:41:22,262 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - [PID]17824
2022-04-17T11:41:22,262 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:41:22,262 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:41:22,262 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:41:22,262 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-04-17T11:41:22,262 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:41:22,262 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-04-17T11:41:22,264 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188482264
2022-04-17T11:41:22,264 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188482264
2022-04-17T11:41:22,264 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9007).
2022-04-17T11:41:22,265 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:41:22,288 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:41:22,289 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - [PID]12428
2022-04-17T11:41:22,289 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:41:22,289 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:41:22,289 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:41:22,290 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-04-17T11:41:22,289 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:41:22,290 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-04-17T11:41:22,291 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188482291
2022-04-17T11:41:22,291 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188482291
2022-04-17T11:41:22,292 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9006).
2022-04-17T11:41:22,292 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:41:22,374 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:41:22,375 [INFO ] nioEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9008 Worker disconnected. WORKER_STARTED
2022-04-17T11:41:22,375 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:41:22,375 [INFO ] nioEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9008 Worker disconnected. WORKER_STARTED
2022-04-17T11:41:22,375 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:41:22,375 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:41:22,375 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:41:22,376 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:41:22,376 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:41:22,376 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:41:22,376 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:41:22,378 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:41:22,376 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:41:22,378 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:41:22,378 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:41:22,378 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:41:22,378 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:41:22,378 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:41:22,379 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stderr
2022-04-17T11:41:22,379 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stderr
2022-04-17T11:41:22,379 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stdout
2022-04-17T11:41:22,379 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:41:22,379 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stdout
2022-04-17T11:41:22,380 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9008 in 233 seconds.
2022-04-17T11:41:22,379 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:41:22,380 [INFO ] W-9008-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stdout
2022-04-17T11:41:22,380 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9008 in 233 seconds.
2022-04-17T11:41:22,380 [INFO ] W-9008-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stdout
2022-04-17T11:41:22,381 [INFO ] W-9008-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stderr
2022-04-17T11:41:22,381 [INFO ] W-9008-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stderr
2022-04-17T11:41:22,453 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:41:22,454 [INFO ] nioEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-04-17T11:41:22,454 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:41:22,454 [INFO ] nioEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-04-17T11:41:22,454 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:41:22,454 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:41:22,454 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:41:22,454 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:41:22,455 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:41:22,455 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:41:22,456 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:41:22,455 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:41:22,456 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:41:22,456 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:41:22,456 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:41:22,456 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:41:22,457 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:41:22,457 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stderr
2022-04-17T11:41:22,457 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stderr
2022-04-17T11:41:22,457 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stdout
2022-04-17T11:41:22,457 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:41:22,457 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stdout
2022-04-17T11:41:22,458 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 233 seconds.
2022-04-17T11:41:22,458 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:41:22,458 [INFO ] W-9000-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stdout
2022-04-17T11:41:22,458 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 233 seconds.
2022-04-17T11:41:22,458 [INFO ] W-9000-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stdout
2022-04-17T11:41:22,459 [INFO ] W-9000-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stderr
2022-04-17T11:41:22,459 [INFO ] W-9000-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stderr
2022-04-17T11:41:22,479 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:41:22,479 [INFO ] nioEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-04-17T11:41:22,479 [INFO ] nioEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-04-17T11:41:22,479 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:41:22,479 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:41:22,479 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:41:22,480 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:41:22,480 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:41:22,480 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:41:22,480 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:41:22,481 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:41:22,480 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:41:22,481 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:41:22,482 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:41:22,482 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:41:22,482 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:41:22,482 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stderr
2022-04-17T11:41:22,482 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:41:22,482 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stderr
2022-04-17T11:41:22,483 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stdout
2022-04-17T11:41:22,482 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:41:22,483 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stdout
2022-04-17T11:41:22,483 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 233 seconds.
2022-04-17T11:41:22,483 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:41:22,483 [INFO ] W-9002-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stdout
2022-04-17T11:41:22,483 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 233 seconds.
2022-04-17T11:41:22,483 [INFO ] W-9002-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stdout
2022-04-17T11:41:22,484 [INFO ] W-9002-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stderr
2022-04-17T11:41:22,484 [INFO ] W-9002-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stderr
2022-04-17T11:41:22,516 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:41:22,516 [INFO ] nioEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-04-17T11:41:22,516 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:41:22,516 [INFO ] nioEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-04-17T11:41:22,517 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:41:22,517 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:41:22,517 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:41:22,517 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:41:22,517 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:41:22,517 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:41:22,518 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:41:22,517 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:41:22,518 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:41:22,519 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:41:22,518 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:41:22,519 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:41:22,519 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:41:22,519 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stderr
2022-04-17T11:41:22,519 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stderr
2022-04-17T11:41:22,519 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stdout
2022-04-17T11:41:22,519 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:41:22,519 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stdout
2022-04-17T11:41:22,520 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 233 seconds.
2022-04-17T11:41:22,520 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:41:22,520 [INFO ] W-9007-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stdout
2022-04-17T11:41:22,520 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 233 seconds.
2022-04-17T11:41:22,520 [INFO ] W-9007-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stdout
2022-04-17T11:41:22,521 [INFO ] W-9007-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stderr
2022-04-17T11:41:22,521 [INFO ] W-9007-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stderr
2022-04-17T11:41:22,541 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:41:22,541 [INFO ] nioEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-04-17T11:41:22,541 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:41:22,541 [INFO ] nioEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-04-17T11:41:22,542 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:41:22,541 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:41:22,542 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:41:22,542 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:41:22,542 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:41:22,542 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:41:22,543 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:41:22,542 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:41:22,543 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:41:22,543 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:41:22,543 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:41:22,543 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:41:22,544 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stderr
2022-04-17T11:41:22,544 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:41:22,544 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stderr
2022-04-17T11:41:22,544 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stdout
2022-04-17T11:41:22,544 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:41:22,544 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stdout
2022-04-17T11:41:22,545 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 233 seconds.
2022-04-17T11:41:22,544 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:41:22,545 [INFO ] W-9006-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stdout
2022-04-17T11:41:22,545 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 233 seconds.
2022-04-17T11:41:22,545 [INFO ] W-9006-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stdout
2022-04-17T11:41:22,546 [INFO ] W-9006-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stderr
2022-04-17T11:41:22,546 [INFO ] W-9006-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stderr
2022-04-17T11:45:09,094 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-04-17T11:45:09,094 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-04-17T11:45:09,856 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:45:09,857 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - [PID]18732
2022-04-17T11:45:09,857 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:45:09,857 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:45:09,857 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:45:09,857 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:45:09,858 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-04-17T11:45:09,858 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-04-17T11:45:09,859 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188709859
2022-04-17T11:45:09,859 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9004).
2022-04-17T11:45:09,859 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188709859
2022-04-17T11:45:09,860 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:45:10,026 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-04-17T11:45:10,026 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-04-17T11:45:10,086 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:45:10,086 [INFO ] nioEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-04-17T11:45:10,086 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:45:10,086 [INFO ] nioEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-04-17T11:45:10,087 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:45:10,086 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:45:10,087 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:45:10,087 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:45:10,087 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:45:10,087 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:45:10,088 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:45:10,087 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:45:10,089 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:45:10,088 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:45:10,089 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:45:10,090 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:45:10,089 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:45:10,090 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:45:10,090 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stderr
2022-04-17T11:45:10,090 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:45:10,090 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stderr
2022-04-17T11:45:10,091 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stdout
2022-04-17T11:45:10,091 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:45:10,091 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stdout
2022-04-17T11:45:10,091 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 377 seconds.
2022-04-17T11:45:10,091 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-04-17T11:45:10,091 [INFO ] W-9004-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stdout
2022-04-17T11:45:10,091 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 377 seconds.
2022-04-17T11:45:10,091 [INFO ] W-9004-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stdout
2022-04-17T11:45:10,093 [INFO ] W-9004-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stderr
2022-04-17T11:45:10,093 [INFO ] W-9004-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stderr
2022-04-17T11:45:10,802 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:45:10,802 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - [PID]8144
2022-04-17T11:45:10,803 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:45:10,803 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:45:10,803 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:45:10,803 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-04-17T11:45:10,803 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:45:10,803 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-04-17T11:45:10,804 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188710804
2022-04-17T11:45:10,804 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188710804
2022-04-17T11:45:10,804 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9003).
2022-04-17T11:45:10,805 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:45:11,025 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:45:11,025 [INFO ] nioEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-04-17T11:45:11,025 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:45:11,025 [INFO ] nioEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-04-17T11:45:11,025 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:45:11,025 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:45:11,025 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:45:11,025 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:45:11,026 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:45:11,026 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:45:11,027 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:45:11,026 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:45:11,027 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:45:11,027 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:45:11,027 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:45:11,027 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:45:11,028 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stderr
2022-04-17T11:45:11,027 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:45:11,028 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stderr
2022-04-17T11:45:11,028 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stdout
2022-04-17T11:45:11,028 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:45:11,028 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stdout
2022-04-17T11:45:11,028 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 377 seconds.
2022-04-17T11:45:11,028 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:45:11,029 [INFO ] W-9003-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stdout
2022-04-17T11:45:11,028 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 377 seconds.
2022-04-17T11:45:11,029 [INFO ] W-9003-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stdout
2022-04-17T11:45:11,029 [INFO ] W-9003-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stderr
2022-04-17T11:45:11,029 [INFO ] W-9003-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stderr
2022-04-17T11:45:13,393 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-04-17T11:45:13,393 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-04-17T11:45:14,172 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:45:14,173 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - [PID]12156
2022-04-17T11:45:14,173 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:45:14,173 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:45:14,173 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:45:14,174 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-04-17T11:45:14,173 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:45:14,174 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-04-17T11:45:14,175 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188714175
2022-04-17T11:45:14,175 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188714175
2022-04-17T11:45:14,175 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9005).
2022-04-17T11:45:14,175 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:45:14,395 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:45:14,395 [INFO ] nioEventLoopGroup-5-11 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-04-17T11:45:14,395 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:45:14,395 [INFO ] nioEventLoopGroup-5-11 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-04-17T11:45:14,395 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:45:14,395 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:45:14,395 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:45:14,396 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:45:14,396 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:45:14,396 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:45:14,396 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:45:14,398 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:45:14,396 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:45:14,398 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:45:14,398 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:45:14,398 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:45:14,398 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:45:14,398 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stderr
2022-04-17T11:45:14,398 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:45:14,398 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stderr
2022-04-17T11:45:14,399 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stdout
2022-04-17T11:45:14,399 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:45:14,399 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stdout
2022-04-17T11:45:14,399 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 377 seconds.
2022-04-17T11:45:14,399 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:45:14,399 [INFO ] W-9005-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stdout
2022-04-17T11:45:14,399 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 377 seconds.
2022-04-17T11:45:14,399 [INFO ] W-9005-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stdout
2022-04-17T11:45:14,400 [INFO ] W-9005-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stderr
2022-04-17T11:45:14,400 [INFO ] W-9005-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stderr
2022-04-17T11:45:15,382 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9008]
2022-04-17T11:45:15,382 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9008]
2022-04-17T11:45:15,462 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-04-17T11:45:15,462 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-04-17T11:45:15,487 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-04-17T11:45:15,487 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-04-17T11:45:15,523 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-04-17T11:45:15,523 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-04-17T11:45:15,547 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-04-17T11:45:15,547 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-04-17T11:45:16,522 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:45:16,524 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - [PID]1784
2022-04-17T11:45:16,524 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:45:16,524 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:45:16,524 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:45:16,525 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9008
2022-04-17T11:45:16,525 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:45:16,525 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9008
2022-04-17T11:45:16,527 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188716527
2022-04-17T11:45:16,527 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9008).
2022-04-17T11:45:16,527 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188716527
2022-04-17T11:45:16,529 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:45:16,571 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:45:16,572 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - [PID]10724
2022-04-17T11:45:16,572 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:45:16,572 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:45:16,572 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:45:16,573 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-04-17T11:45:16,573 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:45:16,573 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-04-17T11:45:16,575 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188716575
2022-04-17T11:45:16,575 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9000).
2022-04-17T11:45:16,575 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188716575
2022-04-17T11:45:16,577 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:45:16,613 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:45:16,615 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - [PID]18516
2022-04-17T11:45:16,616 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:45:16,615 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:45:16,616 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:45:16,616 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-04-17T11:45:16,616 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:45:16,616 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-04-17T11:45:16,619 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188716619
2022-04-17T11:45:16,619 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188716619
2022-04-17T11:45:16,619 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9002).
2022-04-17T11:45:16,620 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:45:16,682 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:45:16,683 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - [PID]18948
2022-04-17T11:45:16,684 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:45:16,684 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:45:16,684 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:45:16,685 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-04-17T11:45:16,684 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:45:16,685 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-04-17T11:45:16,686 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188716686
2022-04-17T11:45:16,686 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188716686
2022-04-17T11:45:16,686 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9007).
2022-04-17T11:45:16,687 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:45:16,708 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:45:16,709 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - [PID]11276
2022-04-17T11:45:16,710 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:45:16,710 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:45:16,710 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:45:16,710 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-04-17T11:45:16,710 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:45:16,710 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-04-17T11:45:16,712 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188716712
2022-04-17T11:45:16,712 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650188716712
2022-04-17T11:45:16,713 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9006).
2022-04-17T11:45:16,714 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:45:16,930 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:45:16,930 [INFO ] nioEventLoopGroup-5-12 org.pytorch.serve.wlm.WorkerThread - 9008 Worker disconnected. WORKER_STARTED
2022-04-17T11:45:16,930 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:45:16,930 [INFO ] nioEventLoopGroup-5-12 org.pytorch.serve.wlm.WorkerThread - 9008 Worker disconnected. WORKER_STARTED
2022-04-17T11:45:16,931 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:45:16,931 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:45:16,931 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:45:16,932 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:45:16,932 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:45:16,932 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:45:16,935 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:45:16,932 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:45:16,935 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:45:16,935 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:45:16,935 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:45:16,935 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:45:16,936 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stderr
2022-04-17T11:45:16,936 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:45:16,936 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stderr
2022-04-17T11:45:16,937 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stdout
2022-04-17T11:45:16,936 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:45:16,937 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stdout
2022-04-17T11:45:16,937 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9008 in 377 seconds.
2022-04-17T11:45:16,937 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:45:16,938 [INFO ] W-9008-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stdout
2022-04-17T11:45:16,937 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9008 in 377 seconds.
2022-04-17T11:45:16,938 [INFO ] W-9008-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stdout
2022-04-17T11:45:16,939 [INFO ] W-9008-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stderr
2022-04-17T11:45:16,939 [INFO ] W-9008-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stderr
2022-04-17T11:45:16,982 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:45:16,983 [INFO ] nioEventLoopGroup-5-13 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-04-17T11:45:16,983 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:45:16,983 [INFO ] nioEventLoopGroup-5-13 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-04-17T11:45:16,984 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:45:16,983 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:45:16,984 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:45:16,984 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:45:16,984 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:45:16,984 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:45:16,987 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:45:16,985 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:45:16,987 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:45:16,987 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:45:16,987 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:45:16,987 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:45:16,988 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stderr
2022-04-17T11:45:16,988 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:45:16,988 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stderr
2022-04-17T11:45:16,989 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stdout
2022-04-17T11:45:16,988 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:45:16,989 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stdout
2022-04-17T11:45:16,989 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 377 seconds.
2022-04-17T11:45:16,989 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:45:16,990 [INFO ] W-9000-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stdout
2022-04-17T11:45:16,989 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 377 seconds.
2022-04-17T11:45:16,990 [INFO ] W-9000-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stdout
2022-04-17T11:45:16,991 [INFO ] W-9000-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stderr
2022-04-17T11:45:16,991 [INFO ] W-9000-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stderr
2022-04-17T11:45:17,020 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:45:17,021 [INFO ] nioEventLoopGroup-5-14 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-04-17T11:45:17,021 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:45:17,021 [INFO ] nioEventLoopGroup-5-14 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-04-17T11:45:17,022 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:45:17,021 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:45:17,022 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:45:17,022 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:45:17,022 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:45:17,022 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:45:17,024 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:45:17,022 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:45:17,024 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:45:17,025 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:45:17,024 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:45:17,025 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:45:17,025 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stderr
2022-04-17T11:45:17,025 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:45:17,025 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stderr
2022-04-17T11:45:17,026 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stdout
2022-04-17T11:45:17,026 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:45:17,026 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stdout
2022-04-17T11:45:17,026 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 377 seconds.
2022-04-17T11:45:17,026 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:45:17,027 [INFO ] W-9002-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stdout
2022-04-17T11:45:17,026 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 377 seconds.
2022-04-17T11:45:17,027 [INFO ] W-9002-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stdout
2022-04-17T11:45:17,028 [INFO ] W-9002-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stderr
2022-04-17T11:45:17,028 [INFO ] W-9002-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stderr
2022-04-17T11:45:17,075 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:45:17,076 [INFO ] nioEventLoopGroup-5-15 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-04-17T11:45:17,075 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:45:17,076 [INFO ] nioEventLoopGroup-5-15 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-04-17T11:45:17,076 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:45:17,076 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:45:17,076 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:45:17,076 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:45:17,077 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:45:17,077 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:45:17,077 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:45:17,079 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:45:17,077 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:45:17,079 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:45:17,079 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:45:17,079 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:45:17,079 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:45:17,080 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:45:17,080 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stderr
2022-04-17T11:45:17,080 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stderr
2022-04-17T11:45:17,081 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stdout
2022-04-17T11:45:17,080 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:45:17,081 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stdout
2022-04-17T11:45:17,081 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 377 seconds.
2022-04-17T11:45:17,081 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:45:17,082 [INFO ] W-9007-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stdout
2022-04-17T11:45:17,081 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 377 seconds.
2022-04-17T11:45:17,082 [INFO ] W-9007-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stdout
2022-04-17T11:45:17,083 [INFO ] W-9007-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stderr
2022-04-17T11:45:17,083 [INFO ] W-9007-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stderr
2022-04-17T11:45:17,104 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:45:17,104 [INFO ] nioEventLoopGroup-5-16 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-04-17T11:45:17,104 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:45:17,104 [INFO ] nioEventLoopGroup-5-16 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-04-17T11:45:17,105 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:45:17,105 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:45:17,105 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:45:17,105 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:45:17,105 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:45:17,105 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:45:17,107 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:45:17,106 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:45:17,107 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:45:17,108 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:45:17,108 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:45:17,108 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:45:17,109 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stderr
2022-04-17T11:45:17,108 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:45:17,109 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stderr
2022-04-17T11:45:17,109 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stdout
2022-04-17T11:45:17,109 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:45:17,109 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stdout
2022-04-17T11:45:17,109 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 377 seconds.
2022-04-17T11:45:17,109 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:45:17,110 [INFO ] W-9006-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stdout
2022-04-17T11:45:17,109 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 377 seconds.
2022-04-17T11:45:17,110 [INFO ] W-9006-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stdout
2022-04-17T11:45:17,111 [INFO ] W-9006-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stderr
2022-04-17T11:45:17,111 [INFO ] W-9006-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stderr
2022-04-17T11:49:30,728 [INFO ] pool-2-thread-29 ACCESS_LOG - /[0:0:0:0:0:0:0:1]:55762 "GET /ping HTTP/1.1" 200 1
2022-04-17T11:49:30,728 [INFO ] pool-2-thread-29 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:null
2022-04-17T11:51:27,094 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-04-17T11:51:27,094 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-04-17T11:51:27,897 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:51:27,898 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - [PID]14896
2022-04-17T11:51:27,898 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:51:27,898 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:51:27,898 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:51:27,899 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-04-17T11:51:27,898 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:51:27,899 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-04-17T11:51:27,900 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650189087900
2022-04-17T11:51:27,900 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650189087900
2022-04-17T11:51:27,900 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9004).
2022-04-17T11:51:27,901 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:51:28,033 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-04-17T11:51:28,033 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-04-17T11:51:28,155 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:51:28,155 [INFO ] nioEventLoopGroup-5-17 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-04-17T11:51:28,155 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:51:28,155 [INFO ] nioEventLoopGroup-5-17 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-04-17T11:51:28,156 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:51:28,156 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:51:28,156 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:51:28,156 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:51:28,156 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:51:28,157 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:51:28,156 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:51:28,158 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:51:28,157 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:51:28,158 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:51:28,158 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:51:28,158 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:51:28,158 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:51:28,159 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:51:28,159 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stderr
2022-04-17T11:51:28,159 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stderr
2022-04-17T11:51:28,159 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stdout
2022-04-17T11:51:28,159 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:51:28,159 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stdout
2022-04-17T11:51:28,160 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 610 seconds.
2022-04-17T11:51:28,160 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:51:28,160 [INFO ] W-9004-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stdout
2022-04-17T11:51:28,160 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 610 seconds.
2022-04-17T11:51:28,160 [INFO ] W-9004-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stdout
2022-04-17T11:51:28,161 [INFO ] W-9004-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stderr
2022-04-17T11:51:28,161 [INFO ] W-9004-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stderr
2022-04-17T11:51:28,826 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:51:28,826 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - [PID]16348
2022-04-17T11:51:28,827 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:51:28,827 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:51:28,827 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:51:28,827 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:51:28,828 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-04-17T11:51:28,828 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-04-17T11:51:28,829 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650189088829
2022-04-17T11:51:28,829 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650189088829
2022-04-17T11:51:28,829 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9003).
2022-04-17T11:51:28,829 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:51:29,057 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:51:29,058 [INFO ] nioEventLoopGroup-5-18 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-04-17T11:51:29,058 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:51:29,058 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:51:29,058 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:51:29,058 [INFO ] nioEventLoopGroup-5-18 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-04-17T11:51:29,059 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:51:29,059 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:51:29,059 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:51:29,059 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:51:29,059 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:51:29,060 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:51:29,059 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:51:29,061 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:51:29,060 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:51:29,061 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:51:29,061 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:51:29,061 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:51:29,061 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:51:29,062 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stderr
2022-04-17T11:51:29,062 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:51:29,062 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stderr
2022-04-17T11:51:29,062 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stdout
2022-04-17T11:51:29,062 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-04-17T11:51:29,062 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stdout
2022-04-17T11:51:29,063 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 610 seconds.
2022-04-17T11:51:29,062 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
2022-04-17T11:51:29,063 [INFO ] W-9003-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stdout
2022-04-17T11:51:29,063 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 610 seconds.
2022-04-17T11:51:29,063 [INFO ] W-9003-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stdout
2022-04-17T11:51:29,064 [INFO ] W-9003-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stderr
2022-04-17T11:51:29,064 [INFO ] W-9003-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stderr
2022-04-17T11:51:31,401 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-04-17T11:51:31,401 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-04-17T11:51:32,184 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:51:32,185 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - [PID]19424
2022-04-17T11:51:32,186 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:51:32,185 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:51:32,186 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:51:32,186 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:51:32,186 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-04-17T11:51:32,186 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-04-17T11:51:32,187 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650189092187
2022-04-17T11:51:32,187 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650189092187
2022-04-17T11:51:32,188 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9005).
2022-04-17T11:51:32,188 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:51:32,420 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:51:32,420 [INFO ] nioEventLoopGroup-5-19 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-04-17T11:51:32,420 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:51:32,420 [INFO ] nioEventLoopGroup-5-19 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-04-17T11:51:32,421 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:51:32,420 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:51:32,421 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:51:32,421 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:51:32,421 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:51:32,421 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:51:32,423 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:51:32,422 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:51:32,423 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:51:32,424 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:51:32,423 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:51:32,424 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:51:32,424 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stderr
2022-04-17T11:51:32,424 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:51:32,424 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stderr
2022-04-17T11:51:32,425 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stdout
2022-04-17T11:51:32,424 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:51:32,425 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stdout
2022-04-17T11:51:32,425 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 610 seconds.
2022-04-17T11:51:32,425 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:51:32,425 [INFO ] W-9005-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stdout
2022-04-17T11:51:32,425 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 610 seconds.
2022-04-17T11:51:32,425 [INFO ] W-9005-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stdout
2022-04-17T11:51:32,426 [INFO ] W-9005-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stderr
2022-04-17T11:51:32,426 [INFO ] W-9005-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stderr
2022-04-17T11:51:33,940 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9008]
2022-04-17T11:51:33,940 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9008]
2022-04-17T11:51:33,990 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-04-17T11:51:33,990 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-04-17T11:51:34,028 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-04-17T11:51:34,028 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-04-17T11:51:34,083 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-04-17T11:51:34,083 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-04-17T11:51:34,112 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-04-17T11:51:34,112 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-04-17T11:51:34,876 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:51:34,876 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - [PID]18860
2022-04-17T11:51:34,877 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:51:34,877 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:51:34,877 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:51:34,877 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9008
2022-04-17T11:51:34,877 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:51:34,877 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9008
2022-04-17T11:51:34,879 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650189094879
2022-04-17T11:51:34,879 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650189094879
2022-04-17T11:51:34,879 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9008).
2022-04-17T11:51:34,880 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:51:34,897 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:51:34,898 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - [PID]18776
2022-04-17T11:51:34,899 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:51:34,899 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:51:34,899 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:51:34,899 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-04-17T11:51:34,899 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:51:34,899 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-04-17T11:51:34,901 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650189094901
2022-04-17T11:51:34,901 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650189094901
2022-04-17T11:51:34,901 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9000).
2022-04-17T11:51:34,902 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:51:34,942 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:51:34,943 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - [PID]14656
2022-04-17T11:51:34,944 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:51:34,944 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:51:34,944 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:51:34,944 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-04-17T11:51:34,944 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:51:34,944 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-04-17T11:51:34,946 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650189094946
2022-04-17T11:51:34,946 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650189094946
2022-04-17T11:51:34,946 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9002).
2022-04-17T11:51:34,947 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:51:34,989 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:51:34,989 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - [PID]19108
2022-04-17T11:51:34,990 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:51:34,990 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:51:34,990 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:51:34,991 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-04-17T11:51:34,990 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:51:34,991 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-04-17T11:51:34,993 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650189094992
2022-04-17T11:51:34,993 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650189094992
2022-04-17T11:51:34,993 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9007).
2022-04-17T11:51:34,994 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:51:35,031 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T11:51:35,033 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - [PID]18420
2022-04-17T11:51:35,034 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:51:35,034 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T11:51:35,034 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T11:51:35,034 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-04-17T11:51:35,034 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T11:51:35,034 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-04-17T11:51:35,036 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650189095036
2022-04-17T11:51:35,036 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650189095036
2022-04-17T11:51:35,036 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9006).
2022-04-17T11:51:35,038 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T11:51:35,147 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:51:35,148 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:51:35,148 [INFO ] nioEventLoopGroup-5-20 org.pytorch.serve.wlm.WorkerThread - 9008 Worker disconnected. WORKER_STARTED
2022-04-17T11:51:35,148 [INFO ] nioEventLoopGroup-5-20 org.pytorch.serve.wlm.WorkerThread - 9008 Worker disconnected. WORKER_STARTED
2022-04-17T11:51:35,148 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:51:35,148 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:51:35,148 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:51:35,149 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:51:35,149 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:51:35,149 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:51:35,149 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:51:35,151 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:51:35,151 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:51:35,151 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:51:35,151 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:51:35,151 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:51:35,151 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:51:35,152 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stderr
2022-04-17T11:51:35,152 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stderr
2022-04-17T11:51:35,152 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stdout
2022-04-17T11:51:35,152 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:51:35,152 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stdout
2022-04-17T11:51:35,153 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9008 in 610 seconds.
2022-04-17T11:51:35,152 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:51:35,153 [INFO ] W-9008-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stdout
2022-04-17T11:51:35,153 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9008 in 610 seconds.
2022-04-17T11:51:35,153 [INFO ] W-9008-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stdout
2022-04-17T11:51:35,154 [INFO ] W-9008-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stderr
2022-04-17T11:51:35,154 [INFO ] W-9008-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stderr
2022-04-17T11:51:35,166 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:51:35,167 [INFO ] nioEventLoopGroup-5-21 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-04-17T11:51:35,167 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:51:35,167 [INFO ] nioEventLoopGroup-5-21 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-04-17T11:51:35,167 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:51:35,167 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:51:35,167 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:51:35,168 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:51:35,168 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:51:35,168 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:51:35,169 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:51:35,168 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:51:35,169 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:51:35,170 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:51:35,169 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:51:35,170 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:51:35,170 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:51:35,170 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stderr
2022-04-17T11:51:35,170 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:51:35,170 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stderr
2022-04-17T11:51:35,171 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stdout
2022-04-17T11:51:35,171 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:51:35,171 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stdout
2022-04-17T11:51:35,172 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 610 seconds.
2022-04-17T11:51:35,171 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:51:35,172 [INFO ] W-9000-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stdout
2022-04-17T11:51:35,172 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 610 seconds.
2022-04-17T11:51:35,172 [INFO ] W-9000-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stdout
2022-04-17T11:51:35,173 [INFO ] W-9000-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stderr
2022-04-17T11:51:35,173 [INFO ] W-9000-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stderr
2022-04-17T11:51:35,209 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:51:35,209 [INFO ] nioEventLoopGroup-5-22 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-04-17T11:51:35,209 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:51:35,209 [INFO ] nioEventLoopGroup-5-22 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-04-17T11:51:35,210 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:51:35,210 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:51:35,210 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:51:35,210 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:51:35,210 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:51:35,210 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:51:35,210 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:51:35,212 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:51:35,211 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:51:35,212 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:51:35,212 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:51:35,212 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:51:35,212 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:51:35,213 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stderr
2022-04-17T11:51:35,212 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:51:35,213 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stderr
2022-04-17T11:51:35,213 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stdout
2022-04-17T11:51:35,213 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:51:35,213 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stdout
2022-04-17T11:51:35,213 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 610 seconds.
2022-04-17T11:51:35,213 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:51:35,214 [INFO ] W-9002-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stdout
2022-04-17T11:51:35,213 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 610 seconds.
2022-04-17T11:51:35,214 [INFO ] W-9002-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stdout
2022-04-17T11:51:35,215 [INFO ] W-9002-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stderr
2022-04-17T11:51:35,215 [INFO ] W-9002-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stderr
2022-04-17T11:51:35,253 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:51:35,253 [INFO ] nioEventLoopGroup-5-23 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-04-17T11:51:35,253 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:51:35,253 [INFO ] nioEventLoopGroup-5-23 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-04-17T11:51:35,254 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:51:35,253 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:51:35,254 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:51:35,254 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:51:35,254 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:51:35,254 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:51:35,254 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:51:35,256 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:51:35,255 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:51:35,256 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:51:35,256 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:51:35,256 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:51:35,256 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:51:35,257 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stderr
2022-04-17T11:51:35,257 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:51:35,257 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stderr
2022-04-17T11:51:35,258 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stdout
2022-04-17T11:51:35,257 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T11:51:35,258 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stdout
2022-04-17T11:51:35,258 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 610 seconds.
2022-04-17T11:51:35,258 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T11:51:35,258 [INFO ] W-9007-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stdout
2022-04-17T11:51:35,258 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 610 seconds.
2022-04-17T11:51:35,258 [INFO ] W-9007-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stdout
2022-04-17T11:51:35,259 [INFO ] W-9007-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stderr
2022-04-17T11:51:35,259 [INFO ] W-9007-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stderr
2022-04-17T11:51:35,295 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T11:51:35,295 [INFO ] nioEventLoopGroup-5-24 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-04-17T11:51:35,295 [INFO ] nioEventLoopGroup-5-24 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-04-17T11:51:35,295 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:51:35,295 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T11:51:35,296 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T11:51:35,295 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T11:51:35,296 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:51:35,296 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T11:51:35,296 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T11:51:35,298 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:51:35,296 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T11:51:35,298 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T11:51:35,298 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:51:35,298 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T11:51:35,298 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T11:51:35,299 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stderr
2022-04-17T11:51:35,298 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T11:51:35,299 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stderr
2022-04-17T11:51:35,299 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stdout
2022-04-17T11:51:35,299 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stdout
2022-04-17T11:51:35,299 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 610 seconds.
2022-04-17T11:51:35,299 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T11:51:35,299 [INFO ] W-9006-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stdout
2022-04-17T11:51:35,299 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 610 seconds.
2022-04-17T11:51:35,299 [INFO ] W-9006-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stdout
2022-04-17T11:51:35,300 [INFO ] W-9006-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stderr
2022-04-17T11:51:35,300 [INFO ] W-9006-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stderr
2022-04-17T12:01:38,161 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-04-17T12:01:38,161 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-04-17T12:01:38,964 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T12:01:38,965 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - [PID]15212
2022-04-17T12:01:38,965 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T12:01:38,965 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T12:01:38,965 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T12:01:38,966 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-04-17T12:01:38,966 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T12:01:38,966 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-04-17T12:01:38,968 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650189698968
2022-04-17T12:01:38,968 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9004).
2022-04-17T12:01:38,968 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650189698968
2022-04-17T12:01:38,969 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T12:01:39,064 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-04-17T12:01:39,064 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-04-17T12:01:39,220 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T12:01:39,220 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T12:01:39,220 [INFO ] nioEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-04-17T12:01:39,220 [INFO ] nioEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-04-17T12:01:39,221 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T12:01:39,220 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T12:01:39,221 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T12:01:39,221 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T12:01:39,221 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T12:01:39,221 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T12:01:39,223 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T12:01:39,221 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T12:01:39,223 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T12:01:39,224 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T12:01:39,223 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T12:01:39,224 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T12:01:39,224 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T12:01:39,224 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stderr
2022-04-17T12:01:39,224 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stderr
2022-04-17T12:01:39,225 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stdout
2022-04-17T12:01:39,224 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T12:01:39,225 [WARN ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-foodnet_1.0-stdout
2022-04-17T12:01:39,225 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 987 seconds.
2022-04-17T12:01:39,225 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T12:01:39,226 [INFO ] W-9004-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stdout
2022-04-17T12:01:39,225 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 987 seconds.
2022-04-17T12:01:39,226 [INFO ] W-9004-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stdout
2022-04-17T12:01:39,227 [INFO ] W-9004-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stderr
2022-04-17T12:01:39,227 [INFO ] W-9004-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-foodnet_1.0-stderr
2022-04-17T12:01:40,082 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T12:01:40,082 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - [PID]14900
2022-04-17T12:01:40,083 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T12:01:40,083 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T12:01:40,083 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T12:01:40,084 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-04-17T12:01:40,083 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T12:01:40,084 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-04-17T12:01:40,086 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650189700086
2022-04-17T12:01:40,086 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650189700086
2022-04-17T12:01:40,086 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9003).
2022-04-17T12:01:40,087 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T12:01:40,429 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T12:01:40,429 [INFO ] nioEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-04-17T12:01:40,429 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T12:01:40,429 [INFO ] nioEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-04-17T12:01:40,430 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T12:01:40,429 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T12:01:40,430 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T12:01:40,430 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T12:01:40,430 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T12:01:40,430 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T12:01:40,432 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T12:01:40,431 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T12:01:40,432 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T12:01:40,433 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T12:01:40,433 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T12:01:40,433 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T12:01:40,434 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T12:01:40,434 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stderr
2022-04-17T12:01:40,434 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stderr
2022-04-17T12:01:40,435 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stdout
2022-04-17T12:01:40,434 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T12:01:40,435 [WARN ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-foodnet_1.0-stdout
2022-04-17T12:01:40,435 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 987 seconds.
2022-04-17T12:01:40,435 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T12:01:40,436 [INFO ] W-9003-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stdout
2022-04-17T12:01:40,435 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 987 seconds.
2022-04-17T12:01:40,436 [INFO ] W-9003-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stdout
2022-04-17T12:01:40,438 [INFO ] W-9003-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stderr
2022-04-17T12:01:40,438 [INFO ] W-9003-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-foodnet_1.0-stderr
2022-04-17T12:01:42,428 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-04-17T12:01:42,428 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-04-17T12:01:43,471 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T12:01:43,472 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - [PID]19048
2022-04-17T12:01:43,473 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T12:01:43,473 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T12:01:43,473 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T12:01:43,473 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-04-17T12:01:43,473 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T12:01:43,473 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-04-17T12:01:43,475 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650189703475
2022-04-17T12:01:43,475 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9005).
2022-04-17T12:01:43,475 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650189703475
2022-04-17T12:01:43,477 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T12:01:43,811 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T12:01:43,812 [INFO ] nioEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-04-17T12:01:43,812 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T12:01:43,812 [INFO ] nioEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-04-17T12:01:43,812 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T12:01:43,812 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T12:01:43,812 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T12:01:43,813 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T12:01:43,813 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T12:01:43,813 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T12:01:43,815 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T12:01:43,813 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T12:01:43,815 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T12:01:43,816 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T12:01:43,815 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T12:01:43,816 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T12:01:43,816 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T12:01:43,816 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stderr
2022-04-17T12:01:43,816 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stderr
2022-04-17T12:01:43,817 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stdout
2022-04-17T12:01:43,816 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T12:01:43,817 [WARN ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-foodnet_1.0-stdout
2022-04-17T12:01:43,817 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 987 seconds.
2022-04-17T12:01:43,817 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T12:01:43,818 [INFO ] W-9005-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stdout
2022-04-17T12:01:43,817 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 987 seconds.
2022-04-17T12:01:43,818 [INFO ] W-9005-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stdout
2022-04-17T12:01:43,820 [INFO ] W-9005-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stderr
2022-04-17T12:01:43,820 [INFO ] W-9005-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-foodnet_1.0-stderr
2022-04-17T12:01:45,155 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9008]
2022-04-17T12:01:45,155 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9008]
2022-04-17T12:01:45,175 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-04-17T12:01:45,175 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-04-17T12:01:45,217 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-04-17T12:01:45,217 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-04-17T12:01:45,262 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-04-17T12:01:45,262 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-04-17T12:01:45,303 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-04-17T12:01:45,303 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-04-17T12:01:46,945 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T12:01:46,947 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - [PID]16248
2022-04-17T12:01:46,948 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T12:01:46,947 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T12:01:46,948 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T12:01:46,949 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9008
2022-04-17T12:01:46,948 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T12:01:46,949 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9008
2022-04-17T12:01:46,951 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650189706951
2022-04-17T12:01:46,951 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650189706951
2022-04-17T12:01:46,951 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9008).
2022-04-17T12:01:46,953 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T12:01:46,978 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T12:01:46,979 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - [PID]17004
2022-04-17T12:01:46,980 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T12:01:46,980 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T12:01:46,980 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T12:01:46,981 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-04-17T12:01:46,980 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T12:01:46,981 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-04-17T12:01:46,983 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650189706983
2022-04-17T12:01:46,983 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9000).
2022-04-17T12:01:46,983 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650189706983
2022-04-17T12:01:46,986 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T12:01:47,046 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T12:01:47,048 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - [PID]13948
2022-04-17T12:01:47,049 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T12:01:47,048 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T12:01:47,049 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T12:01:47,049 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T12:01:47,050 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-04-17T12:01:47,050 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-04-17T12:01:47,053 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650189707053
2022-04-17T12:01:47,053 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9002).
2022-04-17T12:01:47,053 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650189707053
2022-04-17T12:01:47,055 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T12:01:47,110 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T12:01:47,112 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - [PID]19220
2022-04-17T12:01:47,113 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T12:01:47,113 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T12:01:47,113 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T12:01:47,114 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-04-17T12:01:47,114 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T12:01:47,114 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-04-17T12:01:47,117 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650189707117
2022-04-17T12:01:47,117 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650189707117
2022-04-17T12:01:47,117 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9007).
2022-04-17T12:01:47,119 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T12:01:47,187 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T12:01:47,189 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - [PID]6580
2022-04-17T12:01:47,190 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T12:01:47,190 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T12:01:47,190 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-04-17T12:01:47,191 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-04-17T12:01:47,190 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T12:01:47,191 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-04-17T12:01:47,194 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650189707194
2022-04-17T12:01:47,194 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650189707194
2022-04-17T12:01:47,194 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9006).
2022-04-17T12:01:47,195 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T12:01:47,508 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T12:01:47,509 [INFO ] nioEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9008 Worker disconnected. WORKER_STARTED
2022-04-17T12:01:47,509 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T12:01:47,509 [INFO ] nioEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9008 Worker disconnected. WORKER_STARTED
2022-04-17T12:01:47,510 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T12:01:47,509 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T12:01:47,510 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T12:01:47,511 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T12:01:47,510 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T12:01:47,511 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T12:01:47,514 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T12:01:47,511 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T12:01:47,514 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T12:01:47,514 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T12:01:47,514 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T12:01:47,514 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T12:01:47,516 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stderr
2022-04-17T12:01:47,515 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T12:01:47,516 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stderr
2022-04-17T12:01:47,516 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stdout
2022-04-17T12:01:47,516 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T12:01:47,516 [WARN ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9008-foodnet_1.0-stdout
2022-04-17T12:01:47,517 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9008 in 987 seconds.
2022-04-17T12:01:47,517 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T12:01:47,517 [INFO ] W-9008-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stdout
2022-04-17T12:01:47,517 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9008 in 987 seconds.
2022-04-17T12:01:47,517 [INFO ] W-9008-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stdout
2022-04-17T12:01:47,522 [INFO ] W-9008-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stderr
2022-04-17T12:01:47,522 [INFO ] W-9008-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9008-foodnet_1.0-stderr
2022-04-17T12:01:47,549 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T12:01:47,550 [INFO ] nioEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-04-17T12:01:47,549 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T12:01:47,550 [INFO ] nioEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-04-17T12:01:47,550 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T12:01:47,550 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T12:01:47,550 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T12:01:47,551 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T12:01:47,552 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T12:01:47,552 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T12:01:47,552 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T12:01:47,555 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T12:01:47,553 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T12:01:47,555 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T12:01:47,556 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T12:01:47,555 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T12:01:47,556 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T12:01:47,557 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stderr
2022-04-17T12:01:47,556 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T12:01:47,557 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stderr
2022-04-17T12:01:47,558 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stdout
2022-04-17T12:01:47,557 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T12:01:47,558 [WARN ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-foodnet_1.0-stdout
2022-04-17T12:01:47,559 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 987 seconds.
2022-04-17T12:01:47,558 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T12:01:47,559 [INFO ] W-9000-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stdout
2022-04-17T12:01:47,559 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 987 seconds.
2022-04-17T12:01:47,559 [INFO ] W-9000-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stdout
2022-04-17T12:01:47,562 [INFO ] W-9000-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stderr
2022-04-17T12:01:47,562 [INFO ] W-9000-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-foodnet_1.0-stderr
2022-04-17T12:01:47,627 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T12:01:47,628 [INFO ] nioEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-04-17T12:01:47,628 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T12:01:47,628 [INFO ] nioEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-04-17T12:01:47,629 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T12:01:47,628 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T12:01:47,629 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T12:01:47,629 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T12:01:47,630 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T12:01:47,630 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T12:01:47,631 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T12:01:47,630 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T12:01:47,631 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T12:01:47,635 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T12:01:47,635 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T12:01:47,635 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T12:01:47,635 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T12:01:47,635 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T12:01:47,636 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stderr
2022-04-17T12:01:47,636 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T12:01:47,636 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stderr
2022-04-17T12:01:47,637 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stdout
2022-04-17T12:01:47,637 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
2022-04-17T12:01:47,637 [WARN ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-foodnet_1.0-stdout
2022-04-17T12:01:47,638 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 987 seconds.
2022-04-17T12:01:47,637 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
2022-04-17T12:01:47,638 [INFO ] W-9002-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stdout
2022-04-17T12:01:47,638 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 987 seconds.
2022-04-17T12:01:47,638 [INFO ] W-9002-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stdout
2022-04-17T12:01:47,641 [INFO ] W-9002-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stderr
2022-04-17T12:01:47,641 [INFO ] W-9002-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-foodnet_1.0-stderr
2022-04-17T12:01:47,683 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T12:01:47,684 [INFO ] nioEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-04-17T12:01:47,684 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T12:01:47,684 [INFO ] nioEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-04-17T12:01:47,685 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T12:01:47,684 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T12:01:47,685 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T12:01:47,685 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T12:01:47,685 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T12:01:47,685 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T12:01:47,688 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T12:01:47,686 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T12:01:47,688 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T12:01:47,689 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T12:01:47,688 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T12:01:47,689 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T12:01:47,690 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stderr
2022-04-17T12:01:47,689 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T12:01:47,690 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stderr
2022-04-17T12:01:47,690 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stdout
2022-04-17T12:01:47,690 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T12:01:47,690 [WARN ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-foodnet_1.0-stdout
2022-04-17T12:01:47,691 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 987 seconds.
2022-04-17T12:01:47,691 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
2022-04-17T12:01:47,692 [INFO ] W-9007-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stdout
2022-04-17T12:01:47,691 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 987 seconds.
2022-04-17T12:01:47,692 [INFO ] W-9007-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stdout
2022-04-17T12:01:47,694 [INFO ] W-9007-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stderr
2022-04-17T12:01:47,694 [INFO ] W-9007-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-foodnet_1.0-stderr
2022-04-17T12:01:47,736 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T12:01:47,737 [INFO ] nioEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-04-17T12:01:47,737 [INFO ] nioEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-04-17T12:01:47,737 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T12:01:47,737 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T12:01:47,737 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-04-17T12:01:47,738 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T12:01:47,737 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 83, in load
2022-04-17T12:01:47,738 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T12:01:47,740 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T12:01:47,738 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-04-17T12:01:47,740 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: foodnet, error: Worker died.
2022-04-17T12:01:47,741 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T12:01:47,741 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\site-packages\ts\model_loader.py", line 123, in _load_handler_file
2022-04-17T12:01:47,741 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-04-17T12:01:47,742 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stderr
2022-04-17T12:01:47,741 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-04-17T12:01:47,742 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stderr
2022-04-17T12:01:47,742 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stdout
2022-04-17T12:01:47,742 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\lib\importlib\__init__.py", line 127, in import_module
2022-04-17T12:01:47,742 [WARN ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-foodnet_1.0-stdout
2022-04-17T12:01:47,743 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 987 seconds.
2022-04-17T12:01:47,743 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-04-17T12:01:47,744 [INFO ] W-9006-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stdout
2022-04-17T12:01:47,743 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 987 seconds.
2022-04-17T12:01:47,744 [INFO ] W-9006-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stdout
2022-04-17T12:01:47,745 [INFO ] W-9006-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stderr
2022-04-17T12:01:47,745 [INFO ] W-9006-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-foodnet_1.0-stderr
2022-04-17T12:12:56,856 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-04-17T12:12:56,856 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-04-17T12:12:57,068 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.5.3
TS Home: C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages
Current directory: C:\Users\Ameno\Desktop\luna\ai
Temp directory: C:\Users\Ameno\AppData\Local\Temp
Number of GPUs: 0
Number of CPUs: 12
Max heap size: 4048 M
Python executable: C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe
Config file: deployment/config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: C:\Users\Ameno\Desktop\luna\ai\deployment\model-store
Initial Models: foodnet=foodnet_resnet18.mar
Log dir: C:\Users\Ameno\Desktop\luna\ai\logs
Metrics dir: C:\Users\Ameno\Desktop\luna\ai\logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 12
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: C:\Users\Ameno\Desktop\luna\ai\deployment\model-store
Model config: N/A
2022-04-17T12:12:57,068 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.5.3
TS Home: C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages
Current directory: C:\Users\Ameno\Desktop\luna\ai
Temp directory: C:\Users\Ameno\AppData\Local\Temp
Number of GPUs: 0
Number of CPUs: 12
Max heap size: 4048 M
Python executable: C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe
Config file: deployment/config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: C:\Users\Ameno\Desktop\luna\ai\deployment\model-store
Initial Models: foodnet=foodnet_resnet18.mar
Log dir: C:\Users\Ameno\Desktop\luna\ai\logs
Metrics dir: C:\Users\Ameno\Desktop\luna\ai\logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 12
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: C:\Users\Ameno\Desktop\luna\ai\deployment\model-store
Model config: N/A
2022-04-17T12:12:57,081 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-04-17T12:12:57,081 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-04-17T12:12:57,100 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: foodnet_resnet18.mar
2022-04-17T12:12:57,100 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: foodnet_resnet18.mar
2022-04-17T12:12:58,128 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model foodnet
2022-04-17T12:12:58,128 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model foodnet
2022-04-17T12:12:58,128 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model foodnet
2022-04-17T12:12:58,128 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model foodnet
2022-04-17T12:12:58,129 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model foodnet loaded.
2022-04-17T12:12:58,129 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model foodnet loaded.
2022-04-17T12:12:58,129 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: foodnet, count: 12
2022-04-17T12:12:58,129 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: foodnet, count: 12
2022-04-17T12:12:58,141 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-04-17T12:12:58,141 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-04-17T12:12:58,141 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-04-17T12:12:58,141 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-04-17T12:12:58,141 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-04-17T12:12:58,141 [DEBUG] W-9009-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9009]
2022-04-17T12:12:58,141 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-04-17T12:12:58,141 [DEBUG] W-9011-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9011]
2022-04-17T12:12:58,141 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9008]
2022-04-17T12:12:58,141 [DEBUG] W-9001-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9001]
2022-04-17T12:12:58,141 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-04-17T12:12:58,141 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-04-17T12:12:58,141 [DEBUG] W-9011-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9011]
2022-04-17T12:12:58,142 [DEBUG] W-9010-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9010]
2022-04-17T12:12:58,141 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-04-17T12:12:58,141 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-04-17T12:12:58,141 [DEBUG] W-9009-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9009]
2022-04-17T12:12:58,141 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-04-17T12:12:58,141 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-04-17T12:12:58,142 [DEBUG] W-9010-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9010]
2022-04-17T12:12:58,141 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-04-17T12:12:58,141 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9008]
2022-04-17T12:12:58,145 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: NioServerSocketChannel.
2022-04-17T12:12:58,141 [DEBUG] W-9001-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9001]
2022-04-17T12:12:58,141 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-04-17T12:12:58,145 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: NioServerSocketChannel.
2022-04-17T12:12:58,429 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-04-17T12:12:58,429 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-04-17T12:12:58,430 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: NioServerSocketChannel.
2022-04-17T12:12:58,430 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: NioServerSocketChannel.
2022-04-17T12:12:58,434 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-04-17T12:12:58,434 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-04-17T12:12:58,436 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: NioServerSocketChannel.
2022-04-17T12:12:58,436 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: NioServerSocketChannel.
2022-04-17T12:12:58,440 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2022-04-17T12:12:58,440 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2022-04-17T12:12:59,180 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-04-17T12:12:59,180 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-04-17T12:12:59,504 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:100.0|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:1650190379
2022-04-17T12:12:59,507 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:78.0489501953125|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:1650190379
2022-04-17T12:12:59,508 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:377.79967880249023|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:1650190379
2022-04-17T12:12:59,509 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:82.9|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:1650190379
2022-04-17T12:12:59,510 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:5398.046875|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:1650190379
2022-04-17T12:12:59,511 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:10791.6171875|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:1650190379
2022-04-17T12:12:59,512 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:66.7|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:1650190379
2022-04-17T12:13:00,379 [INFO ] W-9011-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T12:13:00,382 [INFO ] W-9011-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T12:13:00,383 [INFO ] W-9011-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T12:13:00,384 [INFO ] W-9011-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-04-17T12:13:00,385 [INFO ] W-9011-foodnet_1.0-stdout MODEL_LOG -     worker.run_server()
2022-04-17T12:13:00,386 [INFO ] W-9011-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py", line 148, in run_server
2022-04-17T12:13:00,387 [INFO ] W-9011-foodnet_1.0-stdout MODEL_LOG -     self.sock.bind((self.sock_name, int(self.port)))
2022-04-17T12:13:00,388 [INFO ] W-9011-foodnet_1.0-stdout MODEL_LOG - OSError: [WinError 10013] An attempt was made to access a socket in a way forbidden by its access permissions
2022-04-17T12:13:00,417 [INFO ] W-9010-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T12:13:00,420 [INFO ] W-9010-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T12:13:00,421 [INFO ] W-9010-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T12:13:00,422 [INFO ] W-9010-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-04-17T12:13:00,422 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T12:13:00,423 [INFO ] W-9010-foodnet_1.0-stdout MODEL_LOG -     worker.run_server()
2022-04-17T12:13:00,424 [INFO ] W-9010-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py", line 148, in run_server
2022-04-17T12:13:00,424 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - [PID]4860
2022-04-17T12:13:00,424 [INFO ] W-9010-foodnet_1.0-stdout MODEL_LOG -     self.sock.bind((self.sock_name, int(self.port)))
2022-04-17T12:13:00,425 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T12:13:00,425 [INFO ] W-9010-foodnet_1.0-stdout MODEL_LOG - OSError: [WinError 10013] An attempt was made to access a socket in a way forbidden by its access permissions
2022-04-17T12:13:00,426 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T12:13:00,426 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change null -> WORKER_STARTED
2022-04-17T12:13:00,426 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change null -> WORKER_STARTED
2022-04-17T12:13:00,435 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-04-17T12:13:00,435 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-04-17T12:13:00,453 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T12:13:00,456 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T12:13:00,459 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - [PID]8364
2022-04-17T12:13:00,459 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - [PID]1716
2022-04-17T12:13:00,460 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change null -> WORKER_STARTED
2022-04-17T12:13:00,460 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change null -> WORKER_STARTED
2022-04-17T12:13:00,460 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T12:13:00,460 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change null -> WORKER_STARTED
2022-04-17T12:13:00,462 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-04-17T12:13:00,460 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T12:13:00,461 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9004).
2022-04-17T12:13:00,461 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T12:13:00,462 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T12:13:00,462 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-04-17T12:13:00,460 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change null -> WORKER_STARTED
2022-04-17T12:13:00,464 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-04-17T12:13:00,464 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-04-17T12:13:00,468 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9005).
2022-04-17T12:13:00,468 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9003).
2022-04-17T12:13:00,470 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650190380470
2022-04-17T12:13:00,470 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650190380470
2022-04-17T12:13:00,470 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650190380470
2022-04-17T12:13:00,470 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650190380470
2022-04-17T12:13:00,470 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650190380470
2022-04-17T12:13:00,470 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650190380470
2022-04-17T12:13:00,534 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T12:13:00,542 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T12:13:00,542 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T12:13:00,542 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T12:13:00,545 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - [PID]19048
2022-04-17T12:13:00,546 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change null -> WORKER_STARTED
2022-04-17T12:13:00,546 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T12:13:00,546 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change null -> WORKER_STARTED
2022-04-17T12:13:00,547 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T12:13:00,550 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-04-17T12:13:00,550 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-04-17T12:13:00,550 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T12:13:00,554 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - [PID]18824
2022-04-17T12:13:00,555 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change null -> WORKER_STARTED
2022-04-17T12:13:00,555 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T12:13:00,555 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change null -> WORKER_STARTED
2022-04-17T12:13:00,556 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-04-17T12:13:00,556 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T12:13:00,556 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-04-17T12:13:00,557 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650190380557
2022-04-17T12:13:00,557 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650190380557
2022-04-17T12:13:00,557 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9000).
2022-04-17T12:13:00,561 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9006).
2022-04-17T12:13:00,564 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650190380564
2022-04-17T12:13:00,564 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650190380564
2022-04-17T12:13:00,587 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T12:13:00,596 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T12:13:00,666 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T12:13:00,668 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - [PID]19284
2022-04-17T12:13:00,669 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T12:13:00,670 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change null -> WORKER_STARTED
2022-04-17T12:13:00,670 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T12:13:00,670 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change null -> WORKER_STARTED
2022-04-17T12:13:00,671 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-04-17T12:13:00,671 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-04-17T12:13:00,675 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650190380675
2022-04-17T12:13:00,675 [INFO ] W-9001-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T12:13:00,676 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9002).
2022-04-17T12:13:00,675 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650190380675
2022-04-17T12:13:00,677 [INFO ] W-9001-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T12:13:00,678 [INFO ] W-9001-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T12:13:00,679 [INFO ] W-9001-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-04-17T12:13:00,680 [INFO ] W-9001-foodnet_1.0-stdout MODEL_LOG -     worker.run_server()
2022-04-17T12:13:00,681 [INFO ] W-9001-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py", line 148, in run_server
2022-04-17T12:13:00,682 [INFO ] W-9001-foodnet_1.0-stdout MODEL_LOG -     self.sock.bind((self.sock_name, int(self.port)))
2022-04-17T12:13:00,683 [INFO ] W-9001-foodnet_1.0-stdout MODEL_LOG - OSError: [WinError 10013] An attempt was made to access a socket in a way forbidden by its access permissions
2022-04-17T12:13:00,710 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T12:13:00,726 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T12:13:00,728 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - [PID]18676
2022-04-17T12:13:00,729 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change null -> WORKER_STARTED
2022-04-17T12:13:00,729 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T12:13:00,729 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change null -> WORKER_STARTED
2022-04-17T12:13:00,730 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T12:13:00,730 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-04-17T12:13:00,730 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-04-17T12:13:00,735 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650190380735
2022-04-17T12:13:00,735 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9007).
2022-04-17T12:13:00,735 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650190380735
2022-04-17T12:13:00,775 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T12:13:00,810 [INFO ] W-9009-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T12:13:00,812 [INFO ] W-9009-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T12:13:00,813 [INFO ] W-9011-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9011-foodnet_1.0-stdout
2022-04-17T12:13:00,813 [INFO ] W-9009-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T12:13:00,813 [INFO ] W-9011-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9011-foodnet_1.0-stderr
2022-04-17T12:13:00,813 [INFO ] W-9011-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9011-foodnet_1.0-stdout
2022-04-17T12:13:00,813 [INFO ] W-9011-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9011-foodnet_1.0-stderr
2022-04-17T12:13:00,813 [INFO ] W-9009-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-04-17T12:13:00,815 [WARN ] W-9011-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9011-foodnet_1.0-stderr
2022-04-17T12:13:00,815 [INFO ] W-9009-foodnet_1.0-stdout MODEL_LOG -     worker.run_server()
2022-04-17T12:13:00,816 [INFO ] W-9009-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py", line 148, in run_server
2022-04-17T12:13:00,815 [WARN ] W-9011-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9011-foodnet_1.0-stderr
2022-04-17T12:13:00,817 [INFO ] W-9009-foodnet_1.0-stdout MODEL_LOG -     self.sock.bind((self.sock_name, int(self.port)))
2022-04-17T12:13:00,818 [WARN ] W-9011-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9011-foodnet_1.0-stdout
2022-04-17T12:13:00,818 [WARN ] W-9011-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9011-foodnet_1.0-stdout
2022-04-17T12:13:00,818 [INFO ] W-9009-foodnet_1.0-stdout MODEL_LOG - OSError: [WinError 10013] An attempt was made to access a socket in a way forbidden by its access permissions
2022-04-17T12:13:00,819 [ERROR] W-9011-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:292) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T12:13:00,819 [ERROR] W-9011-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:292) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T12:13:00,883 [INFO ] W-9010-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9010-foodnet_1.0-stdout
2022-04-17T12:13:00,883 [INFO ] W-9010-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9010-foodnet_1.0-stderr
2022-04-17T12:13:00,883 [INFO ] W-9010-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9010-foodnet_1.0-stdout
2022-04-17T12:13:00,884 [WARN ] W-9010-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9010-foodnet_1.0-stderr
2022-04-17T12:13:00,883 [INFO ] W-9010-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9010-foodnet_1.0-stderr
2022-04-17T12:13:00,884 [WARN ] W-9010-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9010-foodnet_1.0-stderr
2022-04-17T12:13:00,885 [WARN ] W-9010-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9010-foodnet_1.0-stdout
2022-04-17T12:13:00,885 [WARN ] W-9010-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9010-foodnet_1.0-stdout
2022-04-17T12:13:00,885 [ERROR] W-9010-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:292) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T12:13:00,885 [ERROR] W-9010-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:292) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T12:13:00,908 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T12:13:00,910 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - [PID]1844
2022-04-17T12:13:00,911 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T12:13:00,912 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change null -> WORKER_STARTED
2022-04-17T12:13:00,912 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change null -> WORKER_STARTED
2022-04-17T12:13:00,912 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T12:13:00,913 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9008
2022-04-17T12:13:00,913 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9008
2022-04-17T12:13:00,918 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9008).
2022-04-17T12:13:00,920 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650190380919
2022-04-17T12:13:00,920 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650190380919
2022-04-17T12:13:00,958 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T12:13:01,044 [INFO ] W-9001-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-foodnet_1.0-stdout
2022-04-17T12:13:01,044 [INFO ] W-9001-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-foodnet_1.0-stderr
2022-04-17T12:13:01,044 [INFO ] W-9001-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-foodnet_1.0-stdout
2022-04-17T12:13:01,044 [INFO ] W-9001-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-foodnet_1.0-stderr
2022-04-17T12:13:01,045 [WARN ] W-9001-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-foodnet_1.0-stderr
2022-04-17T12:13:01,045 [WARN ] W-9001-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-foodnet_1.0-stderr
2022-04-17T12:13:01,047 [WARN ] W-9001-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-foodnet_1.0-stdout
2022-04-17T12:13:01,047 [WARN ] W-9001-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-foodnet_1.0-stdout
2022-04-17T12:13:01,047 [ERROR] W-9001-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:292) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T12:13:01,047 [ERROR] W-9001-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:292) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T12:13:01,165 [INFO ] W-9009-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9009-foodnet_1.0-stdout
2022-04-17T12:13:01,165 [INFO ] W-9009-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9009-foodnet_1.0-stderr
2022-04-17T12:13:01,165 [INFO ] W-9009-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9009-foodnet_1.0-stdout
2022-04-17T12:13:01,166 [WARN ] W-9009-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9009-foodnet_1.0-stderr
2022-04-17T12:13:01,165 [INFO ] W-9009-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9009-foodnet_1.0-stderr
2022-04-17T12:13:01,166 [WARN ] W-9009-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9009-foodnet_1.0-stderr
2022-04-17T12:13:01,167 [WARN ] W-9009-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9009-foodnet_1.0-stdout
2022-04-17T12:13:01,167 [WARN ] W-9009-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9009-foodnet_1.0-stdout
2022-04-17T12:13:01,168 [ERROR] W-9009-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:292) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T12:13:01,168 [ERROR] W-9009-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:292) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T12:13:05,338 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 4751
2022-04-17T12:13:05,338 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 4751
2022-04-17T12:13:05,339 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-04-17T12:13:05,339 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-04-17T12:13:05,340 [INFO ] W-9000-foodnet_1.0 TS_METRICS - W-9000-foodnet_1.0.ms:7204|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:1650190385
2022-04-17T12:13:05,341 [INFO ] W-9000-foodnet_1.0 TS_METRICS - WorkerThreadTime.ms:33|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:null
2022-04-17T12:13:05,369 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 4827
2022-04-17T12:13:05,369 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 4827
2022-04-17T12:13:05,370 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-04-17T12:13:05,370 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-04-17T12:13:05,371 [INFO ] W-9005-foodnet_1.0 TS_METRICS - W-9005-foodnet_1.0.ms:7233|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:1650190385
2022-04-17T12:13:05,372 [INFO ] W-9005-foodnet_1.0 TS_METRICS - WorkerThreadTime.ms:75|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:null
2022-04-17T12:13:05,422 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 4880
2022-04-17T12:13:05,422 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 4880
2022-04-17T12:13:05,423 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-04-17T12:13:05,423 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-04-17T12:13:05,424 [INFO ] W-9004-foodnet_1.0 TS_METRICS - W-9004-foodnet_1.0.ms:7285|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:1650190385
2022-04-17T12:13:05,425 [INFO ] W-9004-foodnet_1.0 TS_METRICS - WorkerThreadTime.ms:74|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:null
2022-04-17T12:13:05,438 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 4895
2022-04-17T12:13:05,438 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 4895
2022-04-17T12:13:05,439 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-04-17T12:13:05,439 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-04-17T12:13:05,439 [INFO ] W-9003-foodnet_1.0 TS_METRICS - W-9003-foodnet_1.0.ms:7301|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:1650190385
2022-04-17T12:13:05,440 [INFO ] W-9003-foodnet_1.0 TS_METRICS - WorkerThreadTime.ms:75|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:null
2022-04-17T12:13:05,457 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 4861
2022-04-17T12:13:05,457 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 4861
2022-04-17T12:13:05,459 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-04-17T12:13:05,459 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-04-17T12:13:05,460 [INFO ] W-9006-foodnet_1.0 TS_METRICS - W-9006-foodnet_1.0.ms:7322|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:1650190385
2022-04-17T12:13:05,461 [INFO ] W-9006-foodnet_1.0 TS_METRICS - WorkerThreadTime.ms:36|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:null
2022-04-17T12:13:05,486 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 4776
2022-04-17T12:13:05,486 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 4776
2022-04-17T12:13:05,487 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-04-17T12:13:05,487 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-04-17T12:13:05,488 [INFO ] W-9002-foodnet_1.0 TS_METRICS - W-9002-foodnet_1.0.ms:7350|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:1650190385
2022-04-17T12:13:05,489 [INFO ] W-9002-foodnet_1.0 TS_METRICS - WorkerThreadTime.ms:38|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:null
2022-04-17T12:13:05,519 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 4745
2022-04-17T12:13:05,519 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 4745
2022-04-17T12:13:05,520 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-04-17T12:13:05,520 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-04-17T12:13:05,521 [INFO ] W-9007-foodnet_1.0 TS_METRICS - W-9007-foodnet_1.0.ms:7383|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:1650190385
2022-04-17T12:13:05,521 [INFO ] W-9007-foodnet_1.0 TS_METRICS - WorkerThreadTime.ms:41|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:null
2022-04-17T12:13:05,529 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 4571
2022-04-17T12:13:05,529 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 4571
2022-04-17T12:13:05,529 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-04-17T12:13:05,529 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-04-17T12:13:05,530 [INFO ] W-9008-foodnet_1.0 TS_METRICS - W-9008-foodnet_1.0.ms:7392|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:1650190385
2022-04-17T12:13:05,531 [INFO ] W-9008-foodnet_1.0 TS_METRICS - WorkerThreadTime.ms:41|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:null
2022-04-17T12:17:07,645 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-04-17T12:17:07,645 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-04-17T12:17:07,968 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.5.3
TS Home: C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages
Current directory: C:\Users\Ameno\Desktop\luna\ai
Temp directory: C:\Users\Ameno\AppData\Local\Temp
Number of GPUs: 0
Number of CPUs: 12
Max heap size: 4048 M
Python executable: C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe
Config file: deployment/config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: C:\Users\Ameno\Desktop\luna\ai\deployment\model-store
Initial Models: foodnet=foodnet_resnet18.mar
Log dir: C:\Users\Ameno\Desktop\luna\ai\logs
Metrics dir: C:\Users\Ameno\Desktop\luna\ai\logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 12
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: C:\Users\Ameno\Desktop\luna\ai\deployment\model-store
Model config: N/A
2022-04-17T12:17:07,968 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.5.3
TS Home: C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages
Current directory: C:\Users\Ameno\Desktop\luna\ai
Temp directory: C:\Users\Ameno\AppData\Local\Temp
Number of GPUs: 0
Number of CPUs: 12
Max heap size: 4048 M
Python executable: C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe
Config file: deployment/config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: C:\Users\Ameno\Desktop\luna\ai\deployment\model-store
Initial Models: foodnet=foodnet_resnet18.mar
Log dir: C:\Users\Ameno\Desktop\luna\ai\logs
Metrics dir: C:\Users\Ameno\Desktop\luna\ai\logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 12
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: C:\Users\Ameno\Desktop\luna\ai\deployment\model-store
Model config: N/A
2022-04-17T12:17:07,986 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-04-17T12:17:07,986 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-04-17T12:17:08,022 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: foodnet_resnet18.mar
2022-04-17T12:17:08,022 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: foodnet_resnet18.mar
2022-04-17T12:17:09,530 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model foodnet
2022-04-17T12:17:09,530 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model foodnet
2022-04-17T12:17:09,531 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model foodnet
2022-04-17T12:17:09,531 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model foodnet
2022-04-17T12:17:09,532 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model foodnet loaded.
2022-04-17T12:17:09,532 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model foodnet loaded.
2022-04-17T12:17:09,532 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: foodnet, count: 12
2022-04-17T12:17:09,532 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: foodnet, count: 12
2022-04-17T12:17:09,545 [DEBUG] W-9001-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9001]
2022-04-17T12:17:09,545 [DEBUG] W-9009-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9009]
2022-04-17T12:17:09,545 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-04-17T12:17:09,545 [DEBUG] W-9011-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9011]
2022-04-17T12:17:09,545 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-04-17T12:17:09,545 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-04-17T12:17:09,545 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-04-17T12:17:09,545 [DEBUG] W-9010-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9010]
2022-04-17T12:17:09,545 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-04-17T12:17:09,545 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-04-17T12:17:09,545 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9008]
2022-04-17T12:17:09,545 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-04-17T12:17:09,545 [DEBUG] W-9001-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9001]
2022-04-17T12:17:09,545 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-04-17T12:17:09,545 [DEBUG] W-9010-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9010]
2022-04-17T12:17:09,545 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-04-17T12:17:09,545 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-04-17T12:17:09,545 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-04-17T12:17:09,545 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-04-17T12:17:09,545 [DEBUG] W-9011-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9011]
2022-04-17T12:17:09,545 [DEBUG] W-9009-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9009]
2022-04-17T12:17:09,545 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-04-17T12:17:09,545 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9008]
2022-04-17T12:17:09,545 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [C:\Users\Ameno\Anaconda3\envs\luna-ai\python.exe, C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-04-17T12:17:09,550 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: NioServerSocketChannel.
2022-04-17T12:17:09,550 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: NioServerSocketChannel.
2022-04-17T12:17:09,894 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-04-17T12:17:09,894 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-04-17T12:17:09,896 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: NioServerSocketChannel.
2022-04-17T12:17:09,896 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: NioServerSocketChannel.
2022-04-17T12:17:09,899 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-04-17T12:17:09,899 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-04-17T12:17:09,901 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: NioServerSocketChannel.
2022-04-17T12:17:09,901 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: NioServerSocketChannel.
2022-04-17T12:17:09,903 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2022-04-17T12:17:09,903 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2022-04-17T12:17:11,004 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-04-17T12:17:11,004 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-04-17T12:17:11,626 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:100.0|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:1650190631
2022-04-17T12:17:11,629 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:80.0656852722168|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:1650190631
2022-04-17T12:17:11,630 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:375.78294372558594|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:1650190631
2022-04-17T12:17:11,632 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:82.4|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:1650190631
2022-04-17T12:17:11,633 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:5503.23828125|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:1650190631
2022-04-17T12:17:11,635 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:10686.45703125|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:1650190631
2022-04-17T12:17:11,637 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:66.0|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:1650190631
2022-04-17T12:17:13,649 [INFO ] W-9011-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T12:17:13,654 [INFO ] W-9011-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T12:17:13,659 [INFO ] W-9011-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T12:17:13,661 [INFO ] W-9011-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-04-17T12:17:13,663 [INFO ] W-9011-foodnet_1.0-stdout MODEL_LOG -     worker.run_server()
2022-04-17T12:17:13,664 [INFO ] W-9011-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py", line 148, in run_server
2022-04-17T12:17:13,665 [INFO ] W-9011-foodnet_1.0-stdout MODEL_LOG -     self.sock.bind((self.sock_name, int(self.port)))
2022-04-17T12:17:13,667 [INFO ] W-9011-foodnet_1.0-stdout MODEL_LOG - OSError: [WinError 10013] An attempt was made to access a socket in a way forbidden by its access permissions
2022-04-17T12:17:13,667 [INFO ] W-9001-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T12:17:13,670 [INFO ] W-9001-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T12:17:13,671 [INFO ] W-9001-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T12:17:13,673 [INFO ] W-9001-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-04-17T12:17:13,674 [INFO ] W-9001-foodnet_1.0-stdout MODEL_LOG -     worker.run_server()
2022-04-17T12:17:13,675 [INFO ] W-9001-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py", line 148, in run_server
2022-04-17T12:17:13,676 [INFO ] W-9001-foodnet_1.0-stdout MODEL_LOG -     self.sock.bind((self.sock_name, int(self.port)))
2022-04-17T12:17:13,678 [INFO ] W-9001-foodnet_1.0-stdout MODEL_LOG - OSError: [WinError 10013] An attempt was made to access a socket in a way forbidden by its access permissions
2022-04-17T12:17:13,774 [INFO ] W-9009-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T12:17:13,777 [INFO ] W-9009-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T12:17:13,779 [INFO ] W-9009-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T12:17:13,780 [INFO ] W-9009-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-04-17T12:17:13,781 [INFO ] W-9009-foodnet_1.0-stdout MODEL_LOG -     worker.run_server()
2022-04-17T12:17:13,783 [INFO ] W-9009-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py", line 148, in run_server
2022-04-17T12:17:13,784 [INFO ] W-9009-foodnet_1.0-stdout MODEL_LOG -     self.sock.bind((self.sock_name, int(self.port)))
2022-04-17T12:17:13,785 [INFO ] W-9009-foodnet_1.0-stdout MODEL_LOG - OSError: [WinError 10013] An attempt was made to access a socket in a way forbidden by its access permissions
2022-04-17T12:17:13,825 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T12:17:13,827 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T12:17:13,830 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - [PID]6708
2022-04-17T12:17:13,831 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T12:17:13,832 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change null -> WORKER_STARTED
2022-04-17T12:17:13,832 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - [PID]19528
2022-04-17T12:17:13,832 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T12:17:13,832 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change null -> WORKER_STARTED
2022-04-17T12:17:13,833 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T12:17:13,835 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change null -> WORKER_STARTED
2022-04-17T12:17:13,835 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change null -> WORKER_STARTED
2022-04-17T12:17:13,836 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T12:17:13,846 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-04-17T12:17:13,846 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-04-17T12:17:13,846 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-04-17T12:17:13,846 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-04-17T12:17:13,850 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T12:17:13,853 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - [PID]18556
2022-04-17T12:17:13,854 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change null -> WORKER_STARTED
2022-04-17T12:17:13,854 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T12:17:13,854 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change null -> WORKER_STARTED
2022-04-17T12:17:13,856 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-04-17T12:17:13,855 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T12:17:13,856 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-04-17T12:17:13,881 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9003).
2022-04-17T12:17:13,881 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9006).
2022-04-17T12:17:13,881 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9005).
2022-04-17T12:17:13,890 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650190633890
2022-04-17T12:17:13,890 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650190633890
2022-04-17T12:17:13,890 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650190633890
2022-04-17T12:17:13,890 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650190633890
2022-04-17T12:17:13,890 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650190633890
2022-04-17T12:17:13,890 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650190633890
2022-04-17T12:17:13,905 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T12:17:13,912 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - [PID]5520
2022-04-17T12:17:13,914 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change null -> WORKER_STARTED
2022-04-17T12:17:13,914 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T12:17:13,914 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change null -> WORKER_STARTED
2022-04-17T12:17:13,916 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9008
2022-04-17T12:17:13,915 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T12:17:13,916 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9008
2022-04-17T12:17:13,922 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650190633922
2022-04-17T12:17:13,922 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9008).
2022-04-17T12:17:13,922 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650190633922
2022-04-17T12:17:13,926 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T12:17:13,933 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - [PID]19900
2022-04-17T12:17:13,934 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change null -> WORKER_STARTED
2022-04-17T12:17:13,934 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T12:17:13,934 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change null -> WORKER_STARTED
2022-04-17T12:17:13,936 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-04-17T12:17:13,936 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-04-17T12:17:13,942 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650190633942
2022-04-17T12:17:13,942 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650190633942
2022-04-17T12:17:13,946 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T12:17:13,947 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9007).
2022-04-17T12:17:13,976 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T12:17:13,978 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - [PID]19064
2022-04-17T12:17:13,980 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change null -> WORKER_STARTED
2022-04-17T12:17:13,980 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T12:17:13,980 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change null -> WORKER_STARTED
2022-04-17T12:17:13,983 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-04-17T12:17:13,981 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T12:17:13,983 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-04-17T12:17:13,988 [INFO ] W-9008-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T12:17:13,988 [INFO ] W-9005-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T12:17:13,987 [INFO ] W-9003-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T12:17:13,987 [INFO ] W-9006-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T12:17:13,987 [INFO ] W-9007-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T12:17:13,995 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650190633995
2022-04-17T12:17:13,995 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650190633995
2022-04-17T12:17:13,996 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9000).
2022-04-17T12:17:14,017 [INFO ] W-9011-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9011-foodnet_1.0-stderr
2022-04-17T12:17:14,017 [INFO ] W-9011-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9011-foodnet_1.0-stdout
2022-04-17T12:17:14,017 [INFO ] W-9011-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9011-foodnet_1.0-stdout
2022-04-17T12:17:14,017 [INFO ] W-9011-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9011-foodnet_1.0-stderr
2022-04-17T12:17:14,021 [WARN ] W-9011-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9011-foodnet_1.0-stderr
2022-04-17T12:17:14,021 [WARN ] W-9011-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9011-foodnet_1.0-stderr
2022-04-17T12:17:14,023 [WARN ] W-9011-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9011-foodnet_1.0-stdout
2022-04-17T12:17:14,023 [WARN ] W-9011-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9011-foodnet_1.0-stdout
2022-04-17T12:17:14,028 [INFO ] W-9000-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T12:17:14,051 [INFO ] W-9001-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-foodnet_1.0-stdout
2022-04-17T12:17:14,051 [INFO ] W-9001-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-foodnet_1.0-stderr
2022-04-17T12:17:14,051 [INFO ] W-9001-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-foodnet_1.0-stdout
2022-04-17T12:17:14,051 [INFO ] W-9001-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-foodnet_1.0-stderr
2022-04-17T12:17:14,054 [WARN ] W-9001-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-foodnet_1.0-stderr
2022-04-17T12:17:14,054 [WARN ] W-9001-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-foodnet_1.0-stderr
2022-04-17T12:17:14,058 [WARN ] W-9001-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-foodnet_1.0-stdout
2022-04-17T12:17:14,058 [WARN ] W-9001-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-foodnet_1.0-stdout
2022-04-17T12:17:14,059 [ERROR] W-9001-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:292) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T12:17:14,036 [ERROR] W-9011-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:292) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T12:17:14,059 [ERROR] W-9001-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:292) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T12:17:14,036 [ERROR] W-9011-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:292) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T12:17:14,094 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T12:17:14,096 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - [PID]18864
2022-04-17T12:17:14,097 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change null -> WORKER_STARTED
2022-04-17T12:17:14,097 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T12:17:14,097 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change null -> WORKER_STARTED
2022-04-17T12:17:14,099 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-04-17T12:17:14,098 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T12:17:14,099 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-04-17T12:17:14,105 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650190634104
2022-04-17T12:17:14,105 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9004).
2022-04-17T12:17:14,105 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650190634104
2022-04-17T12:17:14,147 [INFO ] W-9004-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T12:17:14,166 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T12:17:14,172 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - [PID]19552
2022-04-17T12:17:14,173 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change null -> WORKER_STARTED
2022-04-17T12:17:14,173 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-17T12:17:14,173 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change null -> WORKER_STARTED
2022-04-17T12:17:14,174 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-04-17T12:17:14,174 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Python runtime: 3.9.4
2022-04-17T12:17:14,174 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-04-17T12:17:14,179 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650190634179
2022-04-17T12:17:14,179 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1650190634179
2022-04-17T12:17:14,180 [INFO ] W-9009-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9009-foodnet_1.0-stdout
2022-04-17T12:17:14,179 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9002).
2022-04-17T12:17:14,180 [INFO ] W-9009-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9009-foodnet_1.0-stdout
2022-04-17T12:17:14,184 [WARN ] W-9009-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9009-foodnet_1.0-stderr
2022-04-17T12:17:14,184 [INFO ] W-9009-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9009-foodnet_1.0-stderr
2022-04-17T12:17:14,184 [WARN ] W-9009-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9009-foodnet_1.0-stderr
2022-04-17T12:17:14,185 [WARN ] W-9009-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9009-foodnet_1.0-stdout
2022-04-17T12:17:14,184 [INFO ] W-9009-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9009-foodnet_1.0-stderr
2022-04-17T12:17:14,185 [WARN ] W-9009-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9009-foodnet_1.0-stdout
2022-04-17T12:17:14,186 [ERROR] W-9009-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:292) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T12:17:14,186 [ERROR] W-9009-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:292) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T12:17:14,217 [INFO ] W-9002-foodnet_1.0-stdout MODEL_LOG - model_name: foodnet, batchSize: 1
2022-04-17T12:17:14,334 [INFO ] W-9010-foodnet_1.0-stdout MODEL_LOG - Listening on port: None
2022-04-17T12:17:14,336 [INFO ] W-9010-foodnet_1.0-stdout MODEL_LOG - Backend worker process died.
2022-04-17T12:17:14,337 [INFO ] W-9010-foodnet_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-04-17T12:17:14,338 [INFO ] W-9010-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-04-17T12:17:14,340 [INFO ] W-9010-foodnet_1.0-stdout MODEL_LOG -     worker.run_server()
2022-04-17T12:17:14,341 [INFO ] W-9010-foodnet_1.0-stdout MODEL_LOG -   File "C:\Users\Ameno\Anaconda3\envs\luna-ai\Lib\site-packages\ts\model_service_worker.py", line 148, in run_server
2022-04-17T12:17:14,342 [INFO ] W-9010-foodnet_1.0-stdout MODEL_LOG -     self.sock.bind((self.sock_name, int(self.port)))
2022-04-17T12:17:14,343 [INFO ] W-9010-foodnet_1.0-stdout MODEL_LOG - OSError: [WinError 10013] An attempt was made to access a socket in a way forbidden by its access permissions
2022-04-17T12:17:14,589 [INFO ] W-9010-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9010-foodnet_1.0-stdout
2022-04-17T12:17:14,589 [INFO ] W-9010-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9010-foodnet_1.0-stderr
2022-04-17T12:17:14,589 [INFO ] W-9010-foodnet_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9010-foodnet_1.0-stdout
2022-04-17T12:17:14,589 [INFO ] W-9010-foodnet_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9010-foodnet_1.0-stderr
2022-04-17T12:17:14,591 [WARN ] W-9010-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9010-foodnet_1.0-stderr
2022-04-17T12:17:14,591 [WARN ] W-9010-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9010-foodnet_1.0-stderr
2022-04-17T12:17:14,593 [WARN ] W-9010-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9010-foodnet_1.0-stdout
2022-04-17T12:17:14,593 [WARN ] W-9010-foodnet_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9010-foodnet_1.0-stdout
2022-04-17T12:17:14,594 [ERROR] W-9010-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:292) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T12:17:14,594 [ERROR] W-9010-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:292) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-04-17T12:17:19,693 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 5705
2022-04-17T12:17:19,693 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 5705
2022-04-17T12:17:19,693 [INFO ] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 5705
2022-04-17T12:17:19,693 [INFO ] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 5705
2022-04-17T12:17:19,695 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-04-17T12:17:19,695 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-04-17T12:17:19,695 [DEBUG] W-9008-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9008-foodnet_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-04-17T12:17:19,695 [DEBUG] W-9005-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-foodnet_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-04-17T12:17:19,696 [INFO ] W-9008-foodnet_1.0 TS_METRICS - W-9008-foodnet_1.0.ms:10154|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:1650190639
2022-04-17T12:17:19,696 [INFO ] W-9005-foodnet_1.0 TS_METRICS - W-9005-foodnet_1.0.ms:10154|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:1650190639
2022-04-17T12:17:19,697 [INFO ] W-9008-foodnet_1.0 TS_METRICS - WorkerThreadTime.ms:70|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:null
2022-04-17T12:17:19,697 [INFO ] W-9005-foodnet_1.0 TS_METRICS - WorkerThreadTime.ms:102|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:null
2022-04-17T12:17:19,701 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 5674
2022-04-17T12:17:19,701 [INFO ] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 5674
2022-04-17T12:17:19,703 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-04-17T12:17:19,703 [DEBUG] W-9000-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-foodnet_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-04-17T12:17:19,703 [INFO ] W-9000-foodnet_1.0 TS_METRICS - W-9000-foodnet_1.0.ms:10164|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:1650190639
2022-04-17T12:17:19,705 [INFO ] W-9000-foodnet_1.0 TS_METRICS - WorkerThreadTime.ms:36|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:null
2022-04-17T12:17:19,759 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 5767
2022-04-17T12:17:19,759 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 5772
2022-04-17T12:17:19,759 [INFO ] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 5767
2022-04-17T12:17:19,761 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-04-17T12:17:19,759 [INFO ] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 5772
2022-04-17T12:17:19,761 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-04-17T12:17:19,761 [DEBUG] W-9007-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-foodnet_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-04-17T12:17:19,761 [DEBUG] W-9003-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-foodnet_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-04-17T12:17:19,763 [INFO ] W-9007-foodnet_1.0 TS_METRICS - W-9007-foodnet_1.0.ms:10221|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:1650190639
2022-04-17T12:17:19,763 [INFO ] W-9003-foodnet_1.0 TS_METRICS - W-9003-foodnet_1.0.ms:10222|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:1650190639
2022-04-17T12:17:19,765 [INFO ] W-9003-foodnet_1.0 TS_METRICS - WorkerThreadTime.ms:103|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:null
2022-04-17T12:17:19,765 [INFO ] W-9007-foodnet_1.0 TS_METRICS - WorkerThreadTime.ms:56|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:null
2022-04-17T12:17:19,814 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 5822
2022-04-17T12:17:19,814 [INFO ] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 5822
2022-04-17T12:17:19,815 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-04-17T12:17:19,815 [DEBUG] W-9006-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-foodnet_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-04-17T12:17:19,816 [INFO ] W-9006-foodnet_1.0 TS_METRICS - W-9006-foodnet_1.0.ms:10274|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:1650190639
2022-04-17T12:17:19,817 [INFO ] W-9006-foodnet_1.0 TS_METRICS - WorkerThreadTime.ms:105|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:null
2022-04-17T12:17:19,820 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 5604
2022-04-17T12:17:19,820 [INFO ] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 5604
2022-04-17T12:17:19,821 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-04-17T12:17:19,821 [DEBUG] W-9002-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-foodnet_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-04-17T12:17:19,822 [INFO ] W-9002-foodnet_1.0 TS_METRICS - W-9002-foodnet_1.0.ms:10281|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:1650190639
2022-04-17T12:17:19,824 [INFO ] W-9002-foodnet_1.0 TS_METRICS - WorkerThreadTime.ms:40|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:null
2022-04-17T12:17:19,846 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 5699
2022-04-17T12:17:19,846 [INFO ] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 5699
2022-04-17T12:17:19,847 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-04-17T12:17:19,847 [DEBUG] W-9004-foodnet_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-foodnet_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-04-17T12:17:19,848 [INFO ] W-9004-foodnet_1.0 TS_METRICS - W-9004-foodnet_1.0.ms:10306|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:1650190639
2022-04-17T12:17:19,849 [INFO ] W-9004-foodnet_1.0 TS_METRICS - WorkerThreadTime.ms:46|#Level:Host|#hostname:DESKTOP-49NTERP,timestamp:null
